<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawal’s Note">
<meta name="dcterms.date" content="2025-03-02">

<title>Experimental Design in Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#experimental-design-in-python" id="toc-experimental-design-in-python" class="nav-link active" data-scroll-target="#experimental-design-in-python"><span class="header-section-number">1</span> Experimental Design in Python</a>
  <ul class="collapse">
  <li><a href="#sec-Chapter1" id="toc-sec-Chapter1" class="nav-link" data-scroll-target="#sec-Chapter1"><span class="header-section-number">1.1</span> Chapter 1: Experimental Design Preliminaries</a>
  <ul class="collapse">
  <li><a href="#sec-Chapter1.1" id="toc-sec-Chapter1.1" class="nav-link" data-scroll-target="#sec-Chapter1.1"><span class="header-section-number">1.1.1</span> Chapter 1.1: Setting up experiments</a></li>
  <li><a href="#exercise-1.1.1" id="toc-exercise-1.1.1" class="nav-link" data-scroll-target="#exercise-1.1.1"><span class="header-section-number">1.1.2</span> Exercise 1.1.1</a></li>
  <li><a href="#instructions" id="toc-instructions" class="nav-link" data-scroll-target="#instructions"><span class="header-section-number">1.1.3</span> Instructions</a></li>
  <li><a href="#exercise-1.1.2" id="toc-exercise-1.1.2" class="nav-link" data-scroll-target="#exercise-1.1.2"><span class="header-section-number">1.1.4</span> Exercise 1.1.2</a></li>
  <li><a href="#instructions-1" id="toc-instructions-1" class="nav-link" data-scroll-target="#instructions-1"><span class="header-section-number">1.1.5</span> Instructions</a></li>
  <li><a href="#sec-Chapter1.2" id="toc-sec-Chapter1.2" class="nav-link" data-scroll-target="#sec-Chapter1.2"><span class="header-section-number">1.1.6</span> Chapter 1.2: Experimental data setup</a></li>
  <li><a href="#exercise-1.2.1" id="toc-exercise-1.2.1" class="nav-link" data-scroll-target="#exercise-1.2.1"><span class="header-section-number">1.1.7</span> Exercise 1.2.1</a></li>
  <li><a href="#exercise-1.2.2" id="toc-exercise-1.2.2" class="nav-link" data-scroll-target="#exercise-1.2.2"><span class="header-section-number">1.1.8</span> Exercise 1.2.2</a></li>
  <li><a href="#sec-Chapter1.3" id="toc-sec-Chapter1.3" class="nav-link" data-scroll-target="#sec-Chapter1.3"><span class="header-section-number">1.1.9</span> Chapter 1.3: Normal data</a></li>
  <li><a href="#exercise-1.3.1" id="toc-exercise-1.3.1" class="nav-link" data-scroll-target="#exercise-1.3.1"><span class="header-section-number">1.1.10</span> Exercise 1.3.1</a></li>
  <li><a href="#exercise-1.3.2" id="toc-exercise-1.3.2" class="nav-link" data-scroll-target="#exercise-1.3.2"><span class="header-section-number">1.1.11</span> Exercise 1.3.2</a></li>
  </ul></li>
  <li><a href="#sec-Chapter2" id="toc-sec-Chapter2" class="nav-link" data-scroll-target="#sec-Chapter2"><span class="header-section-number">1.2</span> Chapter 2: Experimental Design Techniques</a>
  <ul class="collapse">
  <li><a href="#sec-Chapter2.1" id="toc-sec-Chapter2.1" class="nav-link" data-scroll-target="#sec-Chapter2.1"><span class="header-section-number">1.2.1</span> Chapter 2.1: Factorial designs: principles and applications</a></li>
  <li><a href="#exercise-2.1.1" id="toc-exercise-2.1.1" class="nav-link" data-scroll-target="#exercise-2.1.1"><span class="header-section-number">1.2.2</span> Exercise 2.1.1</a></li>
  <li><a href="#exercise-2.1.2" id="toc-exercise-2.1.2" class="nav-link" data-scroll-target="#exercise-2.1.2"><span class="header-section-number">1.2.3</span> Exercise 2.1.2</a></li>
  <li><a href="#sec-Chapter2.2" id="toc-sec-Chapter2.2" class="nav-link" data-scroll-target="#sec-Chapter2.2"><span class="header-section-number">1.2.4</span> Chapter 2.2: Randomized block design: controlling variance</a></li>
  <li><a href="#exercise-2.2.1" id="toc-exercise-2.2.1" class="nav-link" data-scroll-target="#exercise-2.2.1"><span class="header-section-number">1.2.5</span> Exercise 2.2.1</a></li>
  <li><a href="#exercise-2.2.2" id="toc-exercise-2.2.2" class="nav-link" data-scroll-target="#exercise-2.2.2"><span class="header-section-number">1.2.6</span> Exercise 2.2.2</a></li>
  <li><a href="#exercise-2.2.3" id="toc-exercise-2.2.3" class="nav-link" data-scroll-target="#exercise-2.2.3"><span class="header-section-number">1.2.7</span> Exercise 2.2.3</a></li>
  <li><a href="#sec-Chapter2.3" id="toc-sec-Chapter2.3" class="nav-link" data-scroll-target="#sec-Chapter2.3"><span class="header-section-number">1.2.8</span> Chapter 2.3: Covariate adjustment in experimental design</a></li>
  <li><a href="#exercise-2.3.1" id="toc-exercise-2.3.1" class="nav-link" data-scroll-target="#exercise-2.3.1"><span class="header-section-number">1.2.9</span> Exercise 2.3.1</a></li>
  </ul></li>
  <li><a href="#sec-Chapter3" id="toc-sec-Chapter3" class="nav-link" data-scroll-target="#sec-Chapter3"><span class="header-section-number">1.3</span> Chapter 3: Analyzing Experimental Data: Statistical Tests and Power</a>
  <ul class="collapse">
  <li><a href="#sec-Chapter3.1" id="toc-sec-Chapter3.1" class="nav-link" data-scroll-target="#sec-Chapter3.1"><span class="header-section-number">1.3.1</span> Chapter 3.1. Choosing the right statistical test</a></li>
  <li><a href="#exercise-3.1.1" id="toc-exercise-3.1.1" class="nav-link" data-scroll-target="#exercise-3.1.1"><span class="header-section-number">1.3.2</span> Exercise 3.1.1</a></li>
  <li><a href="#exercise-3.1.2" id="toc-exercise-3.1.2" class="nav-link" data-scroll-target="#exercise-3.1.2"><span class="header-section-number">1.3.3</span> Exercise 3.1.2</a></li>
  <li><a href="#exercise-3.1.3" id="toc-exercise-3.1.3" class="nav-link" data-scroll-target="#exercise-3.1.3"><span class="header-section-number">1.3.4</span> Exercise 3.1.3</a></li>
  <li><a href="#sec-Chapter3.2" id="toc-sec-Chapter3.2" class="nav-link" data-scroll-target="#sec-Chapter3.2"><span class="header-section-number">1.3.5</span> Chapter 3.2: Post-hoc analysis following ANOVA</a></li>
  <li><a href="#exercise-3.2.1" id="toc-exercise-3.2.1" class="nav-link" data-scroll-target="#exercise-3.2.1"><span class="header-section-number">1.3.6</span> Exercise 3.2.1</a></li>
  <li><a href="#exercise-3.2.2" id="toc-exercise-3.2.2" class="nav-link" data-scroll-target="#exercise-3.2.2"><span class="header-section-number">1.3.7</span> Exercise 3.2.2</a></li>
  <li><a href="#exercise-3.2.3" id="toc-exercise-3.2.3" class="nav-link" data-scroll-target="#exercise-3.2.3"><span class="header-section-number">1.3.8</span> Exercise 3.2.3</a></li>
  <li><a href="#sec-Chapter3.3" id="toc-sec-Chapter3.3" class="nav-link" data-scroll-target="#sec-Chapter3.3"><span class="header-section-number">1.3.9</span> Chapter 3.3: P-values, alpha, and errors</a></li>
  <li><a href="#exercise-3.3.1" id="toc-exercise-3.3.1" class="nav-link" data-scroll-target="#exercise-3.3.1"><span class="header-section-number">1.3.10</span> Exercise 3.3.1</a></li>
  <li><a href="#exercise-3.3.2" id="toc-exercise-3.3.2" class="nav-link" data-scroll-target="#exercise-3.3.2"><span class="header-section-number">1.3.11</span> Exercise 3.3.2</a></li>
  <li><a href="#sec-Chapter3.4" id="toc-sec-Chapter3.4" class="nav-link" data-scroll-target="#sec-Chapter3.4"><span class="header-section-number">1.3.12</span> Chapter 3.4: Power analysis: sample and effect size</a></li>
  <li><a href="#exercise-3.4.1" id="toc-exercise-3.4.1" class="nav-link" data-scroll-target="#exercise-3.4.1"><span class="header-section-number">1.3.13</span> Exercise 3.4.1</a></li>
  </ul></li>
  <li><a href="#chapter-4-advanced-insights-from-experimental-complexity" id="toc-chapter-4-advanced-insights-from-experimental-complexity" class="nav-link" data-scroll-target="#chapter-4-advanced-insights-from-experimental-complexity"><span class="header-section-number">1.4</span> Chapter 4: Advanced Insights from Experimental Complexity</a>
  <ul class="collapse">
  <li><a href="#chapter-4.1-synthesizing-insights-from-complex-experiments" id="toc-chapter-4.1-synthesizing-insights-from-complex-experiments" class="nav-link" data-scroll-target="#chapter-4.1-synthesizing-insights-from-complex-experiments"><span class="header-section-number">1.4.1</span> Chapter 4.1: Synthesizing insights from complex experiments</a></li>
  <li><a href="#exercise-4.1.1" id="toc-exercise-4.1.1" class="nav-link" data-scroll-target="#exercise-4.1.1"><span class="header-section-number">1.4.2</span> Exercise 4.1.1</a></li>
  <li><a href="#exercise-4.1.2" id="toc-exercise-4.1.2" class="nav-link" data-scroll-target="#exercise-4.1.2"><span class="header-section-number">1.4.3</span> Exercise 4.1.2</a></li>
  <li><a href="#chapter-4.2-addressing-complexities-in-experimental-data" id="toc-chapter-4.2-addressing-complexities-in-experimental-data" class="nav-link" data-scroll-target="#chapter-4.2-addressing-complexities-in-experimental-data"><span class="header-section-number">1.4.4</span> Chapter 4.2: Addressing complexities in experimental data</a></li>
  <li><a href="#exercise-4.2.1" id="toc-exercise-4.2.1" class="nav-link" data-scroll-target="#exercise-4.2.1"><span class="header-section-number">1.4.5</span> Exercise 4.2.1</a></li>
  <li><a href="#exercise-4.2.3" id="toc-exercise-4.2.3" class="nav-link" data-scroll-target="#exercise-4.2.3"><span class="header-section-number">1.4.6</span> Exercise 4.2.3</a></li>
  <li><a href="#chapter-4.3-applying-nonparametric-tests-in-experimental-analysis" id="toc-chapter-4.3-applying-nonparametric-tests-in-experimental-analysis" class="nav-link" data-scroll-target="#chapter-4.3-applying-nonparametric-tests-in-experimental-analysis"><span class="header-section-number">1.4.7</span> Chapter 4.3: Applying nonparametric tests in experimental analysis</a></li>
  <li><a href="#exercise-4.3.1" id="toc-exercise-4.3.1" class="nav-link" data-scroll-target="#exercise-4.3.1"><span class="header-section-number">1.4.8</span> Exercise 4.3.1</a></li>
  <li><a href="#exercise-4.3.2" id="toc-exercise-4.3.2" class="nav-link" data-scroll-target="#exercise-4.3.2"><span class="header-section-number">1.4.9</span> Exercise 4.3.2</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Experimental Design in Python</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Lawal’s Note </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Associate Data Science Course in Python by DataCamp Inc
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 2, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="SOA_Experi.jpg" class="img-fluid"></p>
<section id="experimental-design-in-python" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Experimental Design in Python</h1>
<section id="sec-Chapter1" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-Chapter1"><span class="header-section-number">1.1</span> Chapter 1: Experimental Design Preliminaries</h2>
<p>Building knowledge in experimental design allows you to test hypotheses with best-practice analytical tools and quantify the risk of your work. You’ll begin your journey by setting the foundations of what experimental design is and different experimental design setups such as blocking and stratification. You’ll then learn and apply visual and analytical tests for normality in experimental data.</p>
<section id="sec-Chapter1.1" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="sec-Chapter1.1"><span class="header-section-number">1.1.1</span> Chapter 1.1: Setting up experiments</h3>
<p>Hi! Welcome to this course on experimental design in Python.</p>
<section id="experimental-design-definition" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="experimental-design-definition">Experimental Design definition</h4>
<p>Experimental design is the process in which we carry out research in an objective and controlled fashion. The purpose of this is to ensure we can make specific conclusions in reference to a hypothesis we have.</p>
<p>1 <a href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/experimental-design">https://www.sciencedirect.com/topics/earth-and-planetary-sciences/experimental-design</a></p>
</section>
<section id="forming-robust-statements" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="forming-robust-statements">Forming robust statements</h4>
<p>Because we use objective tools, we need to use quantified language. Instead of using words like ‘probably’, ‘likely’, and ‘small’ when noting our conclusions, we should use precise and quantified language. This often takes the form of noting the percentage risk on a Type I error in the conclusion. Recall that Type I errors occur when we incorrectly reject the null hypothesis when it is actually true. In this course, you’ll learn to design experiments and conduct statistical analyses such that you begin making precise statements about observed results and take informed actions as a result.</p>
</section>
<section id="why-experimental-design" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="why-experimental-design">Why experimental design?</h4>
<p>Experimental design is useful in many fields. Naturally, it is used in academia such as in medical research. It is also useful in many corporate contexts such as marketing and product analytics, which conduct lots of A/B tests. It is also used in agriculture and increasingly in government policy through the use of behavioral psychology experiments.</p>
</section>
<section id="some-terminology" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="some-terminology">Some terminology…</h4>
<p>Before we begin our first topic, let’s define some important terminology. Subjects are what we are experimenting on. It could be people, employees, or users on a website.</p>
<p>A treatment is some change given to one group of subjects. We could call that group the treatment group. The control group is not given any change. This could be a placebo group, for example.</p>
</section>
<section id="assigning-subjects-to-groups" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="assigning-subjects-to-groups">Assigning subjects to groups</h4>
<p>An important concept in experimental design is how to assign subjects to test groups. There are two ways we could do this. We could just split the dataset non-randomly into chunks and assign each chunk to a group. Or we could use random assignment to sample into our desired groups. Let’s look at each option using a DataFrame of 200 subjects’ heights where we want to split into two groups of 100 each.</p>
</section>
<section id="non-random-assignment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="non-random-assignment">Non-random assignment</h4>
<p>Let’s try non-random assignment first. We can use <code>.iloc[]</code> to slice the first 100 rows from heights and assign to group1 and the next 100 rows into group2. We can use pandas’ describe method to check descriptive statistics of our groups. Concatenating the two results with <code>pd.concat()</code> and <code>axis=1</code> will allow for easier comparison. These groups appear very different! Looking at the mean row, we can see there’s a 9cm difference. Because of the differences in these groups, it will be harder to confidently determine if any changes are due to the treatment intervention.</p>
</section>
<section id="random-assignment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="random-assignment">Random assignment</h4>
<p>Let’s now try random assignment. We can use pandas’ sample method to create a sample of size n, or use the frac argument and specify a proportion of the dataset, between 0 and 1, to sample. We want two equally-sized groups, so we specify <code>frac=0.5</code>. Using <code>n=100</code> would also work here. We also set the replace argument to False, so samples aren’t selected twice. The random_state argument allows the splits to be consistently reproduced. group2 can be made by dropping the ids in group1 from the overall DataFrame. Using the same comparison method we see much closer means.</p>
</section>
<section id="assignment-summary" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="assignment-summary">Assignment summary</h4>
<p>This demonstrates the importance of randomly assigning subjects to groups. It means we can attribute observed changes to treatment interventions rather than natural differences between the group. We can use pandas’ sample method to select randomly from a DataFrame, and then use pandas’ describe method to check differences in group assignment.</p>
</section>
</section>
<section id="exercise-1.1.1" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="exercise-1.1.1"><span class="header-section-number">1.1.2</span> Exercise 1.1.1</h3>
<section id="non-random-assignment-of-subjects" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="non-random-assignment-of-subjects">Non-random assignment of subjects</h4>
<p>An agricultural firm is conducting an experiment to measure how feeding sheep different types of grass affects their weight. They have asked for your help to properly set up the experiment. One of their managers has said you can perform the subject assignment by taking the top 250 rows from the DataFrame and that should be fine.</p>
<p>Your task is to use your analytical skills to demonstrate why this might not be a good idea. Assign the subjects to two groups using non-random assignment (the first 250 rows) and observe the differences in descriptive statistics.</p>
<p>You have received the DataFrame, <code>weights</code> which has a column containing the <code>weight</code> of the sheep and a unique <code>id</code> column.</p>
</section>
</section>
<section id="instructions" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="instructions"><span class="header-section-number">1.1.3</span> Instructions</h3>
<ul>
<li><p>Use DataFrame slicing to put the first 250 rows of <code>weights</code> into <code>group1_non_rand</code> and the remaining into <code>group2_non_rand</code>.</p></li>
<li><p>Generate descriptive statistics of the two groups and concatenate them into a single DataFrame.</p></li>
<li><p>Print out to observe the differences.</p></li>
</ul>
<p>Note: Due to non-availability of weights DataFrame, I had to generates mine myself, the proper code should have been this below.</p>
<pre><code># Non-random assignment
group1_non_rand = weights.iloc[0:250, :]
group2_non_rand = weights.iloc[250:, :]

# Compare descriptive statistics of groups
compare_df_non_rand = pd.concat([group1_non_rand['weight'].describe(), group2_non_rand['weight'].describe()], axis=1)
compare_df_non_rand.columns = ['group1', 'group2']

# Print to assess
print(compare_df_non_rand)</code></pre>
<div id="9c365e3d" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group1 between 39.07 and 65.10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>weights_group1 <span class="op">=</span> np.random.uniform(<span class="fl">39.07</span>, <span class="fl">65.10</span>, <span class="dv">250</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group2 between 65.10 and 95.82</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>weights_group2 <span class="op">=</span> np.random.uniform(<span class="fl">65.10</span>, <span class="fl">95.82</span>, <span class="dv">250</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrames for each group</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>group1_non_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">251</span>),</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group1</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>group2_non_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">251</span>, <span class="dv">501</span>),</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group2</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the two groups into one DataFrame</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.concat([group1_non_rand, group2_non_rand]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare descriptive statistics of groups</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>compare_df_non_rand <span class="op">=</span> pd.concat([group1_non_rand[<span class="st">'weight'</span>].describe(), group2_non_rand[<span class="st">'weight'</span>].describe()], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>compare_df_non_rand.columns <span class="op">=</span> [<span class="st">'group1'</span>, <span class="st">'group2'</span>]</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Print to assess</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(compare_df_non_rand)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           group1      group2
count  250.000000  250.000000
mean    52.881443   80.928000
std      7.381485    8.988698
min     39.165461   65.224213
25%     47.169705   73.901509
50%     53.194238   80.949582
75%     58.828407   89.198278
max     65.097701   95.396556</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>Wow! Those two datasets have a much greater difference in means. It may be that the dataset was sorted before you received it. Presenting these results to the firm will help them understand best-practice group assignment. Hopefully you can now work with them to set up the experiment properly.</em></p>
</div>
</div>
</div>
</section>
<section id="exercise-1.1.2" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="exercise-1.1.2"><span class="header-section-number">1.1.4</span> Exercise 1.1.2</h3>
<section id="random-assignment-of-subjects" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="random-assignment-of-subjects">Random assignment of subjects</h4>
<p>Having built trust from your last work with the agricultural firm, you have been given the task of properly setting up the experiment.</p>
<p>Use your knowledge of best practice experimental design set up to assign the sheep to two even groups of 250 each.</p>
</section>
</section>
<section id="instructions-1" class="level3" data-number="1.1.5">
<h3 data-number="1.1.5" class="anchored" data-anchor-id="instructions-1"><span class="header-section-number">1.1.5</span> Instructions</h3>
<ul>
<li><p>Randomly select 250 subjects from the <code>weights</code> DataFrame into a new DataFrame <code>group1</code> without replacement.</p></li>
<li><p>Put the remaining 250 subjects into <code>group2</code>.</p></li>
<li><p>Concatenate the descriptive statistics of your two newly created DataFrames.</p></li>
</ul>
<div id="17de100b" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group1 between 39.07 and 65.10</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>weights_group1 <span class="op">=</span> np.random.uniform(<span class="fl">39.07</span>, <span class="fl">65.10</span>, <span class="dv">250</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group2 between 65.10 and 95.82</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>weights_group2 <span class="op">=</span> np.random.uniform(<span class="fl">65.10</span>, <span class="fl">95.82</span>, <span class="dv">250</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrames for each group</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>group1_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">251</span>),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group1</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>group2_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">251</span>, <span class="dv">501</span>),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group2</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the two groups into one DataFrame</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.concat([group1_rand, group2_rand]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign half</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>group1_random <span class="op">=</span> weights.sample(frac<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create second assignment</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>group2_random <span class="op">=</span> weights.drop(group1_random.index)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare assignments</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>compare_df_random <span class="op">=</span> pd.concat([group1_random[<span class="st">'weight'</span>].describe(), group2_random[<span class="st">'weight'</span>].describe()], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>compare_df_random.columns <span class="op">=</span> [<span class="st">'group1'</span>, <span class="st">'group2'</span>]</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(compare_df_random)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           group1      group2
count  250.000000  250.000000
mean    64.451074   67.521437
std     16.515149   16.942613
min     39.165751   39.231547
25%     49.537486   53.044788
50%     64.135085   67.412082
75%     77.782505   81.534922
max     95.285790   95.738079</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>While there are some differences in these datasets, you can clearly see the mean of the two sets are very close. This best-practice setup will ensure the experiment is on the right path from the beginning. Let’s continue building foundational experimental design skills by learning about experimental design setup.</em></p>
</div>
</div>
</div>
</section>
<section id="sec-Chapter1.2" class="level3" data-number="1.1.6">
<h3 data-number="1.1.6" class="anchored" data-anchor-id="sec-Chapter1.2"><span class="header-section-number">1.1.6</span> Chapter 1.2: Experimental data setup</h3>
<p>We’ve seen that randomization is often the best technique for setting up experimental data, but it isn’t always.</p>
<section id="the-problem-with-randomization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-problem-with-randomization">The problem with randomization</h4>
<p>There are several scenarios where pure randomization can lead to undesirable outcomes. Firstly, when it results in uneven numbers of subjects in different groups, often seen more in smaller experiment sizes.</p>
<p>Covariates are variables that potentially affect experiment results but aren’t the primary focus. If covariates are highly variable or not equally distributed among groups, randomization might not produce balanced groups. This imbalance can lead to biased results. Overall these make it harder to see an effect from a treatment, as these issues may be driving an observed change.</p>
</section>
<section id="block-randomization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="block-randomization">Block randomization</h4>
<p>A solution to our uneven problem is block randomization. This involves splitting into a block of size n first, then randomly splitting. This is what it looks like. Subjects are split into two groups, then randomly assigned to be Treatment (orange) or control (white). This fixes the uneven issue, and the smaller blocks give us more control over the allocation.</p>
</section>
<section id="our-dataset" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="our-dataset">Our dataset</h4>
<p>Let’s give block randomization a go on a dataset of 1000 members from an e-commerce site that contains variables for their average basket size in dollars, the average time spent on the website each day, and whether they are a power user. Power users spend an average of 40+ minutes on the website each day. There are 100 power users in these 1000 subjects.</p>
</section>
<section id="block-randomization-in-python" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="block-randomization-in-python">Block randomization in Python</h4>
<p>We can use pandas’ sample method to randomly assign subjects into two blocks. A block column has also been added to both DataFrames for convenience. This produces even block sizes, fixing the uneven issue, but let’s check for covariates.</p>
</section>
<section id="visualizing-splits" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-splits">Visualizing splits</h4>
<p>A nice way of checking for potential covariate issues is with visualizations. We can use seaborn’s displot function to produce a kde (or kernal density) plot to visualize the distribution of the basket size, split by whether the user is a power user. There is quite a difference in the group distributions. It seems like the power_user variable could have an effect on basket size. When an effect could be because of a variable rather than the treatment, this is often called confounding. The covariate issue can be solved with stratified randomization.</p>
</section>
<section id="stratified-randomization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stratified-randomization">Stratified randomization</h4>
<p>Stratified randomization involves splitting based on a potentially confounding variable first, followed by randomization. This is what it may look like. Firstly, we split into two blocks (sometimes called strata) of power users, in green, and non-power users, in yellow. Then, inside the groups, randomly allocating to treatment or control. This fixes the uneven covariate issue, and can even be done for multiple covariates, but managing more strata does increase complexity.</p>
</section>
<section id="our-first-strata" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="our-first-strata">Our first strata</h4>
<p>Let’s stratify our power users. We separate them out first and label the block. We then sample half the power users to be in Treatment. The <code>T_C</code>Let’s stratify our power users. We separate them out first and label the block. We then sample half the power users to be in Treatment. The <code>T_C</code> column notes this status. We then place the remaining into control by dropping the subjects in the treatment group. column notes this status. We then place the remaining into control by dropping the subjects in the treatment group.</p>
</section>
<section id="the-second-strata" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-second-strata">The second strata</h4>
<p>For our other strata, we separate out non-power users first and label the block differently. The rest of the code is the same as before. We allocate half to treatment and control using the same column headers.</p>
</section>
<section id="confirming-stratification" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confirming-stratification">Confirming stratification</h4>
<p>Let’s bring our work together by firstly concatenating the strata and groups. We can confirm our work using groupby and chaining the <code>.size()</code> method. This will show the number of power users in each block by their treatment or control status. We can see two blocks: one with all 100 power users and another with the other 900 users, split evenly into treatment and control groups.</p>
</section>
</section>
<section id="exercise-1.2.1" class="level3" data-number="1.1.7">
<h3 data-number="1.1.7" class="anchored" data-anchor-id="exercise-1.2.1"><span class="header-section-number">1.1.7</span> Exercise 1.2.1</h3>
<section id="blocking-experimental-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="blocking-experimental-data">Blocking experimental data</h4>
<p>You are working with a manufacturing firm that wants to conduct some experiments on worker productivity. Their dataset only contains 100 rows, so it’s important that experimental groups are balanced.</p>
<p>This sounds like a great opportunity to use your knowledge of blocking to assist them. They have provided a <code>productivity_subjects</code> DataFrame. Split the provided dataset into two even groups of 50 entries each.</p>
</section>
<section id="instructions-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-2">Instructions</h4>
<ul>
<li><p>Randomly select 50 subjects from the <code>productivity_subjects</code> DataFrame into a new DataFrame <code>block_1</code> without replacement.</p></li>
<li><p>Set a new column, <code>block</code> to 1 for the <code>block_1</code> DataFrame.</p></li>
<li><p>Assign the remaining subjects to a DataFrame called <code>block_2</code> and set the <code>block</code> column to 2 for this DataFrame.</p></li>
<li><p>Concatenate the blocks together into a single DataFrame, and print the count of each value in the <code>block</code> column to confirm the blocking worked.</p></li>
</ul>
<div id="e686a497" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame with 100 rows and subject_id ranging from 1 to 100</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>productivity_subjects <span class="op">=</span> pd.DataFrame({<span class="st">'subject_id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>)})</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign half</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>block_1 <span class="op">=</span> productivity_subjects.sample(frac<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the block column</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>block_1[<span class="st">'block'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create second assignment and label</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>block_2 <span class="op">=</span> productivity_subjects.drop(block_1.index)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>block_2[<span class="st">'block'</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate and print</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>productivity_combined <span class="op">=</span> pd.concat([block_1, block_2], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(productivity_combined[<span class="st">'block'</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>block
1    50
2    50
Name: count, dtype: int64</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-1.2.2" class="level3" data-number="1.1.8">
<h3 data-number="1.1.8" class="anchored" data-anchor-id="exercise-1.2.2"><span class="header-section-number">1.1.8</span> Exercise 1.2.2</h3>
<section id="stratifying-an-experiment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stratifying-an-experiment">Stratifying an experiment</h4>
<p>You are working with a government organization that wants to undertake an experiment around how some particular government policies impact the net wealth of individuals in a number of areas.</p>
<p>They have approached you to help set up the experimental design. They have warned you that there is likely to be a small group of users who already have high net wealth and are concerned that this group might overshadow any experimental outcome observed. You know just what to do!</p>
<p>Use your knowledge of experimental design to undertake block randomization, stratifying by the <code>high_wealth</code> column in the provided <code>wealth_data</code> DataFrame. There are 2000 rows in the DataFrame with 200 high net wealth subjects (<code>high_wealth</code> is 1).</p>
</section>
<section id="instructions-13" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-13">Instructions 1/3</h4>
<ol type="1">
<li>Create the first block which contains all the <code>high_wealth</code> subjects and set the <code>Block</code> column to 1.</li>
</ol>
<ul>
<li>Create two groups from this block randomly assigning the <code>high_wealth</code> subjects to the Treatment (T) or control (C) group.</li>
</ul>
<ol start="2" type="1">
<li><p>Repeat for the second block (all the not <code>high_wealth</code> subjects), setting the <code>Block</code> column to 2, and perform the group assignment (randomly assigning to Treatment (T) or control (C) group).</p></li>
<li><p>Concatenate the four groups created into <code>wealth_data_stratified</code> in order of creation (strata 1 group 1, strata 1 group 2, etc.)</p></li>
</ol>
<div id="fbaa9067" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of rows</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>n_rows <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of high net wealth subjects</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>n_high_wealth <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the 'high_wealth' column</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>high_wealth <span class="op">=</span> np.zeros(n_rows, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>high_wealth[:n_high_wealth] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the 'high_wealth' column</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(high_wealth)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>wealth_data <span class="op">=</span> pd.DataFrame({<span class="st">'high_wealth'</span>: high_wealth})</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the first block</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>strata_1 <span class="op">=</span> wealth_data[wealth_data[<span class="st">'high_wealth'</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>strata_1[<span class="st">'Block'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create two groups assigning to Treatment or Control</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>strata_1_g1 <span class="op">=</span> strata_1.sample(frac<span class="op">=</span><span class="fl">0.5</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>strata_1_g1[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'T'</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>strata_1_g2 <span class="op">=</span> strata_1.drop(strata_1_g1.index)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>strata_1_g2[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'C'</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the second block and assign groups</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>strata_2 <span class="op">=</span> wealth_data[wealth_data[<span class="st">'high_wealth'</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>strata_2[<span class="st">'Block'</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>strata_2_g1 <span class="op">=</span> strata_2.sample(frac<span class="op">=</span><span class="fl">0.5</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>strata_2_g1[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'T'</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>strata_2_g2 <span class="op">=</span> strata_2.drop(strata_2_g1.index)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>strata_2_g2[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'C'</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the grouping work</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>wealth_data_stratified <span class="op">=</span> pd.concat([strata_1_g1, strata_1_g2, strata_2_g1, strata_2_g2])</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wealth_data_stratified.groupby([<span class="st">'Block'</span>,<span class="st">'T_C'</span>, <span class="st">'high_wealth'</span>]).size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Block  T_C  high_wealth
1      C    1              100
       T    1              100
2      C    0              900
       T    0              900
dtype: int64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>You were able to split your data into different blocks and then randomly assign to treatment and control. You can clearly see two blocks, where the first block has half the <code>high_wealth</code> subjects split into treatment and control. The same is seen in the second block for the other subjects</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-Chapter1.3" class="level3" data-number="1.1.9">
<h3 data-number="1.1.9" class="anchored" data-anchor-id="sec-Chapter1.3"><span class="header-section-number">1.1.9</span> Chapter 1.3: Normal data</h3>
<p>Let’s review the concept of normal data and how it relates to experimental analysis.</p>
<section id="the-normal-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-normal-distribution">The normal distribution</h4>
<p>Normal data is drawn from a normal distribution, which has the familiar bell curve shape. The normal distribution is intrinsically linked to z-scores, which recall, is a standardized measure of how many standard deviations a value is from the population mean. The most common normal distribution used for z-scores has a mean of zero and a standard deviation of one. This answers questions such as ‘How many standard deviations is this point from the mean?’ and ‘What is the probability of obtaining this score?’.</p>
</section>
<section id="normal-data-and-statistical-tests" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="normal-data-and-statistical-tests">Normal data and statistical tests</h4>
<p>Normal data is an underlying assumption for many statistical tests, called parametric tests. There are also nonparametric tests that don’t assume normal data.</p>
</section>
<section id="normal-z-and-alpha" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="normal-z-and-alpha">Normal, Z, and alpha</h4>
<p>In hypothesis testing, alpha, or the significance level, is often closely linked to the normal distribution. For normal data, we can visually see the risk of error for a given significance level and compare that result to the p-value, which is related to the z-score. An <code>alpha</code> of 0.05 on a standard two-tailed test represents a small region in the tails. It means there is a 5% risk of rejecting the null hypothesis when it is actually true - a so-called Type I error.</p>
</section>
<section id="visualizing-normal-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-normal-data">Visualizing normal data</h4>
<p>We can visually check data for normality using a kde (or kernel density) plot, available via Seaborn’s <code>displot()</code> function. On this salaries dataset, the data appears approximately normal.</p>
</section>
<section id="qq-plots" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qq-plots">QQ plots</h4>
<p>A more statistically robust visual tool is a quantile-quantile, or QQ, plot. It plots the quantiles or sections of two distributions against each other. The qqplot function from statsmodels plots our data. Setting the dist argument to the normal distribution from scipy.stats compares our data against a standard normal distribution. If the distributions are similar, the dots in the QQ plot hug the line tightly. Our data again seems quite normal. Here is another example. The dots bow out at the ends, which means that the data is not very normal.</p>
</section>
<section id="tests-for-normality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="tests-for-normality">Tests for normality</h4>
<p>There are also various numerical hypothesis tests for normality. The Shapiro-Wilk test is known to be good for small datasets. The D’Agostino K-squared test uses kurtosis and skewness to determine normality. These terms relate to the symmetry and size of a distribution’s tails, respectively. Anderson-Darling is another common test which returns a list of values, rather than just one so we can see normality at different levels of alpha. Each of these tests has a null hypothesis that the provided dataset is drawn from a normal distribution.</p>
</section>
<section id="a-shapiro-wilk-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-shapiro-wilk-test">A Shapiro-Wilk test</h4>
<p>Let’s run one of these tests, the Shapiro-Wilk test. We import it from <code>scipy.stats</code>, and set our <code>alpha</code> at 0.05. The function takes a series of values and returns a test statistic and <code>p-value</code>. The <code>p-value</code> is greater than <code>alpha</code>, so we have evidence our data that looked quite normal is normal. We fail to reject the null hypothesis and have evidence that the data sample is normal at the <code>alpha</code> level of 0.05.</p>
</section>
<section id="an-anderson-darling-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="an-anderson-darling-test">An Anderson-Darling test</h4>
<p>To implement an Anderson-Darling test, we provide data and set the dist argument to norm to test for normality. The result object contains a test statistic and a range of critical values and significance levels. To interpret, we check the test statistic against each critical value. If the test statistic is higher than the critical value, the null hypothesis is rejected at that particular significance level, and the data is not normal. 0.2748 is less than all the critical values, so we fail to reject the null hypothesis and suspect that the data is normal.</p>
</section>
</section>
<section id="exercise-1.3.1" class="level3" data-number="1.1.10">
<h3 data-number="1.1.10" class="anchored" data-anchor-id="exercise-1.3.1"><span class="header-section-number">1.1.10</span> Exercise 1.3.1</h3>
<section id="visual-normality-in-an-agricultural-experiment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visual-normality-in-an-agricultural-experiment">Visual normality in an agricultural experiment</h4>
<p>You have been contracted by an agricultural firm conducting an experiment on 50 chickens, divided into four groups, each fed a different diet. Weight measurements were taken every second day for 20 days.</p>
<p>You’ll analyze chicken_data to assess normality, which will determine the suitability of parametric statistical tests, beginning with a visual examination of the data distribution.</p>
</section>
<section id="instructions-3" class="level4" data-number="1.1.10.1">
<h4 data-number="1.1.10.1" class="anchored" data-anchor-id="instructions-3"><span class="header-section-number">1.1.10.1</span> Instructions</h4>
<ol type="1">
<li><p>Plot the distribution of the chickens’ <code>weight</code> using the kernel density estimation (KDE) to visualize normality.</p></li>
<li><p>Create a qq plot with a standard line of the chickens’ <code>weight</code> to assess normality visually.</p></li>
<li><p>Subset <code>chicken_data</code> for a <code>'Time'</code> of 2, and plot the KDE of <code>'weight'</code> from <code>subset_data</code> to check if data is normal across time.</p></li>
</ol>
<div id="ea1a1ac9" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.gofplots <span class="im">import</span> qqplot</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>chicken_data <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chick_weight.csv'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of the chickens' weight</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>chicken_data, x<span class="op">=</span><span class="st">'weight'</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the qq plot of the chickens' weight</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>qqplot(data<span class="op">=</span>chicken_data[<span class="st">'weight'</span>], line<span class="op">=</span><span class="st">'s'</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset the data and plot the weight of the subset</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>subset_data <span class="op">=</span> chicken_data[chicken_data[<span class="st">'Time'</span>] <span class="op">==</span> <span class="dv">2</span>]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>subset_data, x<span class="op">=</span><span class="st">'weight'</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-2.png" width="605" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-3.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-1.3.2" class="level3" data-number="1.1.11">
<h3 data-number="1.1.11" class="anchored" data-anchor-id="exercise-1.3.2"><span class="header-section-number">1.1.11</span> Exercise 1.3.2</h3>
<section id="analytical-normality-in-an-agricultural-experiment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="analytical-normality-in-an-agricultural-experiment">Analytical normality in an agricultural experiment</h4>
<p>Carrying on from your previous work, your visual inspections of the data indicate it may not be a normal dataset overall, but that the initial time point may be.</p>
<p>Build on your previous work by using analytical methods to determine the normality of the dataset.</p>
</section>
<section id="instructions-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-4">Instructions</h4>
<ol type="1">
<li><p>Run a Shapiro-Wilk test of normality on the <code>'weight'</code> column and print the test statistic and p-value.</p></li>
<li><p>Run an Anderson-Darling test for normality and print out the test statistic, significance levels, and critical values from the returned object.</p></li>
</ol>
<div id="42532b8d" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> shapiro</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> anderson</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>chicken_data <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chick_weight.csv'</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a Shapiro-Wilk normality test on the weight column</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>test_statistic, p_value <span class="op">=</span> shapiro(chicken_data[<span class="st">'weight'</span>])</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p: </span><span class="sc">{</span><span class="bu">round</span>(p_value, <span class="dv">4</span>)<span class="sc">}</span><span class="ss"> test stat: </span><span class="sc">{</span><span class="bu">round</span>(test_statistic, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the Anderson-Darling test</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> anderson(x<span class="op">=</span> chicken_data[<span class="st">'weight'</span>], dist<span class="op">=</span><span class="st">'norm'</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test statistic: </span><span class="sc">{</span><span class="bu">round</span>(result.statistic, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Significance Levels: </span><span class="sc">{</span>result<span class="sc">.</span>significance_level<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Critical Values: </span><span class="sc">{</span>result<span class="sc">.</span>critical_values<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>p: 0.0 test stat: 0.9087
Test statistic: 14.6868
Significance Levels: [15.  10.   5.   2.5  1. ]
Critical Values: [0.572 0.652 0.782 0.912 1.085]</code></pre>
</div>
</div>
</section>
<section id="question-2" class="level4" data-number="1.1.11.1">
<h4 data-number="1.1.11.1" class="anchored" data-anchor-id="question-2"><span class="header-section-number">1.1.11.1</span> Question 2</h4>
<p>At a significance level of 0.05, does the Shapiro-Wilk test indicate the data is normally distributed? <em>No</em></p>
</section>
<section id="question-4" class="level4" data-number="1.1.11.2">
<h4 data-number="1.1.11.2" class="anchored" data-anchor-id="question-4"><span class="header-section-number">1.1.11.2</span> Question 4</h4>
<p>Given the returned Anderson-Darling test result, what could you conclude at the 5% significance level</p>
<ul>
<li><em>The critical value which matches the significance level of 5 is 0.781. When compared to the Anderson-Darling test statistic (12.5451), the critical value is much smaller and so we reject the null hypothesis and can conclude the data is unlikely to have been drawn from a normal distribution.</em></li>
</ul>
</section>
</section>
</section>
<section id="sec-Chapter2" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-Chapter2"><span class="header-section-number">1.2</span> Chapter 2: Experimental Design Techniques</h2>
<p>You’ll delve into sophisticated experimental design techniques, focusing on factorial designs, randomized block designs, and covariate adjustments. These methodologies are instrumental in enhancing the accuracy, efficiency, and interpretability of experimental results. Through a combination of theoretical insights and practical applications, you’ll acquire the skills needed to design, implement, and analyze complex experiments in various fields of research.</p>
<section id="sec-Chapter2.1" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="sec-Chapter2.1"><span class="header-section-number">1.2.1</span> Chapter 2.1: Factorial designs: principles and applications</h3>
<p>Welcome back! In this lesson, we’ll explore factorial designs.</p>
<section id="understanding-factorial-design" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-factorial-design">Understanding factorial design</h4>
<p>Factorial designs allow for the simultaneous examination of multiple variables. In this setup, every possible combination of factor levels is tested, which not only measures the direct effects of each factor but also the interactions between them. In the example shown of plant growth in different conditions, implementing a factorial design will mean that we can test the effect of different factors on plant growth, including light conditions and fertilizer type, simultaneously, and identify interactions between them. These interactions can illuminate complex dynamics that might be overlooked in simpler experimental setups.</p>
<p>1 Image Generated with DALL·E 3</p>
</section>
<section id="factorial-design-data-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="factorial-design-data-example">Factorial design data example</h4>
<p>To explain this concept further, we’ll work with this plant growth DataFrame. It has 120 rows and four columns: an identifier column, two factors, and one response/dependent variable. Both factors have two levels: <code>Light_Condition</code> can be Full Sunlight or Partial Shade, and <code>Fertilizer_Type</code> can be either Synthetic or Organic. The <code>Growth_cm</code> column is the numeric response, or dependent variable in the experiment.</p>
</section>
<section id="organizing-data-to-visualize-interactions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="organizing-data-to-visualize-interactions">Organizing data to visualize interactions</h4>
<p>We next create a pivot table from the DataFrame using pandas’ <code>pivot_table</code> function. It aggregates the <code>Growth_cm</code> values by taking their mean for each combination of <code>Light_Condition</code> and <code>Fertilizer_Type</code>. The resulting table displays these average outcomes, with light values as rows and fertilizer values as columns, illustrating how the growth varies across different levels of the two factors. For example, the value 19.869 represents the average growth for the combination of Full Sunlight from <code>Light_Condition</code> and Synthetic from <code>Fertilizer_Type</code>.</p>
</section>
<section id="visualize-interactions-with-heatmap" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualize-interactions-with-heatmap">Visualize interactions with heatmap</h4>
<p>The Seaborn heatmap function paints a picture of how these factors interact, with the color intensity revealing the strength and direction of their interactions. Setting annot to True displays the numerical value of the cell, and ‘coolwarm’ is a color map that ranges from cooler, or bluer colors, to warmer or redder colors. Lastly, the format argument fmt is set to ‘g’ to avoid scientific notation.</p>
</section>
<section id="interpreting-interactions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interpreting-interactions">Interpreting interactions</h4>
<p>The variation in outcomes when changing levels within a factor indicates an interaction. For instance, the decrease from Organic to Synthetic fertilizer within Full Sunlight (from 20.602 to 19.869) contrasts with the modest change within Partial Shade, illustrating how outcomes differ based on factor levels. The differing changes in outcomes between Full Sunlight and Partial Shade across <code>Fertilizer_Type</code> suggest the factors interact, underscoring the need for nuanced strategies considering the interaction of factors.</p>
</section>
<section id="factorial-designs-vs.-randomized-block-designs" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="factorial-designs-vs.-randomized-block-designs">Factorial designs vs.&nbsp;randomized block designs</h4>
<p>Let’s conclude by comparing factorial designs to the randomized block design we saw earlier in the course, and that we’ll dive deeper into in the next video. Factorial designs investigate multiple treatments and their interactions to understand their combined effects on outcomes. They aim to unravel the effects and interactions of various factors, crucial for complex scenarios with multiple influencing variables. In factorial designs, units experience all treatment combinations, offering thorough exploration but requiring more subjects as treatments grow. Randomized block designs utilize blocks to group similar subjects, minimizing confounding impacts and clearer treatment effects. The focus of randomized block designs is on enhancing experimental precision by managing within-block variability, aiding in the detection of treatment differences. Randomized block designs assign one treatment per unit within blocks, ensuring each treatment’s presence in every block to control for block-related variance and bolster treatment effect assessments.</p>
</section>
</section>
<section id="exercise-2.1.1" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="exercise-2.1.1"><span class="header-section-number">1.2.2</span> Exercise 2.1.1</h3>
<section id="understanding-marketing-campaign-effectiveness" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-marketing-campaign-effectiveness">Understanding marketing campaign effectiveness</h4>
<p>Imagine you’re a digital marketer analyzing data from a recent campaign to understand what messaging style and time of day yield the highest conversions. This analysis is crucial for guiding future marketing strategies, ensuring that your messages reach potential customers when they’re most likely to engage. In this exercise, you’re working with a dataset giving the outcomes of different messaging styles (<code>'Casual'</code> versus <code>'Formal'</code>) and times of day (<code>'Morning'</code> versus <code>'Evening'</code>) on conversion rates, a common scenario in marketing data analysis.</p>
</section>
<section id="instructions-5" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-5">Instructions</h4>
<ul>
<li>Create a pivot table with <code>'Messaging_Style'</code> as the index and <code>'Time_of_Day'</code> as the columns, computing the mean of Conversions.</li>
<li>Print this pivot table.</li>
</ul>
<div id="8c70df7d" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>marketing_data <span class="op">=</span> pd.read_feather(<span class="st">'datasets/marketing_data.feather'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pivot table for marketing campaign data</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>marketing_pivot <span class="op">=</span> marketing_data.pivot_table(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  values<span class="op">=</span><span class="st">'Conversions'</span>, </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  index<span class="op">=</span><span class="st">'Messaging_Style'</span>, </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  columns<span class="op">=</span><span class="st">'Time_of_Day'</span>, </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  aggfunc<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># View the pivoted results</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(marketing_pivot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Time_of_Day         Evening     Morning
Messaging_Style                        
Casual           402.329004  401.133891
Formal           432.913043  411.096000</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>!Notice that the mean conversion is highest for the</strong> <code>'Formal'</code> <strong>messaging style in the</strong> <code>'Evening'</code> <strong>time of day.</strong></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2.1.2" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="exercise-2.1.2"><span class="header-section-number">1.2.3</span> Exercise 2.1.2</h3>
<section id="heatmap-of-campaign-interactions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="heatmap-of-campaign-interactions">Heatmap of campaign interactions</h4>
<p>Visualizing data can often reveal patterns that are not immediately obvious. In the context of marketing, understanding how different factors interact and affect the success of a campaign is vital. By creating a heatmap of conversions based on messaging style and time of day, you can quickly identify which combinations perform best and which ones need reevaluation. This visual tool is invaluable for marketing teams looking to optimize their strategies for maximum impact.</p>
</section>
<section id="instructions-6" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-6">Instructions</h4>
<ul>
<li>Visualize interactions between <code>Messaging_Style</code> and <code>Time_of_Day</code> with respect to conversions by creating an annotated cool-warm heatmap of <code>marketing_pivot</code>.</li>
</ul>
<div id="d0f6b108" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>marketing_data <span class="op">=</span> pd.read_feather(<span class="st">'datasets/marketing_data.feather'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pivot table for marketing campaign data</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>marketing_pivot <span class="op">=</span> marketing_data.pivot_table(</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  values<span class="op">=</span><span class="st">'Conversions'</span>, </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  index<span class="op">=</span><span class="st">'Messaging_Style'</span>, </span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>  columns<span class="op">=</span><span class="st">'Time_of_Day'</span>, </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  aggfunc<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># View the pivoted results</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(marketing_pivot)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize interactions with a heatmap</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>sns.heatmap(marketing_pivot, </span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>         annot<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>         cmap<span class="op">=</span><span class="st">'coolwarm'</span>,</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>         fmt<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Time_of_Day         Evening     Morning
Messaging_Style                        
Casual           402.329004  401.133891
Formal           432.913043  411.096000</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-2.png" width="556" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-Chapter2.2" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="sec-Chapter2.2"><span class="header-section-number">1.2.4</span> Chapter 2.2: Randomized block design: controlling variance</h3>
<p>Next, we’ll delve further into the concept of blocking in experimental design.</p>
<section id="understanding-blocking" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-blocking">Understanding blocking</h4>
<p>Blocking involves grouping experimental units, often with similar characteristics, to minimize variance within these groups. This ensures that each block, representing a specific level of the blocking factor, receives every treatment. This approach allows us to concentrate on the treatment effects while controlling for variance attributable to the blocking factor, thus improving the precision of our results.</p>
</section>
<section id="block-design-data-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="block-design-data-example">Block design data example</h4>
<p>For this athlete performance DataFrame of 200 rows, blocking is represented by <code>Initial_Fitness_Level</code> with categories of <code>Beginner</code>, <code>Intermediate</code>, and <code>Advanced</code>. <code>Muscle_Gain_kg</code> is a numeric response variable measured on participants for the year prior to blocks being assigned.</p>
</section>
<section id="implementing-randomized-block-design" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="implementing-randomized-block-design">Implementing randomized block design</h4>
<p>To implement a randomized block design, we’ll group the rows into blocks based on the <code>Initial_Fitness_Level</code> in this case, shuffle the rows within these blocks, and randomly assign a treatment. To shuffle the rows in each <code>Initial_Fitness_Level</code> block, we start with <code>.groupby()</code> on <code>Initial_Fitness_Level</code>. To shuffle each row in that block, we chain the <code>.apply()</code> method to the <code>groupby</code>, and pass it a <code>lambda</code> function that reads: <code>for each group</code>, denoted by x, we sample all rows with <code>frac=1</code>, effectively shuffling them. We reset the index to not have both an index and column called Block. The grouped data is ordered alphabetically by fitness level.</p>
</section>
<section id="implemented-randomized-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="implemented-randomized-blocks">Implemented randomized blocks</h4>
<p>Then, within each block, we assign exercise program treatments randomly using <code>numpy.random.choice()</code>. This method allows us to control for block effects while focusing on the differences caused by the treatments. Here is a sample of the implemented randomized block DataFrame with the treatment randomly applied within each block.</p>
</section>
<section id="visualizing-treatment-effects-within-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-treatment-effects-within-blocks">Visualizing treatment effects within blocks</h4>
<p>A boxplot is an effective tool for visualizing the distribution of treatment effects across different blocks. By plotting the <code>Muscle_Gain_kg</code> variable versus the <code>Initial_Fitness_Level</code>, coloring by <code>Treatment</code>, we observe the central tendencies and variabilities within each block. Scanning this boxplot, we see similar median values throughout the blocks and treatments. The variability is a bit wider for some, though, such as Cardio for Advanced and Beginner.</p>
</section>
<section id="anova-within-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="anova-within-blocks">ANOVA within blocks</h4>
<p>We can use ANOVA to statistically check for these differences. Let’s set a significance level at 5% prior to reviewing our results. We group the DataFrame by the blocking column and then apply a <code>lambda</code> function to each group. Within the lambda function, we perform a one-way ANOVA test between the <code>Muscle_Gain_kg</code> values for each treatment within each block using <code>f_oneway</code> from <code>scipy.stats</code>. Finally, it returns the <code>F-statistic</code> and <code>p-value</code> for each block’s ANOVA test. Each of the p-values are above the alpha significance level of 5%. This gives evidence that significant differences don’t exist across treatments within blocks. This is an ideal goal when setting up randomized block design experiments.</p>
</section>
<section id="visualizing-effects-across-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-effects-across-blocks">Visualizing effects across blocks</h4>
<p>We can also look for differences in the outcome across randomized blocks. Here, we do not break down further by treatment. These boxplots look similar, so we might guess that none of the blocks has a significantly different mean outcome compared to the others.</p>
</section>
<section id="anova-between-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="anova-between-blocks">ANOVA between blocks</h4>
<p>Next we compute the one-way ANOVA test across the blocks. It compares the <code>Muscle_Gain_kg</code> values for each block separately to assess whether there are significant differences in means among the blocks. The function <code>f_oneway</code> calculates the <code>F-statistic</code> and associated <code>p-value</code>, indicating the likelihood of observing the data if the null hypothesis of equal means across all blocks is true. A p-value greater than 0.05 supports what we saw with the boxplot - that there is no significant difference.</p>
</section>
</section>
<section id="exercise-2.2.1" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="exercise-2.2.1"><span class="header-section-number">1.2.5</span> Exercise 2.2.1</h3>
<section id="implementing-a-randomized-block-design" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="implementing-a-randomized-block-design">Implementing a randomized block design</h4>
<p>The manufacturing firm you worked with earlier is still interested in conducting some experiments on worker productivity. Previously, the two blocks were set randomly. While this can work, it can be better to group subjects based on similar characteristics.</p>
<p>The same employees are again loaded but this time in a DataFrame called <code>productivity</code> including 1200 other colleagues. It also includes a worker <code>'productivity_score'</code> column based on units produced per hour. This column was binned into three groups to generate blocks based on similar productivity values. The firm would like to apply a new incentive program with three options (<code>'Bonus'</code>, <code>'Profit Sharing'</code> and <code>'Work from Home'</code>) throughout the firm with treatment applied randomly.</p>
</section>
<section id="instructions-7" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-7">Instructions</h4>
<ul>
<li>Shuffle the blocks to create a new DataFrame called <code>prod_df</code>.</li>
<li>Reset the index so that <code>block</code> is not both an index and a column.</li>
<li>Randomly assign the three treatment values in the <code>'Treatment'</code> column.</li>
</ul>
</section>
</section>
<section id="exercise-2.2.2" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="exercise-2.2.2"><span class="header-section-number">1.2.6</span> Exercise 2.2.2</h3>
<section id="visualizing-productivity-within-blocks-by-incentive" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-productivity-within-blocks-by-incentive">Visualizing productivity within blocks by incentive</h4>
<p>Continuing with the worker productivity example, you’ll explore if the productivity scores are distributed throughout the data as one would expect with random assignment of treatment. Note that this is a precautionary step, and the treatment and follow-up results on the impact of the three treatments is not done yet!</p>
<p>seaborn and matplotlib.pyplot as sns and plt respectively are loaded.</p>
</section>
<section id="instructions-8" class="level4" data-number="1.2.6.1">
<h4 data-number="1.2.6.1" class="anchored" data-anchor-id="instructions-8"><span class="header-section-number">1.2.6.1</span> Instructions</h4>
<ul>
<li>Visualize the productivity scores within blocks by treatment using a boxplot with <code>'block'</code> for <code>x</code>, <code>'productivity_score'</code> for <code>y</code>, and <code>'Treatment'</code> for <code>hue</code>.</li>
</ul>
</section>
</section>
<section id="exercise-2.2.3" class="level3" data-number="1.2.7">
<h3 data-number="1.2.7" class="anchored" data-anchor-id="exercise-2.2.3"><span class="header-section-number">1.2.7</span> Exercise 2.2.3</h3>
<section id="anova-within-blocks-of-employees" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="anova-within-blocks-of-employees">ANOVA within blocks of employees</h4>
<p>Building on your previous analyses with the manufacturing firm, where worker productivity was examined across different blocks and an incentive program was introduced, you’re now delving deeper into the data. The firm, equipped with a more comprehensive dataset in the productivity DataFrame, including 1200 additional employees and their productivity_score, has structured the workforce into three blocks based on productivity levels. Each employee has been randomly assigned one of three incentive options: ‘Bonus’, ‘Profit Sharing’, or ‘Work from Home’.</p>
<p>Before assessing the full impact of these incentive treatments on productivity, it’s crucial to verify that the initial treatment assignment was indeed random and equitable across the different productivity blocks. This step ensures that any observed differences in productivity post-treatment can be confidently attributed to the incentive programs themselves, rather than pre-existing disparities in the blocks.</p>
</section>
<section id="instructions-9" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-9">Instructions</h4>
<ul>
<li>Group <code>prod_df</code> by the appropriate column that represents different blocks in your data.</li>
<li>Use a lambda function to apply the ANOVA test within each <code>block</code>, specifying the <code>lambda</code> function’s argument.</li>
<li>For each treatment group within the blocks, filter <code>prod_df</code> based on the <code>'Treatment'</code> column values and select the <code>'productivity_score'</code> column.</li>
</ul>
<div id="19b58ddb" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>subject_id <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">1301</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>productivity_score <span class="op">=</span> np.random.uniform(<span class="fl">1.1</span>, <span class="fl">30.0</span>, <span class="dv">1300</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1300</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>productivity <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subject_id'</span>: subject_id,</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'productivity_score'</span>: productivity_score,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'block'</span>: block</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign workers to blocks</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>prod_df <span class="op">=</span> productivity.groupby(<span class="st">'block'</span>).<span class="bu">apply</span>(</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the index</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>prod_df <span class="op">=</span> prod_df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign treatment randomly</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>prod_df[<span class="st">'Treatment'</span>] <span class="op">=</span> np.random.choice(</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>  [<span class="st">'Bonus'</span>, <span class="st">'Profit Sharing'</span>, <span class="st">'Work from Home'</span>],</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>  size<span class="op">=</span><span class="bu">len</span>(prod_df)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample Boxplot</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>sns.boxplot(x<span class="op">=</span><span class="st">'block'</span>, </span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">'productivity_score'</span>, </span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">'Treatment'</span>, </span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>prod_df)</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Move the legend outside the plot</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Treatment'</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Show plot</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the within blocks ANOVA, first grouping by block</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>within_block_anova <span class="op">=</span> prod_df.groupby(<span class="st">'block'</span>).<span class="bu">apply</span>(</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Set function</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>  <span class="kw">lambda</span> x: f_oneway(</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter Treatment values based on outcome</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Bonus'</span>][<span class="st">'productivity_score'</span>], </span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Profit Sharing'</span>][<span class="st">'productivity_score'</span>],</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Work from Home'</span>][<span class="st">'productivity_score'</span>])</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(within_block_anova)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="778" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>block
1     (0.8437999669869772, 0.4307774592046686)
2    (0.33640460765350616, 0.7145131123685446)
3    (1.5967688826774828, 0.20381926296045258)
dtype: object</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>An ANOVA analysis was performed to compare productivity scores across different blocks for the three treatment groups. The results show that all three p-values exceed the alpha threshold of 0.05, indicating no significant differences in productivity scores among the treatment groups within the blocks.</strong></p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-Chapter2.3" class="level3" data-number="1.2.8">
<h3 data-number="1.2.8" class="anchored" data-anchor-id="sec-Chapter2.3"><span class="header-section-number">1.2.8</span> Chapter 2.3: Covariate adjustment in experimental design</h3>
<p>Let’s now explore covariates in experimental design and analysis, and how they can be used to minimize confounding. We’ll also learn about ANCOVA, or analysis of covariance, for evaluating treatment effects while controlling for covariates.</p>
<section id="introduction-to-covariates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="introduction-to-covariates">Introduction to covariates</h4>
<p>Recall that covariates are variables that are not of primary interest but are related to the outcome variable and can influence its analysis. Including covariates in statistical analyses is crucial for reducing confounding, which occurs when an external variable influences both the dependent variable and independent variable(s). By adjusting for covariates, researchers can isolate the effect of the independent variable on the outcome, minimizing the influence of confounders. Accounting for covariates in experimental design and analysis controls for variability that is not attributable to the primary variables being studied. This leads to more valid conclusions about the relationship between the independent and dependent variables, as the analysis better reflects the true effect by isolating it from the influence of covariates. Consider the investigation of a new teaching method’s effectiveness on student test scores. Here, the primary variables of interest are the teaching method (independent variable) and the student test scores (dependent variable). However, students’ prior subject knowledge serves as a crucial covariate because prior knowledge can significantly impact learning outcomes, yet it’s not the main focus of the study.</p>
</section>
<section id="experimental-data-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="experimental-data-example">Experimental data example</h4>
<p>Let’s bring back our plant growth data and set it to experimental data as the <code>exp_data</code> DataFrame, keeping <code>Fertilizer_Type</code> as treatment and <code>Growth_cm</code> as response.</p>
</section>
<section id="covariate-data-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="covariate-data-example">Covariate data example</h4>
<p>The covariate_data DataFrame also includes <code>Plant_ID</code> identifiers for each subject, again ranging from 1 to 120, ensuring each subject’s covariate data is matched with their experimental data. <code>Watering_Days_Per_Week</code> is another variable measured for each plant. Recall that covariates are additional variables potentially influencing the outcome and are included in analyses to control for their effects.</p>
</section>
<section id="combining-experimental-data-with-covariates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="combining-experimental-data-with-covariates">Combining experimental data with covariates</h4>
<p>Combining the experimental with covariate data is a crucial step in adjusting for covariates. We use <code>pandas</code>’ <code>merge</code> function to combine DataFrames; we do this on the <code>Plant_ID</code> to ensure each that subject’s experimental and covariate data are aligned.</p>
</section>
<section id="adjusting-for-covariates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="adjusting-for-covariates">Adjusting for covariates</h4>
<p>To adjust for covariates in our analysis, we employ ANCOVA, or analysis of covariance, using the ols model from statsmodels. This <code>ols()</code> function takes a formula that specifies the dependent and independent variables. <code>Growth_cm</code> is the dependent variable we’re interested in, which we want to model using the <code>Fertilizer_Type</code>, the categorical independent variable representing different groups in the experiment, and the potential covariate, <code>Watering_Days_Per_Week</code>, to control for its effects. The first portion of summary output provides details on the significance of the model; it show a large p-value here of 0.531, which implies a lack of support for covariates affecting the model.</p>
</section>
<section id="further-exploring-ancova-results" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="further-exploring-ancova-results">Further exploring ANCOVA results</h4>
<p>Looking at the second and third rows of this second portion of output from summary, we see that the factors and covariate each have large p-values of 0.760 and 0.275, concluding that each of them alone are not significant predictors of growth for this model.</p>
</section>
<section id="visualizing-treatment-effects-with-covariate-adjustment" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-treatment-effects-with-covariate-adjustment">Visualizing treatment effects with covariate adjustment</h4>
<p>This seaborn lmplot shows treatment effects adjusted for the covariate. The regression lines for each treatment category offer a visual representation of how treatment effects trend across different levels of the covariate. We see that <code>Organic</code> remains relatively constant going from 1 watering to 7 <code>Watering_Days_Per_Week</code>. Synthetic shows an increase. The crossing regression lines suggest we may want to add an interaction term of <code>Watering_Days_Per_Week</code> by <code>Fertilizer_Type</code> in another model. Parallel lines would suggest a lack of interaction.</p>
</section>
</section>
<section id="exercise-2.3.1" class="level3" data-number="1.2.9">
<h3 data-number="1.2.9" class="anchored" data-anchor-id="exercise-2.3.1"><span class="header-section-number">1.2.9</span> Exercise 2.3.1</h3>
<section id="covariate-adjustment-with-chick-growth" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="covariate-adjustment-with-chick-growth">Covariate adjustment with chick growth</h4>
<p>Imagine studying in agricultural science the growth patterns of chicks under various dietary regimens. The data from this study sheds light on the intricate relationship between their respective diets and the consequent impact on their weight. This data includes weight measurements of chicks at different ages, allowing for an exploration of covariate adjustment. <code>age</code> serves as a covariate, potentially influencing the outcome variable: the <code>weight</code> of the chicks.</p>
</section>
<section id="instructions-10" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-10">Instructions</h4>
<ol type="1">
<li><p>Join the experimental and covariate data based on common column(s), and print this merged data.</p></li>
<li><p>Produce an ANCOVA predicting <code>'weight'</code> based on <code>'Diet'</code> and <code>'Time'</code>.</p></li>
</ol>
<ul>
<li>Print a summary of the ANCOVA model.</li>
</ul>
<ol start="3" type="1">
<li>Design an <code>lmplot</code> to see <code>hue='Diet'</code> effects on <code>y='weight'</code> adjusted for <code>x='Time'</code>.</li>
</ol>
<div id="660d281d" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Chick (randomly sampled from 1 to 50)</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>chick_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">51</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Diet (randomly sampled from 1 to 4)</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>diet_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">5</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for weight with approximate mean and std</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>weight_values <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">121.818</span>, scale<span class="op">=</span><span class="fl">71.072</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure weights are within the specified range</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>weight_values <span class="op">=</span> np.clip(weight_values, <span class="dv">35</span>, <span class="dv">373</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>exp_chick_data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Chick"</span>: chick_values,</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Diet"</span>: diet_values,</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"weight"</span>: weight_values</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Chick (randomly sampled from 1 to 50)</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>chick_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">51</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Time (normally distributed around mean 10.718 with std 6.758)</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>time_values <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">10.718</span>, scale<span class="op">=</span><span class="fl">6.758</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure Time values are within the specified range (0 to 21)</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>time_values <span class="op">=</span> np.clip(time_values, <span class="dv">0</span>, <span class="dv">21</span>).astype(<span class="bu">int</span>)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>cov_chick_data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Chick"</span>: chick_values,</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Time"</span>: time_values</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Join experimental and covariate data</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>merged_chick_data <span class="op">=</span> pd.merge(exp_chick_data, </span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>                            cov_chick_data, on<span class="op">=</span><span class="st">'Chick'</span>)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the merged data</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(merged_chick_data)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Join experimental and covariate data</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>merged_chick_data <span class="op">=</span> pd.merge(exp_chick_data, </span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>                             cov_chick_data, on<span class="op">=</span><span class="st">'Chick'</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform ANCOVA with Diet and Time as predictors</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ols(<span class="st">'weight ~ Diet + Time'</span>, data<span class="op">=</span>merged_chick_data).fit()</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of the model</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary())</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Diet effects with Time adjustment</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>sns.lmplot(x<span class="op">=</span><span class="st">'Time'</span>, y<span class="op">=</span><span class="st">'weight'</span>, </span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>         hue<span class="op">=</span><span class="st">'Diet'</span>, </span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>merged_chick_data)</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      Chick  Diet      weight  Time
0        39     1  138.685119    11
1        39     1  138.685119    20
2        39     1  138.685119     7
3        39     1  138.685119     7
4        39     1  138.685119    14
...     ...   ...         ...   ...
6727     35     3  150.487736    16
6728     35     3  150.487736     0
6729     35     3  150.487736    21
6730     35     3  150.487736     0
6731     35     3  150.487736     2

[6732 rows x 4 columns]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 weight   R-squared:                       0.001
Model:                            OLS   Adj. R-squared:                  0.001
Method:                 Least Squares   F-statistic:                     4.288
Date:                Thu, 20 Mar 2025   Prob (F-statistic):             0.0138
Time:                        11:11:50   Log-Likelihood:                -37536.
No. Observations:                6732   AIC:                         7.508e+04
Df Residuals:                    6729   BIC:                         7.510e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    121.2380      2.310     52.486      0.000     116.710     125.766
Diet           2.0087      0.691      2.906      0.004       0.654       3.364
Time          -0.0400      0.130     -0.308      0.758      -0.295       0.215
==============================================================================
Omnibus:                      234.618   Durbin-Watson:                   0.188
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              258.758
Skew:                           0.478   Prob(JB):                     6.48e-57
Kurtosis:                       2.904   Cond. No.                         37.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-11-output-2.png" width="526" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-Chapter3" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-Chapter3"><span class="header-section-number">1.3</span> Chapter 3: Analyzing Experimental Data: Statistical Tests and Power</h2>
<p>Master statistical tests like t-tests, ANOVA, and Chi-Square, and dive deep into post-hoc analyses and power analysis essentials. Learn to select the right test, interpret p-values and errors, and skillfully conduct power analysis to determine sample and effect sizes, all while leveraging Python’s powerful libraries to bring your data insights to life.</p>
<section id="sec-Chapter3.1" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="sec-Chapter3.1"><span class="header-section-number">1.3.1</span> Chapter 3.1. Choosing the right statistical test</h3>
<p>We’ll now look into choosing the right statistical test for analyzing experimental data.</p>
<section id="selecting-the-right-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="selecting-the-right-test">Selecting the right test</h4>
<p>Just as choosing the right book or the right measurement tool for is vital to research, choosing the right statistical test is foundational to any data analysis. Understanding our dataset’s features and the hypotheses under examination is vital. It necessitates assessing the data types—categorical or continuous—their distributions, often assumed to be normal by many statistical tests, and the number of variables in the study. It’s essential to align the chosen statistical method with the dataset’s properties and the study’s goals to ensure accurate and dependable outcomes. In this chapter, we’ll explore how to apply t-tests, ANOVA, and Chi-Square tests, focusing on analyzing experimental data.</p>
</section>
<section id="the-dataset-athletic-performance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-dataset-athletic-performance">The dataset: athletic performance</h4>
<p>We’ll work with a DataFrame called <code>athletic_perf</code> containing athletes’ performance data, focusing on the effects of different training programs and diets on athletic performance. Key variables are the type of training program, assigned diet, initial fitness level, and the observed performance increase as a percentage.</p>
</section>
<section id="independent-samples-t-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="independent-samples-t-test">Independent samples t-test</h4>
<p>An independent samples t-test is used to compare the means of two distinct groups to determine if there is a statistically significant difference between them. This test relies on the assumptions that the response data for both groups are normally distributed and have equal variances, ensuring the validity and reliability of the test results. We’ll use an <code>alpha</code> of <code>0.5</code> and compare the mean athletic performance improvements between two groups undergoing High-Intensity Interval Training (HIIT) and Endurance training by assigning their performance increases to group1 and group2. Next we call <code>ttest_ind</code> on <code>group1</code> and <code>group2</code> and retrieve the test statistics and p-value. A large p-value here leads us to conclude that there is no significant difference in the mean performance increase between the HIIT and Endurance groups.</p>
</section>
<section id="one-way-anova" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="one-way-anova">One-way ANOVA</h4>
<p>A one-way ANOVA test is employed to determine if there are statistically significant differences among the means of more than two groups. The one-way corresponds to ANOVA with a single independent variable, and it assumes that the variances among the groups are equal. For our example, we gather the athletic performance increase data for each training program type into a list of groups using a list comprehension. The <code>f_oneway</code> function from <code>scipy.stats</code> is then used to conduct the ANOVA test across these groups by unpacking the groups list using an asterisk. The relatively high P-value implies that, based on the provided data, we cannot confidently assert that different training programs lead to different mean increases in athletic performance.</p>
</section>
<section id="chi-square-test-of-association" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="chi-square-test-of-association">Chi-square test of association</h4>
<p>The Chi-square test of association is a statistical method used to assess whether there is a significant association between two categorical variables. Unlike many other statistical tests, the chi-square test does not require assumptions about the distribution of the data. To prepare for the test, we start by creating a contingency table using <code>crosstab</code> from <code>pandas</code>, which cross-tabulates athletes by their <code>Training_Program</code> and <code>Diet_Type</code>.</p>
</section>
<section id="chi-square-test-of-association-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="chi-square-test-of-association-1">Chi-square test of association</h4>
<p>The <code>chi2_contingency</code> function from <code>scipy.stats</code> is then employed to conduct the chi-square test on the contingency table. The large P-value suggests that any observed association between training programs and diet types is not statistically significant.</p>
</section>
</section>
<section id="exercise-3.1.1" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="exercise-3.1.1"><span class="header-section-number">1.3.2</span> Exercise 3.1.1</h3>
<section id="choosing-the-right-test-petrochemicals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="choosing-the-right-test-petrochemicals">Choosing the right test: petrochemicals</h4>
<p>In a chemistry research lab, scientists are examining the efficiency of three well-known catalysts—Palladium (Pd), Platinum (Pt), and Nickel (Ni)—in facilitating a particular reaction. Each catalyst is used in a set of identical reactions under controlled conditions, and the time taken for each reaction to reach completion is meticulously recorded. Your goal is to compare the mean reaction times across the three catalyst groups to identify which catalyst, if any, has a significantly different reaction time.</p>
</section>
<section id="instructions-11" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-11">Instructions</h4>
<ul>
<li>Use a list comprehension to filter into <code>groups</code> iterating over the <code>catalyst_types</code> and each of their <code>'Reaction_Time'</code>s.</li>
</ul>
<div id="fbaf12ed" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>chemical_reactions <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chemical_reactions.csv'</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>catalyst_types <span class="op">=</span> [<span class="st">'Palladium'</span>, <span class="st">'Platinum'</span>, <span class="st">'Nickel'</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect reaction times for each catalyst into a list</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [chemical_reactions[chemical_reactions[<span class="st">'Catalyst'</span>] <span class="op">==</span> catalyst][<span class="st">'Reaction_Time'</span>] <span class="cf">for</span> catalyst <span class="kw">in</span> catalyst_types]</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the one-way ANOVA across the three groups</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F-Statistic: </span><span class="sc">{</span>f_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>F-Statistic: 478.7412625615453, P-value: 4.710677600047866e-151</code></pre>
</div>
</div>
</section>
<section id="question" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="question">Question</h4>
<p>Assume a significance level of <span class="math inline">\(\alpha = 0.01\)</span>. What is the appropriate conclusion to glean from the P-value in comparison with this <span class="math inline">\(\alpha\)</span> value?</p>
<p><em>The P-value is substantially smaller than the <span class="math inline">\(\alpha\)</span> value, indicating a significant difference in reaction times across the catalysts.</em></p>
</section>
</section>
<section id="exercise-3.1.2" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="exercise-3.1.2"><span class="header-section-number">1.3.3</span> Exercise 3.1.2</h3>
<section id="choosing-the-right-test-human-resources" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="choosing-the-right-test-human-resources">Choosing the right test: human resources</h4>
<p>In human resources, it’s essential to understand the relationships between different variables that might influence employee satisfaction or turnover. Consider a scenario where an HR department is interested in understanding the association between the department in which employees work and their participation in a new workplace wellness program. The HR team has compiled this data over the past two years and has asked you if there’s any significant association between an employee’s department and their enrolling in the wellness program.</p>
</section>
<section id="instructions-12" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-12">Instructions</h4>
<ul>
<li>Create a contingency table comparing <code>'Department'</code> and <code>'Wellness_Program_Status'</code>.</li>
<li>Perform a chi-square test of association on the contingency table and print the p-value.</li>
</ul>
<div id="2ab01cc1" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>hr_wellness <span class="op">=</span> pd.read_csv(<span class="st">'datasets/hr_wellness.csv'</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a contingency table</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>contingency_table <span class="op">=</span> pd.crosstab(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  hr_wellness[<span class="st">'Department'</span>], </span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  hr_wellness[<span class="st">'Wellness_Program_Status'</span>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the chi-square test of association</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>chi2_stat, p_val, dof, expected <span class="op">=</span> chi2_contingency(contingency_table)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F-Statistic: </span><span class="sc">{</span>chi2_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>F-Statistic: 3.4775739037369617, P-value: 0.17573344450112738</code></pre>
</div>
</div>
</section>
<section id="question-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="question-1">Question</h4>
<ul>
<li>Assume a significance level of 0.05. Given the P-value, what is the appropriate conclusion?</li>
</ul>
<p><em>There’s no significant association between department and enrollment in the wellness program, as the P-value is larger than <code>0.05</code>.</em></p>
</section>
</section>
<section id="exercise-3.1.3" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="exercise-3.1.3"><span class="header-section-number">1.3.4</span> Exercise 3.1.3</h3>
<section id="choosing-the-right-test-finance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="choosing-the-right-test-finance">Choosing the right test: finance</h4>
<p>In the realm of finance, investment strategists are continually evaluating different approaches to maximize returns. Consider a scenario where a financial firm wishes to assess the effectiveness of two investment strategies: “Quantitative Analysis” and “Fundamental Analysis”. The firm has applied each strategy to a separate set of investment portfolios for a year and now asks you to compare the annual returns to determine if there is any difference in strategy returns by comparing the mean returns of the two groups.</p>
</section>
<section id="instructions-14" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-14">Instructions</h4>
<ol type="1">
<li>What type of hypothesis test should be performed in this scenario?</li>
</ol>
<ul>
<li>Possible answer: Independent samples t-test</li>
</ul>
<ol start="2" type="1">
<li><p>Filter <code>'Strategy_Type'</code> on <code>'Quantitative'</code> to retrieve their <code>'Annual_Return'</code> and do the same for <code>'Fundamental'</code> strategies.</p></li>
<li><p>Complete for the two groups an independent samples t-test and print the p-value.</p></li>
</ol>
<div id="e98b9b43" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>investment_returns <span class="op">=</span> pd.read_csv(<span class="st">'datasets/investment_returns.csv'</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the annual returns by strategy type</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>quantitative_returns <span class="op">=</span> investment_returns[investment_returns[<span class="st">'Strategy_Type'</span>] <span class="op">==</span> <span class="st">'Quantitative'</span>][<span class="st">'Annual_Return'</span>]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>fundamental_returns <span class="op">=</span> investment_returns[investment_returns[<span class="st">'Strategy_Type'</span>] <span class="op">==</span> <span class="st">'Fundamental'</span>][<span class="st">'Annual_Return'</span>]</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the independent samples t-test between the two groups</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> ttest_ind(quantitative_returns, fundamental_returns)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"T-statistic : </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>T-statistic : 7.784788496693728, P-value: 2.0567003424807146e-14</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Assume a significance level of 0.1. What is the appropriate conclusion to glean from the P-value in comparison with this <span class="math inline">\(\alpha\)</span> value?</li>
</ol>
<p><em>The P-value is much smaller than <span class="math inline">\(\alpha\)</span>, suggesting a significant difference in returns between the two strategies.</em></p>
</section>
</section>
<section id="sec-Chapter3.2" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="sec-Chapter3.2"><span class="header-section-number">1.3.5</span> Chapter 3.2: Post-hoc analysis following ANOVA</h3>
<p>After conducting ANOVA, we often need to understand specific differences between groups. This is where post-hoc analysis comes in, providing detailed insights into pairwise comparisons.</p>
<section id="when-to-use-post-hoc-tests" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="when-to-use-post-hoc-tests">When to use post-hoc tests</h4>
<p>Post-hoc tests are pivotal when ANOVA reveals significant differences among groups. They allow us to pinpoint which specific pairs of groups differ, allowing us to peek behind the curtain to explore the inner workings of pairwise differences.</p>
</section>
<section id="key-post-hoc-methods" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="key-post-hoc-methods">Key post-hoc methods</h4>
<p>There are two common post-hoc methods: Tukey’s HSD, named after statistician John Tukey, which is known for its robustness in multiple comparisons. There’s also the Bonferroni correction, named after mathematician Carlo Bonferroni, which adjusts p-values to control for Type I errors. For broader comparisons, use Tukey’s HSD; Bonferroni is better for reducing false positives in more focused tests.</p>
<p>1 <a href="https://www.amphilsoc.org/item-detail/photograph-john-wilder-tukey">https://www.amphilsoc.org/item-detail/photograph-john-wilder-tukey</a></p>
<p>2 <a href="https://en.wikipedia.org/wiki/Carlo_Emilio_Bonferroni">https://en.wikipedia.org/wiki/Carlo_Emilio_Bonferroni</a></p>
</section>
<section id="the-dataset-marketing-ad-campaigns" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-dataset-marketing-ad-campaigns">The dataset: marketing ad campaigns</h4>
<p>We’ll work with a dataset of marketing campaigns, examining the <code>Click_Through_Rate</code> for different <code>Ad</code> campaigns to identify differences and which strategy is most effective.</p>
</section>
<section id="data-organization-with-pivot-tables" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-organization-with-pivot-tables">Data organization with pivot tables</h4>
<p>Pivot tables in pandas can be extremely helpful for organizing data, especially before conducting post-hoc analysis. It provides a clear comparison of the mean <code>Click_Through_Rates</code> for each campaign type.</p>
</section>
<section id="performing-anova" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="performing-anova">Performing ANOVA</h4>
<p>We start with ANOVA to assess if there’s a significant difference in these <code>Click_Through_Rates</code> among the campaigns. This sets the stage for further analysis if significant differences are found. First, we specify the different campaign types. Then we create the groups using a list comprehension to extract the <code>Click_Through_Rate</code> for each <code>Ad_Campaign</code>. Next, we perform the ANOVA across the three campaign types, unpacking the groups using an asterisk, to compare their mean click-through rates. The very small P-value here indicates significant differences in these means.</p>
</section>
<section id="tukeys-hsd-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="tukeys-hsd-test">Tukey’s HSD test</h4>
<p>If ANOVA indicates significant differences, Tukey’s HSD test helps us understand exactly which campaigns differ. The <code>pairwise_tukeyhsd</code> function from <code>statsmodels.stats</code> takes arguments for the continuous response variable, <code>Click_Through_Rat</code>e in this case, the categorical variable with more than two groups, <code>Ad_Campaign</code>, and <span class="math inline">\(\alpha\)</span>. To interpret the results of this table, we focus on the meandiff, p-adj (adjusted P-value), and reject columns. For the first row, Loyalty Reward versus New Arrival, the mean difference is 0.2211, with a p-value less than 0.05, indicating that the <code>Loyalty Reward</code> group has a significantly higher mean than the <code>New Arrival group</code>. For <code>Loyalty Reward</code> versus Seasonal Discount, on row 2, the mean difference is -0.2738. With a p-value less than 0.05, it suggests that the Loyalty Reward group has a significantly lower mean than the Seasonal Discount group. Lastly, for New Arrival versus Seasonal Discount, the mean difference is -0.4949, with a p-value less than 0.05, indicating that the New Arrival group has a significantly lower mean than the Seasonal Discount group.</p>
</section>
<section id="bonferroni-correction-set-up" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bonferroni-correction-set-up">Bonferroni correction set-up</h4>
<p>The Bonferroni correction is a stringent method to adjust p-values when conducting multiple pairwise comparisons, effectively reducing the chances of a Type I error. A little more data preparation is required before applying the Bonferroni correction. We begin by creating an empty P-values list. Then, we lay out a list of tuples containing the pairwise comparisons that we will iterate over. Next, we iterate over the tuples in comparisons, using the tuple elements to extract the <code>Click_Through_Rate</code> for both groups. We run <code>ttest_ind</code> on the click through rates in a pairwise fashion, and append the p-values to our list.</p>
</section>
<section id="performing-bonferroni-correction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="performing-bonferroni-correction">Performing Bonferroni correction</h4>
<p>Now we apply the Bonferroni correction using the multipletests function. The resulting p-values for the three comparisons are all extremely small. This again provides evidence that each of the three groups have significant click through rate differences.</p>
</section>
</section>
<section id="exercise-3.2.1" class="level3" data-number="1.3.6">
<h3 data-number="1.3.6" class="anchored" data-anchor-id="exercise-3.2.1"><span class="header-section-number">1.3.6</span> Exercise 3.2.1</h3>
<section id="anxiety-treatments-anova" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="anxiety-treatments-anova">Anxiety treatments ANOVA</h4>
<p>Psychologists conducted a study to compare the effectiveness of three types of therapy on reducing anxiety levels: Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), and Acceptance and Commitment Therapy (ACT). Participants were randomly assigned to one of the three therapy groups, and their anxiety levels were measured before and after the therapy sessions. The psychologists have asked you to determine if there are any significant differences in the effectiveness of these therapies.</p>
</section>
<section id="instructions-15" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-15">Instructions</h4>
<ol type="1">
<li>Create a pivot table to calculate the mean <code>'Anxiety_Reduction'</code> value across groups of <code>'Therapy_Type'</code> in this data.</li>
<li>Filter groups of therapy types and their <code>'Anxiety_Reduction'</code> values by first creating a list of the three therapy types: <code>'CBT'</code>, <code>'DBT'</code>, and <code>'ACT'</code>.</li>
</ol>
<div id="331ce1a4" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary statistics</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(therapy_outcomes.describe())</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pivot table</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>pivot_table <span class="op">=</span> therapy_outcomes.pivot_table(</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span><span class="st">'Anxiety_Reduction'</span>, </span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="st">'Therapy_Type'</span>, </span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    aggfunc<span class="op">=</span><span class="st">'mean'</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Pivot to view the mean anxiety reduction for each therapy</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pivot_table)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create groups to prepare the data for ANOVA</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> therapy][<span class="st">'Anxiety_Reduction'</span>] <span class="cf">for</span> therapy <span class="kw">in</span> therapy_types]</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct ANOVA</span></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>       Anxiety_Reduction
count        1422.000000
mean           15.224009
std             4.781342
min             0.723195
25%            11.855642
50%            15.360508
75%            18.602824
max            34.264000
              Anxiety_Reduction
Therapy_Type                   
ACT                   15.653751
CBT                   14.701516
DBT                   15.310404
0.007803856353541549</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>By analyzing the data with ANOVA, you’ve taken an important step in comparing the effectiveness of different therapies. Assuming an <span class="math inline">\(\alpha\)</span> of 0.05, the P-value indicates significant differences in therapy effectiveness.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3.2.2" class="level3" data-number="1.3.7">
<h3 data-number="1.3.7" class="anchored" data-anchor-id="exercise-3.2.2"><span class="header-section-number">1.3.7</span> Exercise 3.2.2</h3>
<section id="applying-tukeys-hsd" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applying-tukeys-hsd">Applying Tukey’s HSD</h4>
<p>Following the ANOVA analysis which suggested significant differences in the effectiveness of the three types of therapy, the psychologists are keen to delve deeper. They wish for you to explain exactly which therapy types differ from each other in terms of reducing anxiety levels. This is where Tukey’s Honest Significant Difference (HSD) test comes into play. It’s a post-hoc test used to make pairwise comparisons between group means after an ANOVA has shown a significant difference. Tukey’s HSD test helps in identifying specific pairs of groups that have significant differences in their means.</p>
</section>
<section id="instructions-16" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-16">Instructions</h4>
<ul>
<li>At a significance level of <code>0.05</code>, perform Tukey’s HSD test to compare the mean anxiety reduction across the three therapy groups.</li>
</ul>
<div id="b1169937" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multicomp <span class="im">import</span> pairwise_tukeyhsd</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pivot table</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>pivot_table <span class="op">=</span> therapy_outcomes.pivot_table(</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span><span class="st">'Anxiety_Reduction'</span>, </span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="st">'Therapy_Type'</span>, </span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    aggfunc<span class="op">=</span><span class="st">'mean'</span></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Pivot to view the mean anxiety reduction for each therapy</span></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pivot_table)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create groups to prepare the data for ANOVA</span></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> therapy][<span class="st">'Anxiety_Reduction'</span>] <span class="cf">for</span> therapy <span class="kw">in</span> therapy_types]</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct ANOVA</span></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_val)</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Tukey's HSD test</span></span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>tukey_results <span class="op">=</span> pairwise_tukeyhsd(</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    therapy_outcomes[<span class="st">'Anxiety_Reduction'</span>], </span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    therapy_outcomes[<span class="st">'Therapy_Type'</span>], </span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tukey_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              Anxiety_Reduction
Therapy_Type                   
ACT                   15.653751
CBT                   14.701516
DBT                   15.310404
0.007803856353541549
Multiple Comparison of Means - Tukey HSD, FWER=0.05 
====================================================
group1 group2 meandiff p-adj   lower   upper  reject
----------------------------------------------------
   ACT    CBT  -0.9522 0.0059 -1.6767 -0.2278   True
   ACT    DBT  -0.3433 0.5087 -1.0697   0.383  False
   CBT    DBT   0.6089  0.123 -0.1205  1.3383  False
----------------------------------------------------</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The Tukey HSD test provided clear insights into which therapy types significantly differ in reducing anxiety. These findings can guide psychologists in refining treatment approaches. Did you catch that (ACT and DBT) and (CBT and DBT)don’t differ significantly from this experiment?</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3.2.3" class="level3" data-number="1.3.8">
<h3 data-number="1.3.8" class="anchored" data-anchor-id="exercise-3.2.3"><span class="header-section-number">1.3.8</span> Exercise 3.2.3</h3>
<section id="applying-bonferoni-correction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applying-bonferoni-correction">Applying Bonferoni correction</h4>
<p>After identifying significant differences between therapy groups with Tukey’s HSD, we want to confirm our findings with the Bonferroni correction. The Bonferroni correction is a conservative statistical adjustment used to counteract the problem of multiple comparisons. It reduces the chance of obtaining false-positive results by adjusting the significance level. In the context of your study on the effectiveness of CBT, DBT, and ACT, applying the Bonferroni correction will help ensure that the significant differences you observe between therapy groups are not due to chance.</p>
</section>
<section id="instructions-17" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-17">Instructions</h4>
<ul>
<li>Conduct independent t-tests between all pairs of therapy groups in <code>therapy_pairs</code> and append the p-values (<code>p_val</code>) to the <code>p_values</code> list.</li>
<li>Apply the Bonferroni correction to adjust the <code>p-values</code> from the multiple tests and print them.</li>
</ul>
<div id="7a1a0d7d" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.sandbox.stats.multicomp <span class="im">import</span> multipletests</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> []</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>therapy_pairs <span class="op">=</span> [(<span class="st">'CBT'</span>, <span class="st">'DBT'</span>), (<span class="st">'CBT'</span>, <span class="st">'ACT'</span>), (<span class="st">'DBT'</span>, <span class="st">'ACT'</span>)]</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct t-tests and collect P-values</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pair <span class="kw">in</span> therapy_pairs:</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>    group1 <span class="op">=</span> therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> pair[<span class="dv">0</span>]][<span class="st">'Anxiety_Reduction'</span>]</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>    group2 <span class="op">=</span> therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> pair[<span class="dv">1</span>]][<span class="st">'Anxiety_Reduction'</span>]</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>    t_stat, p_val <span class="op">=</span> ttest_ind(group1, group2)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>    p_values.append(p_val)</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Bonferroni correction</span></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'bonferroni'</span>)[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[0.15515276 0.00483525 0.83050552]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The Bonferroni correction applied to adjust for the P-values for multiple comparisons. This step is critical to control for Type I error, ensuring the reliability of my findings. Here again it is obvious that (ACT and DBT) and (CBT and DBT) don’t differ significantly from this experiment due to the corrected P-value of 1.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-Chapter3.3" class="level3" data-number="1.3.9">
<h3 data-number="1.3.9" class="anchored" data-anchor-id="sec-Chapter3.3"><span class="header-section-number">1.3.9</span> Chapter 3.3: P-values, alpha, and errors</h3>
<p>In this lesson, we’ll deepen our understanding of p-values, alpha levels, and experimental errors. This will prepare us for the next video, where we’ll tackle a key concept in experimental design called power analysis!</p>
<section id="p-values-and-alpha" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="p-values-and-alpha">P-values and alpha</h4>
<p>P-values and alpha can be viewed as a game. Think of conducting a scientific experiment where we are trying to determine whether a certain strategy (our hypothesis) leads to winning (or a significant result) more often than just by chance. P-values help us understand the likelihood of observing our data if the null hypothesis was true. That is they serve as the scoreboard of the game. Setting an <span class="math inline">\(\alpha\)</span> level, often 0.05, allows us to determine the threshold at which we consider our results statistically significant, akin to setting the rules of a game before playing. Alpha is like establishing a rule for what counts as a “remarkable” win in this game. If your P-value is below this alpha level, it’s as if we’ve achieved a high score or a remarkable performance in the game, leading us to conclude that our strategy (the alternative hypothesis) might indeed be effective, and it’s not just the luck of the draw.</p>
</section>
<section id="the-dataset-crop-yields" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-dataset-crop-yields">The dataset: crop yields</h4>
<p>We’ll work with a dataset of crop yields from different fields, where each field was treated with either organic or synthetic fertilizer. Our goal is to analyze this data to determine if there’s a significant difference in crop yields between the two fertilizer types.</p>
</section>
<section id="visualizing-the-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-the-data">Visualizing the data</h4>
<p>It’s helpful to visualize the crop yields for each fertilizer type. By plotting the kernel density estimates (kde), we get a sense of how the two fertilizers might differ in terms of their effect on crop yields and whether there’s an overlap between their effects. It appears that Organic tends to produce a higher yield than Synthetic with some overlap.</p>
</section>
<section id="conducting-an-independent-samples-t-test" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="conducting-an-independent-samples-t-test">Conducting an independent samples t-test</h4>
<p>We set our alpha to the standard five-percent level. To compare the effectiveness of organic versus synthetic fertilizers, we perform a t-test on the crop yields from the two groups. The p-value is smaller than alpha suggesting that fertilizer type has a statistically significant impact on crop yield.</p>
</section>
<section id="exploring-experimental-errors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exploring-experimental-errors">Exploring experimental errors</h4>
<p>In experimental design, we encounter two main types of errors. Type I errors occur when we incorrectly reject a true null hypothesis, akin to a false alarm. Type II errors happen when we fail to reject a false null hypothesis, similar to a missed detection.</p>
</section>
<section id="more-on-alpha" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="more-on-alpha">More on alpha</h4>
<p>Alpha, or the significance level, is crucial in hypothesis testing; it indicates the probability of a Type I error—rejecting a true null hypothesis. Common <span class="math inline">\(\alpha\)</span> levels include 0.05, 0.01, and 0.10, representing risks of 5%, 1%, and 10%, respectively, for such errors. Selecting an alpha hinges on the study’s context and a balance between tolerating a Type I error and the risk of overlooking a true effect, known as a Type II error. The choice should align with the study’s goals and the implications of potential errors. Conventionally, 0.05 is the standard for statistical significance across many disciplines. For more rigorous scrutiny, particularly where the cost of a Type I error is high, an <span class="math inline">\(\alpha\)</span> of 0.01 is preferred. In preliminary studies, where a higher error tolerance is permissible, an <span class="math inline">\(\alpha\)</span> of 0.10 may be utilized, allowing for a broader exploration of potential effects with subsequent validation through more stringent testing.</p>
</section>
</section>
<section id="exercise-3.3.1" class="level3" data-number="1.3.10">
<h3 data-number="1.3.10" class="anchored" data-anchor-id="exercise-3.3.1"><span class="header-section-number">1.3.10</span> Exercise 3.3.1</h3>
<section id="analyzing-toy-durability" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="analyzing-toy-durability">Analyzing toy durability</h4>
<p>In product development within the toy industry, it’s crucial to understand the durability of toys, particularly when comparing educational toys to recreational ones. Durability can significantly impact customer satisfaction and repeat business. Researchers in a toy manufacturing company have asked you to conduct the analysis of a study comparing the durability of educational toys versus recreational toys. The <code>toy_durability</code> DataFrame contains the results of these tests, with durability scores assigned based on rigorous testing protocols.</p>
</section>
<section id="instructions-18" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-18">Instructions</h4>
<ul>
<li>Calculate the mean <code>'Durability_Score'</code> for both <code>'Educational'</code> and <code>'Recreational'</code> toys using a pivot table.</li>
<li>Perform an independent samples t-test to compare the durability of <code>'Educational'</code> and ‘Recreational’ toys by first separating durability scores by <code>Toy_Type</code>.</li>
</ul>
<div id="88e82d4f" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">36</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Toy_Type column with approximately equal distribution</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>toy_types <span class="op">=</span> np.random.choice([<span class="st">'Educational'</span>, <span class="st">'Recreational'</span>], size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Durability_Score based on the means for each Toy_Type</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>durability_scores <span class="op">=</span> np.where(</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    toy_types <span class="op">==</span> <span class="st">'Educational'</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">80.101</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>),</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">79.461</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>toy_durability <span class="op">=</span> pd.DataFrame({<span class="st">'Toy_Type'</span>: toy_types, <span class="st">'Durability_Score'</span>: durability_scores})</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pivot table</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>mean_durability <span class="op">=</span> toy_durability.pivot_table(values<span class="op">=</span><span class="st">'Durability_Score'</span>, index<span class="op">=</span><span class="st">'Toy_Type'</span>, aggfunc<span class="op">=</span>np.mean)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_durability)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform t-test</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>educational_durability_scores <span class="op">=</span> toy_durability[toy_durability[<span class="st">'Toy_Type'</span>] <span class="op">==</span> <span class="st">'Educational'</span>][<span class="st">'Durability_Score'</span>]</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>recreational_durability_scores <span class="op">=</span> toy_durability[toy_durability[<span class="st">'Toy_Type'</span>] <span class="op">==</span> <span class="st">'Recreational'</span>][<span class="st">'Durability_Score'</span>]</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> ttest_ind(educational_durability_scores, recreational_durability_scores)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P-value:"</span>, p_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              Durability_Score
Toy_Type                      
Educational          80.060756
Recreational         79.430888
P-value: 0.025042366796540375</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The P-value suggests that there’s a statistically significant difference in durability between <code>'Educational'</code> and <code>'Recreational'</code> toys, assuming an alpha of <code>0.05</code>. This insight could be crucial for product development and marketing strategies.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3.3.2" class="level3" data-number="1.3.11">
<h3 data-number="1.3.11" class="anchored" data-anchor-id="exercise-3.3.2"><span class="header-section-number">1.3.11</span> Exercise 3.3.2</h3>
<section id="visualizing-durability-differences" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-durability-differences">Visualizing durability differences</h4>
<p>Following the analysis of toy durability, the research team is interested in you visualizing the distribution of durability scores for both Educational and Recreational toys. Such visualizations can offer intuitive insights into the data, potentially highlighting the range and variability of scores within each category. This step is essential for presenting findings to non-technical stakeholders and guiding further product development decisions.</p>
</section>
<section id="instructions-19" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-19">Instructions</h4>
<ul>
<li>Visualize the distribution of <code>'Durability_Score'</code> for Educational and Recreational toys using a Kernel Density Estimate (KDE) plot, highlighting differences by using the <code>'Toy_Type'</code> column to color the distributions differently.</li>
</ul>
<div id="ded7695b" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">36</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Toy_Type column with approximately equal distribution</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>toy_types <span class="op">=</span> np.random.choice([<span class="st">'Educational'</span>, <span class="st">'Recreational'</span>], size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Durability_Score based on the means for each Toy_Type</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>durability_scores <span class="op">=</span> np.where(</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    toy_types <span class="op">==</span> <span class="st">'Educational'</span>,</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">80.101</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>),</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">79.461</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>toy_durability <span class="op">=</span> pd.DataFrame({<span class="st">'Toy_Type'</span>: toy_types, <span class="st">'Durability_Score'</span>: durability_scores})</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of Durability_Score for each Toy_Type</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>toy_durability, x<span class="op">=</span><span class="st">"Durability_Score"</span>, </span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>         hue<span class="op">=</span><span class="st">"Toy_Type"</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Durability Score Distribution by Toy Type'</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Durability Score'</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.png" width="609" height="488" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-Chapter3.4" class="level3" data-number="1.3.12">
<h3 data-number="1.3.12" class="anchored" data-anchor-id="sec-Chapter3.4"><span class="header-section-number">1.3.12</span> Chapter 3.4: Power analysis: sample and effect size</h3>
<p>We now dive into the intricacies of power analysis, focusing on understanding effect size and how it influences sample size.</p>
<section id="a-primer-on-effect-size" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-primer-on-effect-size">A primer on effect size</h4>
<p>Effect size quantifies the magnitude of the difference between groups, beyond just noting if the difference is statistically significant. Cohen’s d is a commonly used measure, calculated as the difference in means divided by a pooled standard deviation.</p>
</section>
<section id="the-dataset-video-game-engagement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-dataset-video-game-engagement">The dataset: video game engagement</h4>
<p>A video game company conducted an experiment with sixty participants to understand player engagement across two game genres: Action and Puzzle. They recorded the average number of hours players spent engaged to assess which type tends to captivate players more effectively.</p>
</section>
<section id="calculating-power-overview" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="calculating-power-overview">Calculating power overview</h4>
<p>Power analysis revolves around the probability that our test will correctly reject a false null hypothesis. This corresponds to identifying a true effect, avoiding a Type II error. A type II error is denoted as beta, so power is one minus beta and it ranges from zero to one, where one is certainty in our ability to detect a true effect. To calculate power, we first assume an effect size. Here we choose a value of 1, derived from historical data comparing the engagement scores of video game genres. We can also use our sample data to make an estimate of the effect size, but traditionally power analysis is done prior to the data collection. This can also help us determine how big of a sample size we should use in our study. We initialize the power object and call the <code>.solve_power()</code> method, using a sample size for how many video game players were assessed in either group (30), our assumed effect size, and our alpha of 0.05. This high power tells us the likelihood that our test will detect a significant result, given our effect size and sample size.</p>
</section>
<section id="cohens-d-formulation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="cohens-d-formulation">Cohen’s d formulation</h4>
<p>To calculate Cohen’s d as an effect size, we define a function. Its two inputs are numeric data corresponding to the two groups from our sample data. We calculate the difference in the means of the two groups, their sample sizes, and their variances. Next, we determine a pooled standard deviation using this information. Lastly, Cohen’s d is the difference in means divided by the pooled standard deviation.</p>
</section>
<section id="cohens-d-for-video-game-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="cohens-d-for-video-game-data">Cohen’s d for video game data</h4>
<p>To apply this to the video game data, we first split the data into two groups based on the genre. Then we apply our function to get the effect size. The result here is near the theoretical result of 1 we assumed earlier.</p>
</section>
<section id="understanding-sample-size-and-power" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-sample-size-and-power">Understanding sample size and power</h4>
<p>Balancing the need for sufficient power with practical constraints on sample size is a fundamental aspect of planning a study, such as comparing engagement times across different video game genres. A larger sample size can enhance an experiment’s power, improving the likelihood of detecting a true effect.</p>
<p>1 <a href="https://grabngoinfo.com/power-analysis-for-sample-size-using-python/">https://grabngoinfo.com/power-analysis-for-sample-size-using-python/</a></p>
</section>
<section id="sample-size-calculation-in-context" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sample-size-calculation-in-context">Sample size calculation in context</h4>
<p>Let’s contextualize this within our video game study. Assuming our calculated value for Cohen’s d engagement time between game genres, we calculate the sample size needed for each group to achieve 99% power with an alpha of 0.05 and equally-sized groups with a ratio of 1. This calculation is pivotal in ensuring our study is adequately powered to detect meaningful differences in player engagement across genres. Assuming we have an effect size of around 1.2, we would need at least 28 participants in each group to achieve a power of 99%. Recall we collected 30 participants, so we can feel confident about our experiment’s power.</p>
</section>
<section id="visualizing-sample-size-requirements" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-sample-size-requirements">Visualizing sample size requirements</h4>
<p>We next build a visualization illustrating the relationship between effect size measured as Cohen’s d and required sample size for our video game study, by plotting varying effect sizes against required sample sizes. As effect size increases, the required sample size for each group decreases, highlighting the importance of understanding the expected magnitude of differences when planning a study.</p>
</section>
</section>
<section id="exercise-3.4.1" class="level3" data-number="1.3.13">
<h3 data-number="1.3.13" class="anchored" data-anchor-id="exercise-3.4.1"><span class="header-section-number">1.3.13</span> Exercise 3.4.1</h3>
<section id="estimating-required-sample-size-for-energy-study" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="estimating-required-sample-size-for-energy-study">Estimating required sample size for energy study</h4>
<p>In the energy sector, researchers are often tasked with evaluating the effectiveness of new technologies or initiatives to enhance energy efficiency or reduce consumption. A study is being designed to compare the impact of two energy-saving measures: “Smart Thermostats” and “LED Lighting”. To ensure the study has sufficient power to detect a meaningful difference in energy savings between these two measures, you’ll conduct a power analysis.</p>
</section>
<section id="instructions-20" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-20">Instructions</h4>
<ul>
<li>Instantiate a <code>TTestIndPower</code> object.</li>
<li>Conduct the power analysis to estimate the required sample size for each group (Smart Thermostats and LED Lighting) to achieve a power of 0.9, assuming a moderate effect size (Cohen’s d = 0.5) and an alpha of 0.05 with an equal sized groups.</li>
</ul>
<pre><code>import numpy as np
import pandas as pd
from statsmodels.stats.power import TTestIndPower

# Instantiate a TTestIndPower object
power_analysis = TTestIndPower()

# Conduct a power analysis to determine the required sample size
required_n = power_analysis.solve_power(
    effect_size=0.5, 
    alpha=0.05, 
    power=0.9, 
    ratio=1)

print(required_n)

&lt;script.py&gt; output:
    85.03128688801092</code></pre>
<p><em>By conducting a power analysis, you’ve determined that approximately <code>85</code> participants are required in each group to achieve a power of <code>0.9</code>, assuming an Cohen’s d effect size of <code>0.5</code>. This information is crucial for planning a sufficiently powered study to compare the energy-saving effectiveness of Smart Thermostats versus LED Lighting.</em></p>
</section>
</section>
</section>
<section id="chapter-4-advanced-insights-from-experimental-complexity" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="chapter-4-advanced-insights-from-experimental-complexity"><span class="header-section-number">1.4</span> Chapter 4: Advanced Insights from Experimental Complexity</h2>
<p>Hop into the complexities of experimental data analysis. Learn to synthesize insights using pandas, address data issues like heteroscedasticity with <code>scipy.stats</code>, and apply nonparametric tests like Mann-Whitney U. Learn additional techniques for transforming, visualizing, and interpreting complex data, enhancing your ability to conduct robust analyses in various experimental settings.</p>
<section id="chapter-4.1-synthesizing-insights-from-complex-experiments" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="chapter-4.1-synthesizing-insights-from-complex-experiments"><span class="header-section-number">1.4.1</span> Chapter 4.1: Synthesizing insights from complex experiments</h3>
<p>We’ll next explore how to synthesize insights from complex experiments, focusing on integrating data across different experimental factors to derive meaningful conclusions.</p>
<section id="manufacturing-yield-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="manufacturing-yield-data">Manufacturing yield data</h4>
<p>We’ll work with <code>manufacturing_yield</code> dataset, which captures how factors like <code>material</code> type, <code>production speed</code>, and <code>temperature</code> settings impact the <code>yield</code> in our experiment. The <code>BatchID</code> column stores a unique identifier for each item in the data. Determining whether these factors have an impact on the yield strength can be used to optimize manufacturing outcomes.</p>
<pre><code>manufacturing_yield

BatchID MaterialType ProductionSpeed TemperatureSetting YieldStrength
39           Polymer          Medium            Optimal         58.83
195            Metal            High               High         51.29
462           Polymer           High             Optimal        55.15
696         Composite         Medium                 Low        50.27
142         Composite           High                 Low        57.62</code></pre>
</section>
<section id="manufacturing-quality-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="manufacturing-quality-data">Manufacturing quality data</h4>
<p>A separate experiment was also done on the same items exploring the impact of production speed on the quality of the product as the response. This data is stored in the <code>manufacturing_quality</code> DataFrame.</p>
<pre><code>manufacturing_quality

BatchID ProductionSpeed ProductQuality
149                 Low         93.87
739                 High        93.35
617               Medium        90.45
131                 High        90.26
684                  Low        91.62</code></pre>
</section>
<section id="merging-strategy" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="merging-strategy">Merging strategy</h4>
<p>We can use the pandas merge method to seamlessly integrate the <code>manufacturing_yield</code> and <code>manufacturing_quality</code> datasets, joining on the <code>BatchID</code> and <code>ProductionSpeed</code> columns so associated data is connected together. We can now explore this data in a variety of ways, looking for relationships in the data with the two response columns of yield and quality.</p>
<pre><code>merged_manufacturing = pd.merge(manufacturing_yield, manufacturing_quality, on=['BatchID', 'ProductionSpeed'])

print(merged_manufacturing)

BatchID MaterialType ProductionSpeed TemperatureSetting YieldStrength ProductQuality
1               Metal            Low               High         57.32          91.19
5           Composite         Medium             Optimal        51.82          90.20
7             Polymer            Low                High        56.12          91.66
8           Composite           High             Optimal        50.91          93.05
11            Polymer            Low                High         50.13         92.31</code></pre>
</section>
<section id="side-by-side-bar-graph" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="side-by-side-bar-graph">Side-by-side bar graph</h4>
<p>We can showcase potential interactions between <code>MaterialType</code> and <code>ProductionSpeed</code> on <code>YieldStrength</code> using Seaborn’s catplot function. <code>Yield</code> is on the vertical axis broken down by <code>material</code> on the horizontal, and the bars are colored by <code>ProductionSpeed</code>. It seems that Polymer tends to have the highest yield followed by Composite and then by Metal. Production speed has a negative impact on yield across each of the materials as well with slower production leading to better yield than faster production.</p>
<pre><code>import seaborn as sns
sns.catplot(x='MaterialType', y='YieldStrength', hue='ProductionSpeed', kind='bar',
data=merged_manufacturing)</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="catplot.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
</section>
<section id="three-variable-scatterplot" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="three-variable-scatterplot">Three variable scatterplot</h4>
<p>To further explore relationships in the data, we can look to see how both of the response variables relate conditioned on ProductionSpeed. We use a scatterplot with each of the response variables on the axes colored by speed. The green High values tend to be lower in each, with the orange Medium values more near the center of the plot, and the low ProductionSpeed points tending to be near the upper right of the plot.</p>
<pre><code>sns.relplot(x='YieldStrength', y='ProductQuality', hue='ProductionSpeed',
kind='scatter', data=merged_manufacturing)
plt.title('Yield Strength vs. Product Quality by Production Speed')</code></pre>
<p><img src="scatter.png" class="img-fluid"></p>
</section>
<section id="communicating-data-to-technical-audiences" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="communicating-data-to-technical-audiences">Communicating data to technical audiences</h4>
<p>Now that we’ve seen some visualizations on complex experimental data, let’s focus on how we can tailor our approach when presenting to technical audiences. Crafting data narratives for this group involves integrating detailed statistical analysis, such as p-values, test statistics, and significance levels, into our stories. This not only enriches the narrative but also supports the validity of our findings with concrete evidence. Additionally, visualizing complex data for technical stakeholders should go beyond basic charts and include advanced visualizations like heat maps, scatter plots using multiple colors, and projection lines. These types of visuals can more precisely demonstrate relationships and trends within the data, catering to an audience that values depth and detail in data exploration.</p>
<p><img src="comm.png" class="img-fluid"></p>
</section>
<section id="engaging-non-technical-audiences-with-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="engaging-non-technical-audiences-with-data">Engaging non-technical audiences with data</h4>
<p>Moving on to non-technical audiences, our focus shifts towards simplifying the insights derived from our data. It’s crucial to distill complex information into its essence, presenting it in a clear and straightforward manner. Use foundational visualizations like bar graphs and line charts, which are easier to interpret and highlight key points without the need for statistical jargon. When preparing presentations for a non-technical crowd, ensure that the content is audience-centric by highlighting why the data matters to them in practical terms. Connect the data insights to real-world applications and outcomes that resonate with their interests and professional challenges. This approach not only maintains relevance but also enhances engagement by aligning the presentation contents with their level of expertise and need for application rather than detailed analysis.</p>
<p><img src="eng.png" class="img-fluid"></p>
</section>
</section>
<section id="exercise-4.1.1" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="exercise-4.1.1"><span class="header-section-number">1.4.2</span> Exercise 4.1.1</h3>
<section id="visualizing-loan-approval-yield" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-loan-approval-yield">Visualizing loan approval yield</h4>
<p>In the realm of financial services, understanding the factors that influence loan approval rates is crucial for both lenders and borrowers. A financial institution has conducted a study and collected data on loan applications, detailing the amount requested, the applicant’s credit score, employment status, and the ultimate yield of the approval process. This rich dataset offers a window into the nuanced dynamics at play in loan decision-making. You have been asked to dive into the <code>loan_approval_yield</code> dataset to understand how loan amounts and credit scores influence approval yields.</p>
</section>
<section id="instructions-21" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-21">Instructions</h4>
<ol type="1">
<li><p>Create a side-by-side bar graph, setting the x-axis to <code>'LoanAmount'</code>, the y-axis to <code>'ApprovalYield'</code>, and differentiating the bars with hues for <code>'CreditScore'</code>.</p></li>
<li><p>Question: What does the analysis of approval yields across different credit scores and loan amounts reveal?</p></li>
</ol>
<p><em>The data shows that Poor credit scores tend to have similar approval yields across various loan amounts, while Good credit scores exhibit more variability, reflecting different lending criteria based on the loan size.</em></p>
<div id="0387a5c7" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>loan_approval_yield <span class="op">=</span> pd.read_csv(<span class="st">'datasets/loan_approval_yield.csv'</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Seaborn to create the bar graph</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>sns.catplot(x<span class="op">=</span><span class="st">"LoanAmount"</span>, </span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"ApprovalYield"</span>, </span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">"CreditScore"</span>, </span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>            kind<span class="op">=</span><span class="st">"bar"</span>, </span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>loan_approval_yield)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loan Approval Yield by Amount and Credit Score"</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-20-output-1.png" width="572" height="488" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.1.2" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="exercise-4.1.2"><span class="header-section-number">1.4.3</span> Exercise 4.1.2</h3>
<section id="exploring-customer-satisfaction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exploring-customer-satisfaction">Exploring customer satisfaction</h4>
<p>Merging datasets is a crucial skill in data analysis, especially when dealing with related data from different sources. You’re working on a project for a financial institution to understand the relationship between loan approval rates and customer satisfaction. Two separate studies have been conducted: one focusing on loan approval yield based on various factors, and another on customer satisfaction under different conditions. Your task is to analyze how approval yield correlates with customer satisfaction, considering another variable such as interest rates.</p>
</section>
<section id="instructions-22" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-22">Instructions</h4>
<ol type="1">
<li><p>Merge <code>loan_approval_yield</code> with <code>customer_satisfaction</code>.</p></li>
<li><p>Create a scatter plot to compare <code>'SatisfactionQuality'</code> versus <code>'ApprovalYield'</code>, coloring the points by <code>'InterestRate'</code>.</p></li>
</ol>
<div id="759b9956" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>loan_approval_yield <span class="op">=</span> pd.read_csv(<span class="st">'datasets/loan_approval_yield.csv'</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>customer_satisfaction <span class="op">=</span> pd.read_csv(<span class="st">'datasets/customer_satisfaction.csv'</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge the two datasets</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>merged_data <span class="op">=</span> pd.merge(loan_approval_yield, </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>                      customer_satisfaction, </span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>                      on<span class="op">=</span><span class="st">'ApplicationID'</span>)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Seaborn to create the scatter plot</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>sns.relplot(x<span class="op">=</span><span class="st">"ApprovalYield"</span>, </span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"SatisfactionQuality"</span>, </span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">"InterestRate"</span>, </span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>            kind<span class="op">=</span><span class="st">"scatter"</span>, </span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>merged_data)</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Satisfaction Quality by Approval Yield and Interest Rate"</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.png" width="565" height="488" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Question: What does the scatterplot of Customer Satisfaction versus Approval Yield, including Interest Rate as a variable, indicate about their relationship in the experimental data?</li>
</ol>
<p><em>There isn’t a strong relationship between Customer Satisfaction and Approval Yield in this experimental data. The resulting scatterplot looks similar to white noise scattered all about even when including Interest Rate</em></p>
</section>
</section>
<section id="chapter-4.2-addressing-complexities-in-experimental-data" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="chapter-4.2-addressing-complexities-in-experimental-data"><span class="header-section-number">1.4.4</span> Chapter 4.2: Addressing complexities in experimental data</h3>
<p>Next, we will look into addressing complexities in experimental data, focusing on identifying and mitigating issues like interactions, confounding variables, and heteroscedasticity.</p>
<section id="geological-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="geological-data">Geological data</h4>
<p>The <code>mineral_rocks</code> dataset encompasses 300 rock samples, detailing attributes like rock type, geographical location, mineral hardness, and rock porosity. Each entry in the dataset represents a unique sample, identified by its <code>SampleID</code>, and characterized by varying levels of <code>MineralHardness</code> and <code>RockPorosity</code> across different rock types and locations. Understanding the distribution and interactions within this data is critical for selecting the right statistical tests for our analysis.</p>
<pre><code>mineral_rocks

SampleID     RockType Location MineralHardness RockPorosity
       1  Metamorphic     West             5.9         12.3
       2      Igneous    North             5.3          1.6
       3  Metamorphic     East             5.6         11.0
       4  Metamorphic    South             3.2         12.2
       5  Sedimentary    South             2.0         29.8</code></pre>
</section>
<section id="understanding-data-complexities" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-data-complexities">Understanding data complexities</h4>
<p>Our exploration begins by identifying potential complexities within our <code>mineral_rocks</code> dataset: Interactions between rock types and their mineral hardness might influence the observed mineral properties. The variance in rock porosity, a key feature of our dataset, might not be consistent across all samples, indicating potential heteroscedasticity. There could be confounding variables that affect both mineral hardness and rock porosity. This is often the hardest problem to solve as it likely means that further data gathering is necessary to retrieve that extra variable information. Understanding these issues helps us decide whether parametric tests, which assume normality and homoscedasticity, can be employed or if we should rely on non-parametric tests, not assuming a specific distribution.</p>
<p><img src="rock.png" class="img-fluid"></p>
</section>
<section id="addressing-interactions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="addressing-interactions">Addressing interactions</h4>
<p>With the <code>mineral_rocks</code> dataset, we begin by visualizing the relationship between <code>MineralHardness</code> and <code>RockPorosity</code>, colored by <code>RockType</code>. This initial exploration helps identify potential complexities, such as interactions between variables. We seem to have an interaction between rock type and mineral hardness on rock porosity from the plot, since there are distinct groupings by <code>RockType</code>. Addressing interactions helps us understand whether more robust non-parametric methods are necessary for accurate analysis.</p>
<pre><code>sns.scatterplot(x='MineralHardness', y='RockPorosity',
hue='RockType', data=mineral_rocks)</code></pre>
<p><img src="graph.png" class="img-fluid"></p>
</section>
<section id="addressing-heteroscedasticity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="addressing-heteroscedasticity">Addressing heteroscedasticity</h4>
<p>Heteroscedasticity refers to the changing variability of a variable across the range of another variable. We use Seaborn’s <code>residplot</code> to check for heteroscedasticity in our data, plotting residuals of <code>RockPorosity</code> against <code>MineralHardness</code>. We include the <code>lowess</code> smoothing option to show the trend in the data going from left to right. We see that, overall, the <code>lowess</code> line remains somewhat close to 0 and relatively flat, but the curve does lead us to be a little cautious since it highlights the spread being different in some areas of our data.</p>
<pre><code>sns.residplot(x='MineralHardness', y='RockPorosity',
data=mineral_rocks, lowess=True)</code></pre>
<p><img src="graph2.png" class="img-fluid"></p>
</section>
<section id="non-normal-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="non-normal-data">Non-normal data</h4>
<p>When the residual plot deviates from expectations, it can be useful to explore the distribution of the variables used. Here, we investigate <code>RockPorosity</code> with a histogram using Seaborn’s <code>displot</code> function. We see that the data is skewed and of a non-normal shape.</p>
<pre><code>sns.displot(mineral_rocks['RockPorosity'])</code></pre>
<p><img src="graph3.png" class="img-fluid"></p>
</section>
<section id="data-transformation-with-box-cox" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-transformation-with-box-cox">Data transformation with Box-Cox</h4>
<p>To address issues like skewness and heteroscedasticity, we can apply data transformations. Here, we use the <code>Box-Cox</code> transformation from <code>scipy.stats</code> on <code>RockPorosity</code> to stabilize variance and make the data more closely resemble a normal distribution. We add the transformed data as a column to our DataFrame. The <code>Box-Cox</code> transformation requires non-zero entries, which we have for all <code>RockPorosity</code> values. Note that this transformed data isn’t perfectly normal, but does have much more of that bell shape than it did originally.</p>
<pre><code>from scipy.stats import boxcox

mineral_rocks['TransformedRockPorosity'], _ = boxcox(mineral_rocks['RockPorosity'])

sns.displot(mineral_rocks['TransformedRockPorosity'])</code></pre>
<p><img src="graph4.png" class="img-fluid"></p>
</section>
<section id="post-transformation-analysis" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="post-transformation-analysis">Post-transformation analysis</h4>
<p>To verify that we’ve better addressed the heteroscedasticity with the Box-Cox transformation, we can repeat our <code>residplot</code> with the <code>TransformedRockPorosity</code>. This visualization helps us understand whether the Box-Cox transformation has successfully stabilized the variance across the range of <code>MineralHardness</code>, an important assumption for many statistical tests. The <code>lowess</code> line is now much flatter, going from left to right across the plot. We can now feel more confident that this transformed data has better addressed heteroscedasticity than the non-transformed data.</p>
<pre><code>sns.residplot(x='MineralHardness', y='TransformedRockPorosity',
data=mineral_rocks, lowess=True)</code></pre>
<p><img src="graph5.png" class="img-fluid"></p>
</section>
</section>
<section id="exercise-4.2.1" class="level3" data-number="1.4.5">
<h3 data-number="1.4.5" class="anchored" data-anchor-id="exercise-4.2.1"><span class="header-section-number">1.4.5</span> Exercise 4.2.1</h3>
<section id="check-for-heteroscedasticity-in-shelf-life" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="check-for-heteroscedasticity-in-shelf-life">Check for heteroscedasticity in shelf life</h4>
<p>When examining food preservation methods, it’s crucial to understand how the variance of one variable, such as shelf life, might change across the range of another variable like nutrient retention. Identifying such patterns, known as heteroscedasticity, can provide insights into the consistency of preservation effects. The <code>food_preservation</code> dataset encapsulates the outcomes of various preservation methods on different food types, specifically highlighting the balance between nutrient retention and resultant shelf life.</p>
</section>
<section id="instructions-23" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-23">Instructions</h4>
<ul>
<li>Use an appropriate plot to check for heteroscedasticity between <code>'NutrientRetention'</code> and <code>'ShelfLife'</code>.</li>
</ul>
<div id="a4e47641" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for heteroscedasticity with a residual plot</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>sns.residplot(x<span class="op">=</span><span class="st">'NutrientRetention'</span>, y<span class="op">=</span><span class="st">'ShelfLife'</span>, </span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>food_preservation, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Residual Plot of Shelf Life and Nutrient Retention'</span>)</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Nutrient Retention (%)'</span>)</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-1.png" width="596" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.3" class="level3" data-number="1.4.6">
<h3 data-number="1.4.6" class="anchored" data-anchor-id="exercise-4.2.3"><span class="header-section-number">1.4.6</span> Exercise 4.2.3</h3>
<section id="exploring-and-transforming-shelf-life-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exploring-and-transforming-shelf-life-data">Exploring and transforming shelf life data</h4>
<p>Understanding the distribution of different variables in our data is a key aspect of any data work including experimental analysis. The <code>food_preservation</code> dataset captures various food preservation methods and their impact on nutrient retention and shelf life. A crucial aspect of this data involves the shelf life of preserved foods, which can vary significantly across different preservation methods and food types.</p>
</section>
<section id="instructions-24" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-24">Instructions</h4>
<ol type="1">
<li>Visualize the original distribution of the <code>'ShelfLife'</code> column.</li>
<li>Visualize the distribution of the <code>'ShelfLifeTransformed'</code>.</li>
</ol>
<div id="55ca0fb3" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> boxcox</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the original ShelfLife distribution</span></span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>sns.displot(food_preservation[<span class="st">'ShelfLife'</span>])</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Shelf Life Distribution'</span>)</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Box-Cox transformation</span></span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a>ShelfLifeTransformed, _ <span class="op">=</span> boxcox(food_preservation[<span class="st">'ShelfLife'</span>])</span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformed ShelfLife distribution</span></span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a>sns.displot(ShelfLifeTransformed)</span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Transformed Shelf Life Distribution'</span>)</span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-1.png" width="469" height="488" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 672x480 with 0 Axes&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-3.png" width="469" height="489" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-4.3-applying-nonparametric-tests-in-experimental-analysis" class="level3" data-number="1.4.7">
<h3 data-number="1.4.7" class="anchored" data-anchor-id="chapter-4.3-applying-nonparametric-tests-in-experimental-analysis"><span class="header-section-number">1.4.7</span> Chapter 4.3: Applying nonparametric tests in experimental analysis</h3>
<p>We’ll now explore the world of nonparametric tests, which are vital tools in situations where parametric test assumptions don’t hold.</p>
<section id="when-to-use-nonparametric-tests" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="when-to-use-nonparametric-tests">When to use nonparametric tests</h4>
<p>Nonparametric tests come into play when data challenges the usual assumptions of parametric tests. For example, they serve as an alternative to needing to transform data in order for normality assumptions to hold. They’re ideal for ordinal data or distributions far from normality, offering resilience against outliers and accommodating a wider range of data behaviors.</p>
</section>
<section id="exploring-nonparametric-methods" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exploring-nonparametric-methods">Exploring nonparametric methods</h4>
<p>When data doesn’t meet parametric assumptions, nonparametric methods offer a solution. The <code>Mann-Whitney U</code> Test is our go-to for comparing two independent groups - the non-parametric alternative to the independent two-sample <code>t-test</code>. When our experiment involves more than two groups with a numeric response, we turn to the <code>Kruskal-Wallis</code> Test - the non-parametric version of the one-way ANOVA test.</p>
<p><img src="image1.png" class="img-fluid"></p>
</section>
<section id="visualizing-nonparametric-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-nonparametric-data">Visualizing nonparametric data</h4>
<p>Visualizing nonparametric data effectively can reveal underlying patterns. Violin plots offer a comprehensive view of our data’s distribution across multiple groups. Let’s compare <code>MineralHardness</code> for Igneous and Metamorphic rocks from our data. We begin by using the <code>.isin()</code> method to extract these two groups of data into a DataFrame called <code>condensed_data</code>. Next, we use Seaborn’s <code>violinplot</code> function on the two variables of interest. This violin plot contrasts <code>MineralHardness</code> between metamorphic and igneous rocks. Notice that the violins for each do not have a normal shape mirrored vertically, but instead exhibit some skew. Metamorphic rocks show a greater hardness range and lower median than igneous rocks (denoted by the white line in the center of each “violin”). Igneous rocks display smaller hardness variability and higher median values.</p>
<pre><code>condensed_data = mineral_rocks[mineral_rocks['RockType'].isin(['Igneous', 'Metamorphic'])]

sns.violinplot(x='RockType', y='MineralHardness', data=condensed_data)</code></pre>
<p><img src="image2.png" class="img-fluid"></p>
</section>
<section id="visualizing-nonparametric-data-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-nonparametric-data-1">Visualizing nonparametric data</h4>
<p>Boxen plots are an extended version of box plots that provide more information about the shape of the distribution. We use Seaborn’s <code>boxenplot</code> function to display the distribution of <code>MineralHardness</code> across three rock types: metamorphic, igneous, and sedimentary. Sedimentary rocks show the smallest median hardness value, with outliers indicating some extreme values. Metamorphic rocks show the most skew of the three rock types and have a median hardness between that of sedimentary and igneous. They also have a wider interquartile range, indicating significant variability. Igneous rocks exhibit the highest median hardness and a narrower interquartile range, suggesting less variability.</p>
<pre><code>sns.boxenplot(x=
'RockType'
, y=
'MineralHardness'
, data=mineral_rocks)</code></pre>
<p><img src="image3.png" class="img-fluid"></p>
</section>
<section id="applying-nonparametric-tests---mann-whitney-u" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applying-nonparametric-tests---mann-whitney-u">Applying nonparametric tests - Mann Whitney U</h4>
<p>We perform the <code>Mann-Whitney U</code> test to compare the distributions of <code>MineralHardness</code> between igneous and sedimentary rocks using data from the <code>mineral_rocks</code> DataFrame. We select the hardness values corresponding to each rock type and apply the test to determine if there’s a statistically significant difference in their medians. The test returns a p-value of <code>0.9724</code>. The high p-value indicates that there is no significant difference in the median mineral hardness between igneous and sedimentary rocks at the common significance levels.</p>
<pre><code>from scipy.stats import mannwhitneyu, kruskal

u_stat, u_pval = mannwhitneyu(
mineral_rocks[mineral_rocks['RockType'] == 'Igneous']['MineralHardness'],
mineral_rocks[mineral_rocks['RockType'] == 'Sedimentary']['MineralHardness']
)

print(f"Mann-Whitney U test p-value: {u_pval:.4f}")

Mann-Whitney U test p-value: 0.9724</code></pre>
</section>
<section id="applying-nonparametric-tests---kruskal-wallis" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applying-nonparametric-tests---kruskal-wallis">Applying nonparametric tests - Kruskal-Wallis</h4>
<p>We apply the <code>Kruskal-Wallis</code> test, a nonparametric method, to determine if there are statistically significant differences in mineral hardness distributions across igneous, sedimentary, and metamorphic rock types from the <code>mineral_rocks</code> dataset. It computes the p-value for the hypothesis that the medians of all groups are equal. This test returns a p-value of <code>0.0630</code>, which indicates that there’s a suggestion of a difference in medians, but it does not reach the conventional significance threshold of <code>0.05</code>. Therefore, while there may be differences in mineral hardness by rock type, they are not statistically significant at the 5% level.</p>
<pre><code>k_stat, k_pval = kruskal(
mineral_rocks[mineral_rocks['RockType'] == 'Igneous']['MineralHardness'],
mineral_rocks[mineral_rocks['RockType'] == 'Sedimentary']['MineralHardness'],
mineral_rocks[mineral_rocks['RockType'] == 'Metamorphic']['MineralHardness']
)

print(f"Kruskal-Wallis test p-value: {k_pval:.4f}")

Kruskal-Wallis test p-value: 0.0630</code></pre>
</section>
</section>
<section id="exercise-4.3.1" class="level3" data-number="1.4.8">
<h3 data-number="1.4.8" class="anchored" data-anchor-id="exercise-4.3.1"><span class="header-section-number">1.4.8</span> Exercise 4.3.1</h3>
<section id="visualizing-and-testing-preservation-methods" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-and-testing-preservation-methods">Visualizing and testing preservation methods</h4>
<p>As a food scientist, you’re tasked with evaluating the effectiveness of different preservation methods on nutrient retention and how these methods impact shelf life. You have been provided with a dataset, food_preservation, that includes various types of food preserved by methods such as freezing and canning. Each entry in the dataset captures the nutrient retention and calculated shelf life for these foods, providing a unique opportunity to analyze the impacts of preservation techniques on food quality.</p>
</section>
<section id="instructions-25" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-25">Instructions</h4>
<ol type="1">
<li>Filter the DataFrame to include only Freezing and Canning rows.</li>
</ol>
<ul>
<li>Create a violin plot to visualize the distribution of nutrient retention for different preservation methods.</li>
</ul>
<ol start="2" type="1">
<li>Extract the nutrient retention values for both Freezing and Canning entries.</li>
</ol>
<ul>
<li>Perform a Mann Whitney U test to compare nutrient retention between Freezing and Canning methods.</li>
</ul>
<div id="875ce16a" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> mannwhitneyu</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to Freezing and Canning data</span></span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a>condensed_food_data <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>].isin([<span class="st">'Freezing'</span>, <span class="st">'Canning'</span>])]</span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a violin plot for nutrient retention by preservation method</span></span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a>sns.violinplot(data<span class="op">=</span>condensed_food_data, </span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>     x<span class="op">=</span><span class="st">"PreservationMethod"</span>, </span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span><span class="st">"NutrientRetention"</span>)</span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate nutrient retention for Freezing and Canning methods</span></span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a>freezing <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Freezing'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a>canning <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Canning'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Mann Whitney U test</span></span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a>u_stat, p_val <span class="op">=</span> mannwhitneyu(freezing, canning)</span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the p-value</span></span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mann Whitney U test p-value:"</span>, p_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-24-output-1.png" width="593" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Mann Whitney U test p-value: 0.2333133474790744</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The violin plot shows that the distribution and median values are similar across Freezing and Canning. The large p-value leads us to suspect that a statistical difference does not exist in the medians of nutrient retention for freezing versus canning preservation methods.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.3.2" class="level3" data-number="1.4.9">
<h3 data-number="1.4.9" class="anchored" data-anchor-id="exercise-4.3.2"><span class="header-section-number">1.4.9</span> Exercise 4.3.2</h3>
<section id="further-analyzing-food-preservation-techniques" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="further-analyzing-food-preservation-techniques">Further analyzing food preservation techniques</h4>
<p>In your role as a food scientist, you’re exploring into the comparative effects of various food preservation methods on nutrient retention, utilizing a <code>food_preservation</code> dataset that includes measurements from freezing, canning, and drying methods. This dataset has been crafted to incorporate variations in shelf life that depend on the nutrient retention values, reflecting real-world scenarios where preservation efficacy varies significantly. Your analysis will involve visually exploring these differences using advanced plotting techniques and nonparametric tests.</p>
</section>
<section id="instructions-26" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-26">Instructions</h4>
<ol type="1">
<li>Create a boxen plot to explore the distribution of nutrient retention across the three different preservation methods.</li>
</ol>
<div id="c2401120" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> kruskal</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb59-43"><a href="#cb59-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a boxen plot for nutrient retention by preservation</span></span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a>sns.boxenplot(data<span class="op">=</span>food_preservation, </span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a>      x<span class="op">=</span><span class="st">"PreservationMethod"</span>, </span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span><span class="st">"NutrientRetention"</span>)</span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb59-56"><a href="#cb59-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-57"><a href="#cb59-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate nutrient retention for each preservation method</span></span>
<span id="cb59-58"><a href="#cb59-58" aria-hidden="true" tabindex="-1"></a>freezing <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Freezing'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb59-59"><a href="#cb59-59" aria-hidden="true" tabindex="-1"></a>canning <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Canning'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb59-60"><a href="#cb59-60" aria-hidden="true" tabindex="-1"></a>drying <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Drying'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb59-61"><a href="#cb59-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-62"><a href="#cb59-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Kruskal-Wallis test</span></span>
<span id="cb59-63"><a href="#cb59-63" aria-hidden="true" tabindex="-1"></a>k_stat, k_pval <span class="op">=</span> kruskal(freezing, canning, drying)</span>
<span id="cb59-64"><a href="#cb59-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Kruskal-Wallis test p-value:"</span>, k_pval)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-25-output-1.png" width="593" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Kruskal-Wallis test p-value: 0.4070541794831697</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>By effectively visualizing and statistically analyzing the nutrient retention across different preservation methods, I’ve gained insights into how these methods impact food quality. The boxen plot provided a deeper understanding of the data’s distribution, and the Kruskal-Wallis test helped me assess the statistical differences between groups. The large p-value leads us to fail to conclude that a difference in the median values across the three groups of preservation methods exists for nutrient retention.</em></p>
</div>
</div>
</div>
<!-- -->

</section>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb61" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Experimental Design in Python"</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: "Lawal's Note"</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: "Associate Data Science Course in Python by DataCamp Inc"</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-03-02"</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="an">highlight-style:</span><span class="co"> pygments</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co">    geometry:</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - top=30mm</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="co">      - left=20mm</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 4</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-height: 3</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="co">    pdf-engine: xelatex</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a><span class="co">  docx: default</span></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: true</span></span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a><span class="co">  output: true</span></span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a><span class="co">  error: false</span></span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a><span class="co">  include: true</span></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a><span class="al">![](SOA_Experi.jpg)</span></span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a><span class="fu"># Experimental Design in Python</span></span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 1: Experimental Design Preliminaries {#sec-Chapter1}</span></span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>Building knowledge in experimental design allows you to test hypotheses with best-practice analytical tools and quantify the risk of your work. You’ll begin your journey by setting the foundations of what experimental design is and different experimental design setups such as blocking and stratification. You’ll then learn and apply visual and analytical tests for normality in experimental data.</span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.1: Setting up experiments {#sec-Chapter1.1}</span></span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>Hi! Welcome to this course on experimental design in Python.</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Experimental Design definition {.unnumbered}</span></span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>Experimental design is the process in which we carry out research in an objective and controlled fashion. The purpose of this is to ensure we can make specific conclusions in reference to a hypothesis we have.</span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a>1 <span class="co">[</span><span class="ot">https://www.sciencedirect.com/topics/earth-and-planetary-sciences/experimental-design</span><span class="co">](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/experimental-design)</span></span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Forming robust statements {.unnumbered}</span></span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a>Because we use objective tools, we need to use quantified language. Instead of using words like 'probably', 'likely', and 'small' when noting our conclusions, we should use precise and quantified language. This often takes the form of noting the percentage risk on a Type I error in the conclusion. Recall that Type I errors occur when we incorrectly reject the null hypothesis when it is actually true. In this course, you'll learn to design experiments and conduct statistical analyses such that you begin making precise statements about observed results and take informed actions as a result.</span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Why experimental design? {.unnumbered}</span></span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-57"><a href="#cb61-57" aria-hidden="true" tabindex="-1"></a>Experimental design is useful in many fields. Naturally, it is used in academia such as in medical research. It is also useful in many corporate contexts such as marketing and product analytics, which conduct lots of A/B tests. It is also used in agriculture and increasingly in government policy through the use of behavioral psychology experiments.</span>
<span id="cb61-58"><a href="#cb61-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-59"><a href="#cb61-59" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Some terminology... {.unnumbered}</span></span>
<span id="cb61-60"><a href="#cb61-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-61"><a href="#cb61-61" aria-hidden="true" tabindex="-1"></a>Before we begin our first topic, let's define some important terminology. Subjects are what we are experimenting on. It could be people, employees, or users on a website.</span>
<span id="cb61-62"><a href="#cb61-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-63"><a href="#cb61-63" aria-hidden="true" tabindex="-1"></a>A treatment is some change given to one group of subjects. We could call that group the treatment group. The control group is not given any change. This could be a placebo group, for example.</span>
<span id="cb61-64"><a href="#cb61-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-65"><a href="#cb61-65" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Assigning subjects to groups {.unnumbered}</span></span>
<span id="cb61-66"><a href="#cb61-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-67"><a href="#cb61-67" aria-hidden="true" tabindex="-1"></a>An important concept in experimental design is how to assign subjects to test groups. There are two ways we could do this. We could just split the dataset non-randomly into chunks and assign each chunk to a group. Or we could use random assignment to sample into our desired groups. Let's look at each option using a DataFrame of 200 subjects' heights where we want to split into two groups of 100 each.</span>
<span id="cb61-68"><a href="#cb61-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-69"><a href="#cb61-69" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Non-random assignment {.unnumbered}</span></span>
<span id="cb61-70"><a href="#cb61-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-71"><a href="#cb61-71" aria-hidden="true" tabindex="-1"></a>Let's try non-random assignment first. We can use <span class="in">`.iloc[]`</span> to slice the first 100 rows from heights and assign to group1 and the next 100 rows into group2. We can use pandas' describe method to check descriptive statistics of our groups. Concatenating the two results with <span class="in">`pd.concat()`</span> and <span class="in">`axis=1`</span> will allow for easier comparison. These groups appear very different! Looking at the mean row, we can see there's a 9cm difference. Because of the differences in these groups, it will be harder to confidently determine if any changes are due to the treatment intervention.</span>
<span id="cb61-72"><a href="#cb61-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-73"><a href="#cb61-73" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random assignment {.unnumbered}</span></span>
<span id="cb61-74"><a href="#cb61-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-75"><a href="#cb61-75" aria-hidden="true" tabindex="-1"></a>Let's now try random assignment. We can use pandas' sample method to create a sample of size n, or use the frac argument and specify a proportion of the dataset, between 0 and 1, to sample. We want two equally-sized groups, so we specify <span class="in">`frac=0.5`</span>. Using <span class="in">`n=100`</span> would also work here. We also set the replace argument to False, so samples aren't selected twice. The random_state argument allows the splits to be consistently reproduced. group2 can be made by dropping the ids in group1 from the overall DataFrame. Using the same comparison method we see much closer means.</span>
<span id="cb61-76"><a href="#cb61-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-77"><a href="#cb61-77" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Assignment summary {.unnumbered}</span></span>
<span id="cb61-78"><a href="#cb61-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-79"><a href="#cb61-79" aria-hidden="true" tabindex="-1"></a>This demonstrates the importance of randomly assigning subjects to groups. It means we can attribute observed changes to treatment interventions rather than natural differences between the group. We can use pandas' sample method to select randomly from a DataFrame, and then use pandas' describe method to check differences in group assignment.</span>
<span id="cb61-80"><a href="#cb61-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-81"><a href="#cb61-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.1</span></span>
<span id="cb61-82"><a href="#cb61-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-83"><a href="#cb61-83" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Non-random assignment of subjects {.unnumbered}</span></span>
<span id="cb61-84"><a href="#cb61-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-85"><a href="#cb61-85" aria-hidden="true" tabindex="-1"></a>An agricultural firm is conducting an experiment to measure how feeding sheep different types of grass affects their weight. They have asked for your help to properly set up the experiment. One of their managers has said you can perform the subject assignment by taking the top 250 rows from the DataFrame and that should be fine.</span>
<span id="cb61-86"><a href="#cb61-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-87"><a href="#cb61-87" aria-hidden="true" tabindex="-1"></a>Your task is to use your analytical skills to demonstrate why this might not be a good idea. Assign the subjects to two groups using non-random assignment (the first 250 rows) and observe the differences in descriptive statistics.</span>
<span id="cb61-88"><a href="#cb61-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-89"><a href="#cb61-89" aria-hidden="true" tabindex="-1"></a>You have received the DataFrame, <span class="in">`weights`</span> which has a column containing the <span class="in">`weight`</span> of the sheep and a unique <span class="in">`id`</span> column.</span>
<span id="cb61-90"><a href="#cb61-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-91"><a href="#cb61-91" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instructions</span></span>
<span id="cb61-92"><a href="#cb61-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-93"><a href="#cb61-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use DataFrame slicing to put the first 250 rows of <span class="in">`weights`</span> into <span class="in">`group1_non_rand`</span> and the remaining into <span class="in">`group2_non_rand`</span>.</span>
<span id="cb61-94"><a href="#cb61-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-95"><a href="#cb61-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generate descriptive statistics of the two groups and concatenate them into a single DataFrame.</span>
<span id="cb61-96"><a href="#cb61-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-97"><a href="#cb61-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print out to observe the differences.</span>
<span id="cb61-98"><a href="#cb61-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-99"><a href="#cb61-99" aria-hidden="true" tabindex="-1"></a>Note: Due to non-availability of weights DataFrame, I had to generates mine myself, the proper code should have been this below.</span>
<span id="cb61-100"><a href="#cb61-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-101"><a href="#cb61-101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-102"><a href="#cb61-102" aria-hidden="true" tabindex="-1"></a><span class="in"># Non-random assignment</span></span>
<span id="cb61-103"><a href="#cb61-103" aria-hidden="true" tabindex="-1"></a><span class="in">group1_non_rand = weights.iloc[0:250, :]</span></span>
<span id="cb61-104"><a href="#cb61-104" aria-hidden="true" tabindex="-1"></a><span class="in">group2_non_rand = weights.iloc[250:, :]</span></span>
<span id="cb61-105"><a href="#cb61-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-106"><a href="#cb61-106" aria-hidden="true" tabindex="-1"></a><span class="in"># Compare descriptive statistics of groups</span></span>
<span id="cb61-107"><a href="#cb61-107" aria-hidden="true" tabindex="-1"></a><span class="in">compare_df_non_rand = pd.concat([group1_non_rand['weight'].describe(), group2_non_rand['weight'].describe()], axis=1)</span></span>
<span id="cb61-108"><a href="#cb61-108" aria-hidden="true" tabindex="-1"></a><span class="in">compare_df_non_rand.columns = ['group1', 'group2']</span></span>
<span id="cb61-109"><a href="#cb61-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-110"><a href="#cb61-110" aria-hidden="true" tabindex="-1"></a><span class="in"># Print to assess</span></span>
<span id="cb61-111"><a href="#cb61-111" aria-hidden="true" tabindex="-1"></a><span class="in">print(compare_df_non_rand)</span></span>
<span id="cb61-112"><a href="#cb61-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-113"><a href="#cb61-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-116"><a href="#cb61-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-117"><a href="#cb61-117" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-118"><a href="#cb61-118" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-119"><a href="#cb61-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-120"><a href="#cb61-120" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group1 between 39.07 and 65.10</span></span>
<span id="cb61-121"><a href="#cb61-121" aria-hidden="true" tabindex="-1"></a>weights_group1 <span class="op">=</span> np.random.uniform(<span class="fl">39.07</span>, <span class="fl">65.10</span>, <span class="dv">250</span>)</span>
<span id="cb61-122"><a href="#cb61-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-123"><a href="#cb61-123" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group2 between 65.10 and 95.82</span></span>
<span id="cb61-124"><a href="#cb61-124" aria-hidden="true" tabindex="-1"></a>weights_group2 <span class="op">=</span> np.random.uniform(<span class="fl">65.10</span>, <span class="fl">95.82</span>, <span class="dv">250</span>)</span>
<span id="cb61-125"><a href="#cb61-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-126"><a href="#cb61-126" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrames for each group</span></span>
<span id="cb61-127"><a href="#cb61-127" aria-hidden="true" tabindex="-1"></a>group1_non_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-128"><a href="#cb61-128" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">251</span>),</span>
<span id="cb61-129"><a href="#cb61-129" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group1</span>
<span id="cb61-130"><a href="#cb61-130" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-131"><a href="#cb61-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-132"><a href="#cb61-132" aria-hidden="true" tabindex="-1"></a>group2_non_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-133"><a href="#cb61-133" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">251</span>, <span class="dv">501</span>),</span>
<span id="cb61-134"><a href="#cb61-134" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group2</span>
<span id="cb61-135"><a href="#cb61-135" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-136"><a href="#cb61-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-137"><a href="#cb61-137" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the two groups into one DataFrame</span></span>
<span id="cb61-138"><a href="#cb61-138" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.concat([group1_non_rand, group2_non_rand]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-139"><a href="#cb61-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-140"><a href="#cb61-140" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare descriptive statistics of groups</span></span>
<span id="cb61-141"><a href="#cb61-141" aria-hidden="true" tabindex="-1"></a>compare_df_non_rand <span class="op">=</span> pd.concat([group1_non_rand[<span class="st">'weight'</span>].describe(), group2_non_rand[<span class="st">'weight'</span>].describe()], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-142"><a href="#cb61-142" aria-hidden="true" tabindex="-1"></a>compare_df_non_rand.columns <span class="op">=</span> [<span class="st">'group1'</span>, <span class="st">'group2'</span>]</span>
<span id="cb61-143"><a href="#cb61-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-144"><a href="#cb61-144" aria-hidden="true" tabindex="-1"></a><span class="co"># Print to assess</span></span>
<span id="cb61-145"><a href="#cb61-145" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(compare_df_non_rand)</span>
<span id="cb61-146"><a href="#cb61-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-147"><a href="#cb61-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-148"><a href="#cb61-148" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-149"><a href="#cb61-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-150"><a href="#cb61-150" aria-hidden="true" tabindex="-1"></a>*Wow! Those two datasets have a much greater difference in means. It may be that the dataset was sorted before you received it. Presenting these results to the firm will help them understand best-practice group assignment. Hopefully you can now work with them to set up the experiment properly.*</span>
<span id="cb61-151"><a href="#cb61-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-152"><a href="#cb61-152" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-153"><a href="#cb61-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-154"><a href="#cb61-154" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.2</span></span>
<span id="cb61-155"><a href="#cb61-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-156"><a href="#cb61-156" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random assignment of subjects {.unnumbered}</span></span>
<span id="cb61-157"><a href="#cb61-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-158"><a href="#cb61-158" aria-hidden="true" tabindex="-1"></a>Having built trust from your last work with the agricultural firm, you have been given the task of properly setting up the experiment.</span>
<span id="cb61-159"><a href="#cb61-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-160"><a href="#cb61-160" aria-hidden="true" tabindex="-1"></a>Use your knowledge of best practice experimental design set up to assign the sheep to two even groups of 250 each.</span>
<span id="cb61-161"><a href="#cb61-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-162"><a href="#cb61-162" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instructions</span></span>
<span id="cb61-163"><a href="#cb61-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-164"><a href="#cb61-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Randomly select 250 subjects from the <span class="in">`weights`</span> DataFrame into a new DataFrame <span class="in">`group1`</span> without replacement.</span>
<span id="cb61-165"><a href="#cb61-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-166"><a href="#cb61-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Put the remaining 250 subjects into <span class="in">`group2`</span>.</span>
<span id="cb61-167"><a href="#cb61-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-168"><a href="#cb61-168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Concatenate the descriptive statistics of your two newly created DataFrames.</span>
<span id="cb61-169"><a href="#cb61-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-172"><a href="#cb61-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-173"><a href="#cb61-173" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-174"><a href="#cb61-174" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-175"><a href="#cb61-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-176"><a href="#cb61-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group1 between 39.07 and 65.10</span></span>
<span id="cb61-177"><a href="#cb61-177" aria-hidden="true" tabindex="-1"></a>weights_group1 <span class="op">=</span> np.random.uniform(<span class="fl">39.07</span>, <span class="fl">65.10</span>, <span class="dv">250</span>)</span>
<span id="cb61-178"><a href="#cb61-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-179"><a href="#cb61-179" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 250 random weights for group2 between 65.10 and 95.82</span></span>
<span id="cb61-180"><a href="#cb61-180" aria-hidden="true" tabindex="-1"></a>weights_group2 <span class="op">=</span> np.random.uniform(<span class="fl">65.10</span>, <span class="fl">95.82</span>, <span class="dv">250</span>)</span>
<span id="cb61-181"><a href="#cb61-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-182"><a href="#cb61-182" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrames for each group</span></span>
<span id="cb61-183"><a href="#cb61-183" aria-hidden="true" tabindex="-1"></a>group1_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-184"><a href="#cb61-184" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">251</span>),</span>
<span id="cb61-185"><a href="#cb61-185" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group1</span>
<span id="cb61-186"><a href="#cb61-186" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-187"><a href="#cb61-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-188"><a href="#cb61-188" aria-hidden="true" tabindex="-1"></a>group2_rand <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-189"><a href="#cb61-189" aria-hidden="true" tabindex="-1"></a>    <span class="st">'id'</span>: <span class="bu">range</span>(<span class="dv">251</span>, <span class="dv">501</span>),</span>
<span id="cb61-190"><a href="#cb61-190" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight'</span>: weights_group2</span>
<span id="cb61-191"><a href="#cb61-191" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-192"><a href="#cb61-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-193"><a href="#cb61-193" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the two groups into one DataFrame</span></span>
<span id="cb61-194"><a href="#cb61-194" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.concat([group1_rand, group2_rand]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-195"><a href="#cb61-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-196"><a href="#cb61-196" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign half</span></span>
<span id="cb61-197"><a href="#cb61-197" aria-hidden="true" tabindex="-1"></a>group1_random <span class="op">=</span> weights.sample(frac<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-198"><a href="#cb61-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-199"><a href="#cb61-199" aria-hidden="true" tabindex="-1"></a><span class="co"># Create second assignment</span></span>
<span id="cb61-200"><a href="#cb61-200" aria-hidden="true" tabindex="-1"></a>group2_random <span class="op">=</span> weights.drop(group1_random.index)</span>
<span id="cb61-201"><a href="#cb61-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-202"><a href="#cb61-202" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare assignments</span></span>
<span id="cb61-203"><a href="#cb61-203" aria-hidden="true" tabindex="-1"></a>compare_df_random <span class="op">=</span> pd.concat([group1_random[<span class="st">'weight'</span>].describe(), group2_random[<span class="st">'weight'</span>].describe()], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-204"><a href="#cb61-204" aria-hidden="true" tabindex="-1"></a>compare_df_random.columns <span class="op">=</span> [<span class="st">'group1'</span>, <span class="st">'group2'</span>]</span>
<span id="cb61-205"><a href="#cb61-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(compare_df_random)</span>
<span id="cb61-206"><a href="#cb61-206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-207"><a href="#cb61-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-208"><a href="#cb61-208" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-209"><a href="#cb61-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-210"><a href="#cb61-210" aria-hidden="true" tabindex="-1"></a>_While there are some differences in these datasets, you can clearly see the mean of the two sets are very close. This best-practice setup will ensure the experiment is on the right path from the beginning. Let's continue building foundational experimental design skills by learning about experimental design setup._</span>
<span id="cb61-211"><a href="#cb61-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-212"><a href="#cb61-212" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-213"><a href="#cb61-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-214"><a href="#cb61-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-215"><a href="#cb61-215" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.2: Experimental data setup {#sec-Chapter1.2}</span></span>
<span id="cb61-216"><a href="#cb61-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-217"><a href="#cb61-217" aria-hidden="true" tabindex="-1"></a>We've seen that randomization is often the best technique for setting up experimental data, but it isn't always.</span>
<span id="cb61-218"><a href="#cb61-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-219"><a href="#cb61-219" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The problem with randomization {.unnumbered}</span></span>
<span id="cb61-220"><a href="#cb61-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-221"><a href="#cb61-221" aria-hidden="true" tabindex="-1"></a>There are several scenarios where pure randomization can lead to undesirable outcomes. Firstly, when it results in uneven numbers of subjects in different groups, often seen more in smaller experiment sizes.</span>
<span id="cb61-222"><a href="#cb61-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-223"><a href="#cb61-223" aria-hidden="true" tabindex="-1"></a>Covariates are variables that potentially affect experiment results but aren't the primary focus. If covariates are highly variable or not equally distributed among groups, randomization might not produce balanced groups. This imbalance can lead to biased results. Overall these make it harder to see an effect from a treatment, as these issues may be driving an observed change.</span>
<span id="cb61-224"><a href="#cb61-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-225"><a href="#cb61-225" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block randomization {.unnumbered}</span></span>
<span id="cb61-226"><a href="#cb61-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-227"><a href="#cb61-227" aria-hidden="true" tabindex="-1"></a>A solution to our uneven problem is block randomization. This involves splitting into a block of size n first, then randomly splitting. This is what it looks like. Subjects are split into two groups, then randomly assigned to be Treatment (orange) or control (white). This fixes the uneven issue, and the smaller blocks give us more control over the allocation.</span>
<span id="cb61-228"><a href="#cb61-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-229"><a href="#cb61-229" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Our dataset {.unnumbered}</span></span>
<span id="cb61-230"><a href="#cb61-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-231"><a href="#cb61-231" aria-hidden="true" tabindex="-1"></a>Let's give block randomization a go on a dataset of 1000 members from an e-commerce site that contains variables for their average basket size in dollars, the average time spent on the website each day, and whether they are a power user. Power users spend an average of 40+ minutes on the website each day. There are 100 power users in these 1000 subjects.</span>
<span id="cb61-232"><a href="#cb61-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-233"><a href="#cb61-233" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block randomization in Python {.unnumbered}</span></span>
<span id="cb61-234"><a href="#cb61-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-235"><a href="#cb61-235" aria-hidden="true" tabindex="-1"></a>We can use pandas' sample method to randomly assign subjects into two blocks. A block column has also been added to both DataFrames for convenience. This produces even block sizes, fixing the uneven issue, but let's check for covariates.</span>
<span id="cb61-236"><a href="#cb61-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-237"><a href="#cb61-237" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing splits {.unnumbered}</span></span>
<span id="cb61-238"><a href="#cb61-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-239"><a href="#cb61-239" aria-hidden="true" tabindex="-1"></a>A nice way of checking for potential covariate issues is with visualizations. We can use seaborn's displot function to produce a kde (or kernal density) plot to visualize the distribution of the basket size, split by whether the user is a power user. There is quite a difference in the group distributions. It seems like the power_user variable could have an effect on basket size. When an effect could be because of a variable rather than the treatment, this is often called confounding. The covariate issue can be solved with stratified randomization.</span>
<span id="cb61-240"><a href="#cb61-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-241"><a href="#cb61-241" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stratified randomization {.unnumbered}</span></span>
<span id="cb61-242"><a href="#cb61-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-243"><a href="#cb61-243" aria-hidden="true" tabindex="-1"></a>Stratified randomization involves splitting based on a potentially confounding variable first, followed by randomization. This is what it may look like. Firstly, we split into two blocks (sometimes called strata) of power users, in green, and non-power users, in yellow. Then, inside the groups, randomly allocating to treatment or control. This fixes the uneven covariate issue, and can even be done for multiple covariates, but managing more strata does increase complexity.</span>
<span id="cb61-244"><a href="#cb61-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-245"><a href="#cb61-245" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Our first strata {.unnumbered}</span></span>
<span id="cb61-246"><a href="#cb61-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-247"><a href="#cb61-247" aria-hidden="true" tabindex="-1"></a>Let's stratify our power users. We separate them out first and label the block. We then sample half the power users to be in Treatment. The <span class="in">`T_C`</span>Let's stratify our power users. We separate them out first and label the block. We then sample half the power users to be in Treatment. The <span class="in">`T_C`</span> column notes this status. We then place the remaining into control by dropping the subjects in the treatment group.</span>
<span id="cb61-248"><a href="#cb61-248" aria-hidden="true" tabindex="-1"></a> column notes this status. We then place the remaining into control by dropping the subjects in the treatment group.</span>
<span id="cb61-249"><a href="#cb61-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-250"><a href="#cb61-250" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The second strata {.unnumbered}</span></span>
<span id="cb61-251"><a href="#cb61-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-252"><a href="#cb61-252" aria-hidden="true" tabindex="-1"></a>For our other strata, we separate out non-power users first and label the block differently. The rest of the code is the same as before. We allocate half to treatment and control using the same column headers.</span>
<span id="cb61-253"><a href="#cb61-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-254"><a href="#cb61-254" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Confirming stratification {.unnumbered}</span></span>
<span id="cb61-255"><a href="#cb61-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-256"><a href="#cb61-256" aria-hidden="true" tabindex="-1"></a>Let's bring our work together by firstly concatenating the strata and groups. We can confirm our work using groupby and chaining the <span class="in">`.size()`</span> method. This will show the number of power users in each block by their treatment or control status. We can see two blocks: one with all 100 power users and another with the other 900 users, split evenly into treatment and control groups.</span>
<span id="cb61-257"><a href="#cb61-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-258"><a href="#cb61-258" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.2.1</span></span>
<span id="cb61-259"><a href="#cb61-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-260"><a href="#cb61-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Blocking experimental data {.unnumbered}</span></span>
<span id="cb61-261"><a href="#cb61-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-262"><a href="#cb61-262" aria-hidden="true" tabindex="-1"></a>You are working with a manufacturing firm that wants to conduct some experiments on worker productivity. Their dataset only contains 100 rows, so it's important that experimental groups are balanced.</span>
<span id="cb61-263"><a href="#cb61-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-264"><a href="#cb61-264" aria-hidden="true" tabindex="-1"></a>This sounds like a great opportunity to use your knowledge of blocking to assist them. They have provided a <span class="in">`productivity_subjects`</span> DataFrame. Split the provided dataset into two even groups of 50 entries each.</span>
<span id="cb61-265"><a href="#cb61-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-266"><a href="#cb61-266" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-267"><a href="#cb61-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-268"><a href="#cb61-268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Randomly select 50 subjects from the <span class="in">`productivity_subjects`</span> DataFrame into a new DataFrame <span class="in">`block_1`</span> without replacement.</span>
<span id="cb61-269"><a href="#cb61-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-270"><a href="#cb61-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Set a new column, <span class="in">`block`</span> to 1 for the <span class="in">`block_1`</span> DataFrame.</span>
<span id="cb61-271"><a href="#cb61-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-272"><a href="#cb61-272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Assign the remaining subjects to a DataFrame called <span class="in">`block_2`</span> and set the <span class="in">`block`</span> column to 2 for this DataFrame.</span>
<span id="cb61-273"><a href="#cb61-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-274"><a href="#cb61-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Concatenate the blocks together into a single DataFrame, and print the count of each value in the <span class="in">`block`</span> column to confirm the blocking worked.</span>
<span id="cb61-275"><a href="#cb61-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-278"><a href="#cb61-278" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-279"><a href="#cb61-279" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-280"><a href="#cb61-280" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-281"><a href="#cb61-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-282"><a href="#cb61-282" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame with 100 rows and subject_id ranging from 1 to 100</span></span>
<span id="cb61-283"><a href="#cb61-283" aria-hidden="true" tabindex="-1"></a>productivity_subjects <span class="op">=</span> pd.DataFrame({<span class="st">'subject_id'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>)})</span>
<span id="cb61-284"><a href="#cb61-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-285"><a href="#cb61-285" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign half</span></span>
<span id="cb61-286"><a href="#cb61-286" aria-hidden="true" tabindex="-1"></a>block_1 <span class="op">=</span> productivity_subjects.sample(frac<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-287"><a href="#cb61-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-288"><a href="#cb61-288" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the block column</span></span>
<span id="cb61-289"><a href="#cb61-289" aria-hidden="true" tabindex="-1"></a>block_1[<span class="st">'block'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-290"><a href="#cb61-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-291"><a href="#cb61-291" aria-hidden="true" tabindex="-1"></a><span class="co"># Create second assignment and label</span></span>
<span id="cb61-292"><a href="#cb61-292" aria-hidden="true" tabindex="-1"></a>block_2 <span class="op">=</span> productivity_subjects.drop(block_1.index)</span>
<span id="cb61-293"><a href="#cb61-293" aria-hidden="true" tabindex="-1"></a>block_2[<span class="st">'block'</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb61-294"><a href="#cb61-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-295"><a href="#cb61-295" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate and print</span></span>
<span id="cb61-296"><a href="#cb61-296" aria-hidden="true" tabindex="-1"></a>productivity_combined <span class="op">=</span> pd.concat([block_1, block_2], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-297"><a href="#cb61-297" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(productivity_combined[<span class="st">'block'</span>].value_counts())</span>
<span id="cb61-298"><a href="#cb61-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-299"><a href="#cb61-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-300"><a href="#cb61-300" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.2.2</span></span>
<span id="cb61-301"><a href="#cb61-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-302"><a href="#cb61-302" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stratifying an experiment {.unnumbered}</span></span>
<span id="cb61-303"><a href="#cb61-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-304"><a href="#cb61-304" aria-hidden="true" tabindex="-1"></a>You are working with a government organization that wants to undertake an experiment around how some particular government policies impact the net wealth of individuals in a number of areas.</span>
<span id="cb61-305"><a href="#cb61-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-306"><a href="#cb61-306" aria-hidden="true" tabindex="-1"></a>They have approached you to help set up the experimental design. They have warned you that there is likely to be a small group of users who already have high net wealth and are concerned that this group might overshadow any experimental outcome observed. You know just what to do!</span>
<span id="cb61-307"><a href="#cb61-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-308"><a href="#cb61-308" aria-hidden="true" tabindex="-1"></a>Use your knowledge of experimental design to undertake block randomization, stratifying by the <span class="in">`high_wealth`</span> column in the provided <span class="in">`wealth_data`</span> DataFrame. There are 2000 rows in the DataFrame with 200 high net wealth subjects (<span class="in">`high_wealth`</span> is 1).</span>
<span id="cb61-309"><a href="#cb61-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-310"><a href="#cb61-310" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions 1/3 {.unnumbered}</span></span>
<span id="cb61-311"><a href="#cb61-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-312"><a href="#cb61-312" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create the first block which contains all the <span class="in">`high_wealth`</span> subjects and set the <span class="in">`Block`</span> column to 1.</span>
<span id="cb61-313"><a href="#cb61-313" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create two groups from this block randomly assigning the <span class="in">`high_wealth`</span> subjects to the Treatment (T) or control (C) group.</span>
<span id="cb61-314"><a href="#cb61-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-315"><a href="#cb61-315" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Repeat for the second block (all the not <span class="in">`high_wealth`</span> subjects), setting the <span class="in">`Block`</span> column to 2, and perform the group assignment (randomly assigning to Treatment (T) or control (C) group).</span>
<span id="cb61-316"><a href="#cb61-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-317"><a href="#cb61-317" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Concatenate the four groups created into <span class="in">`wealth_data_stratified`</span> in order of creation (strata 1 group 1, strata 1 group 2, etc.)</span>
<span id="cb61-318"><a href="#cb61-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-321"><a href="#cb61-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-322"><a href="#cb61-322" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-323"><a href="#cb61-323" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-324"><a href="#cb61-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-325"><a href="#cb61-325" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-326"><a href="#cb61-326" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb61-327"><a href="#cb61-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-328"><a href="#cb61-328" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of rows</span></span>
<span id="cb61-329"><a href="#cb61-329" aria-hidden="true" tabindex="-1"></a>n_rows <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb61-330"><a href="#cb61-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-331"><a href="#cb61-331" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of high net wealth subjects</span></span>
<span id="cb61-332"><a href="#cb61-332" aria-hidden="true" tabindex="-1"></a>n_high_wealth <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb61-333"><a href="#cb61-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-334"><a href="#cb61-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the 'high_wealth' column</span></span>
<span id="cb61-335"><a href="#cb61-335" aria-hidden="true" tabindex="-1"></a>high_wealth <span class="op">=</span> np.zeros(n_rows, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb61-336"><a href="#cb61-336" aria-hidden="true" tabindex="-1"></a>high_wealth[:n_high_wealth] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-337"><a href="#cb61-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-338"><a href="#cb61-338" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the 'high_wealth' column</span></span>
<span id="cb61-339"><a href="#cb61-339" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(high_wealth)</span>
<span id="cb61-340"><a href="#cb61-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-341"><a href="#cb61-341" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb61-342"><a href="#cb61-342" aria-hidden="true" tabindex="-1"></a>wealth_data <span class="op">=</span> pd.DataFrame({<span class="st">'high_wealth'</span>: high_wealth})</span>
<span id="cb61-343"><a href="#cb61-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-344"><a href="#cb61-344" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the first block</span></span>
<span id="cb61-345"><a href="#cb61-345" aria-hidden="true" tabindex="-1"></a>strata_1 <span class="op">=</span> wealth_data[wealth_data[<span class="st">'high_wealth'</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb61-346"><a href="#cb61-346" aria-hidden="true" tabindex="-1"></a>strata_1[<span class="st">'Block'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb61-347"><a href="#cb61-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-348"><a href="#cb61-348" aria-hidden="true" tabindex="-1"></a><span class="co"># Create two groups assigning to Treatment or Control</span></span>
<span id="cb61-349"><a href="#cb61-349" aria-hidden="true" tabindex="-1"></a>strata_1_g1 <span class="op">=</span> strata_1.sample(frac<span class="op">=</span><span class="fl">0.5</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-350"><a href="#cb61-350" aria-hidden="true" tabindex="-1"></a>strata_1_g1[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'T'</span></span>
<span id="cb61-351"><a href="#cb61-351" aria-hidden="true" tabindex="-1"></a>strata_1_g2 <span class="op">=</span> strata_1.drop(strata_1_g1.index)</span>
<span id="cb61-352"><a href="#cb61-352" aria-hidden="true" tabindex="-1"></a>strata_1_g2[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'C'</span></span>
<span id="cb61-353"><a href="#cb61-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-354"><a href="#cb61-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the second block and assign groups</span></span>
<span id="cb61-355"><a href="#cb61-355" aria-hidden="true" tabindex="-1"></a>strata_2 <span class="op">=</span> wealth_data[wealth_data[<span class="st">'high_wealth'</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb61-356"><a href="#cb61-356" aria-hidden="true" tabindex="-1"></a>strata_2[<span class="st">'Block'</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb61-357"><a href="#cb61-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-358"><a href="#cb61-358" aria-hidden="true" tabindex="-1"></a>strata_2_g1 <span class="op">=</span> strata_2.sample(frac<span class="op">=</span><span class="fl">0.5</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-359"><a href="#cb61-359" aria-hidden="true" tabindex="-1"></a>strata_2_g1[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'T'</span></span>
<span id="cb61-360"><a href="#cb61-360" aria-hidden="true" tabindex="-1"></a>strata_2_g2 <span class="op">=</span> strata_2.drop(strata_2_g1.index)</span>
<span id="cb61-361"><a href="#cb61-361" aria-hidden="true" tabindex="-1"></a>strata_2_g2[<span class="st">'T_C'</span>] <span class="op">=</span> <span class="st">'C'</span></span>
<span id="cb61-362"><a href="#cb61-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-363"><a href="#cb61-363" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the grouping work</span></span>
<span id="cb61-364"><a href="#cb61-364" aria-hidden="true" tabindex="-1"></a>wealth_data_stratified <span class="op">=</span> pd.concat([strata_1_g1, strata_1_g2, strata_2_g1, strata_2_g2])</span>
<span id="cb61-365"><a href="#cb61-365" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wealth_data_stratified.groupby([<span class="st">'Block'</span>,<span class="st">'T_C'</span>, <span class="st">'high_wealth'</span>]).size())</span>
<span id="cb61-366"><a href="#cb61-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-367"><a href="#cb61-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-368"><a href="#cb61-368" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution collapse="true"}</span>
<span id="cb61-369"><a href="#cb61-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-370"><a href="#cb61-370" aria-hidden="true" tabindex="-1"></a>_You were able to split your data into different blocks and then randomly assign to treatment and control. You can clearly see two blocks, where the first block has half the `high_wealth` subjects split into treatment and control. The same is seen in the second block for the other subjects_</span>
<span id="cb61-371"><a href="#cb61-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-372"><a href="#cb61-372" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-373"><a href="#cb61-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-374"><a href="#cb61-374" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.3: Normal data {#sec-Chapter1.3}</span></span>
<span id="cb61-375"><a href="#cb61-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-376"><a href="#cb61-376" aria-hidden="true" tabindex="-1"></a>Let's review the concept of normal data and how it relates to experimental analysis.</span>
<span id="cb61-377"><a href="#cb61-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-378"><a href="#cb61-378" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The normal distribution {.unnumbered}</span></span>
<span id="cb61-379"><a href="#cb61-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-380"><a href="#cb61-380" aria-hidden="true" tabindex="-1"></a>Normal data is drawn from a normal distribution, which has the familiar bell curve shape. The normal distribution is intrinsically linked to z-scores, which recall, is a standardized measure of how many standard deviations a value is from the population mean. The most common normal distribution used for z-scores has a mean of zero and a standard deviation of one. This answers questions such as 'How many standard deviations is this point from the mean?' and 'What is the probability of obtaining this score?'.</span>
<span id="cb61-381"><a href="#cb61-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-382"><a href="#cb61-382" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Normal data and statistical tests {.unnumbered}</span></span>
<span id="cb61-383"><a href="#cb61-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-384"><a href="#cb61-384" aria-hidden="true" tabindex="-1"></a>Normal data is an underlying assumption for many statistical tests, called parametric tests. There are also nonparametric tests that don't assume normal data.</span>
<span id="cb61-385"><a href="#cb61-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-386"><a href="#cb61-386" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Normal, Z, and alpha {.unnumbered}</span></span>
<span id="cb61-387"><a href="#cb61-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-388"><a href="#cb61-388" aria-hidden="true" tabindex="-1"></a>In hypothesis testing, alpha, or the significance level, is often closely linked to the normal distribution. For normal data, we can visually see the risk of error for a given significance level and compare that result to the p-value, which is related to the z-score. An <span class="in">`alpha`</span> of 0.05 on a standard two-tailed test represents a small region in the tails. It means there is a 5% risk of rejecting the null hypothesis when it is actually true - a so-called Type I error.</span>
<span id="cb61-389"><a href="#cb61-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-390"><a href="#cb61-390" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing normal data {.unnumbered}</span></span>
<span id="cb61-391"><a href="#cb61-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-392"><a href="#cb61-392" aria-hidden="true" tabindex="-1"></a>We can visually check data for normality using a kde (or kernel density) plot, available via Seaborn's <span class="in">`displot()`</span> function. On this salaries dataset, the data appears approximately normal.</span>
<span id="cb61-393"><a href="#cb61-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-394"><a href="#cb61-394" aria-hidden="true" tabindex="-1"></a><span class="fu">#### QQ plots {.unnumbered}</span></span>
<span id="cb61-395"><a href="#cb61-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-396"><a href="#cb61-396" aria-hidden="true" tabindex="-1"></a>A more statistically robust visual tool is a quantile-quantile, or QQ, plot. It plots the quantiles or sections of two distributions against each other. The qqplot function from statsmodels plots our data. Setting the dist argument to the normal distribution from scipy.stats compares our data against a standard normal distribution. If the distributions are similar, the dots in the QQ plot hug the line tightly. Our data again seems quite normal. Here is another example. The dots bow out at the ends, which means that the data is not very normal.</span>
<span id="cb61-397"><a href="#cb61-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-398"><a href="#cb61-398" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Tests for normality {.unnumbered}</span></span>
<span id="cb61-399"><a href="#cb61-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-400"><a href="#cb61-400" aria-hidden="true" tabindex="-1"></a>There are also various numerical hypothesis tests for normality. The Shapiro-Wilk test is known to be good for small datasets. The D'Agostino K-squared test uses kurtosis and skewness to determine normality. These terms relate to the symmetry and size of a distribution's tails, respectively. Anderson-Darling is another common test which returns a list of values, rather than just one so we can see normality at different levels of alpha. Each of these tests has a null hypothesis that the provided dataset is drawn from a normal distribution.</span>
<span id="cb61-401"><a href="#cb61-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-402"><a href="#cb61-402" aria-hidden="true" tabindex="-1"></a><span class="fu">#### A Shapiro-Wilk test {.unnumbered}</span></span>
<span id="cb61-403"><a href="#cb61-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-404"><a href="#cb61-404" aria-hidden="true" tabindex="-1"></a>Let's run one of these tests, the Shapiro-Wilk test. We import it from <span class="in">`scipy.stats`</span>, and set our <span class="in">`alpha`</span> at 0.05. The function takes a series of values and returns a test statistic and <span class="in">`p-value`</span>. The <span class="in">`p-value`</span> is greater than <span class="in">`alpha`</span>, so we have evidence our data that looked quite normal is normal. We fail to reject the null hypothesis and have evidence that the data sample is normal at the <span class="in">`alpha`</span> level of 0.05.</span>
<span id="cb61-405"><a href="#cb61-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-406"><a href="#cb61-406" aria-hidden="true" tabindex="-1"></a><span class="fu">#### An Anderson-Darling test {.unnumbered}</span></span>
<span id="cb61-407"><a href="#cb61-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-408"><a href="#cb61-408" aria-hidden="true" tabindex="-1"></a>To implement an Anderson-Darling test, we provide data and set the dist argument to norm to test for normality. The result object contains a test statistic and a range of critical values and significance levels. To interpret, we check the test statistic against each critical value. If the test statistic is higher than the critical value, the null hypothesis is rejected at that particular significance level, and the data is not normal. 0.2748 is less than all the critical values, so we fail to reject the null hypothesis and suspect that the data is normal.</span>
<span id="cb61-409"><a href="#cb61-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-410"><a href="#cb61-410" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.1</span></span>
<span id="cb61-411"><a href="#cb61-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-412"><a href="#cb61-412" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visual normality in an agricultural experiment {.unnumbered}</span></span>
<span id="cb61-413"><a href="#cb61-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-414"><a href="#cb61-414" aria-hidden="true" tabindex="-1"></a>You have been contracted by an agricultural firm conducting an experiment on 50 chickens, divided into four groups, each fed a different diet. Weight measurements were taken every second day for 20 days.</span>
<span id="cb61-415"><a href="#cb61-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-416"><a href="#cb61-416" aria-hidden="true" tabindex="-1"></a>You'll analyze chicken_data to assess normality, which will determine the suitability of parametric statistical tests, beginning with a visual examination of the data distribution. </span>
<span id="cb61-417"><a href="#cb61-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-418"><a href="#cb61-418" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions</span></span>
<span id="cb61-419"><a href="#cb61-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-420"><a href="#cb61-420" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Plot the distribution of the chickens' <span class="in">`weight`</span> using the kernel density estimation (KDE) to visualize normality.</span>
<span id="cb61-421"><a href="#cb61-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-422"><a href="#cb61-422" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Create a qq plot with a standard line of the chickens' <span class="in">`weight`</span> to assess normality visually.</span>
<span id="cb61-423"><a href="#cb61-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-424"><a href="#cb61-424" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Subset <span class="in">`chicken_data`</span> for a <span class="in">`'Time'`</span> of 2, and plot the KDE of <span class="in">`'weight'`</span> from <span class="in">`subset_data`</span> to check if data is normal across time.</span>
<span id="cb61-425"><a href="#cb61-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-428"><a href="#cb61-428" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-429"><a href="#cb61-429" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-430"><a href="#cb61-430" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-431"><a href="#cb61-431" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-432"><a href="#cb61-432" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-433"><a href="#cb61-433" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.gofplots <span class="im">import</span> qqplot</span>
<span id="cb61-434"><a href="#cb61-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-435"><a href="#cb61-435" aria-hidden="true" tabindex="-1"></a>chicken_data <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chick_weight.csv'</span>)</span>
<span id="cb61-436"><a href="#cb61-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-437"><a href="#cb61-437" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of the chickens' weight</span></span>
<span id="cb61-438"><a href="#cb61-438" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>chicken_data, x<span class="op">=</span><span class="st">'weight'</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb61-439"><a href="#cb61-439" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-440"><a href="#cb61-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-441"><a href="#cb61-441" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the qq plot of the chickens' weight</span></span>
<span id="cb61-442"><a href="#cb61-442" aria-hidden="true" tabindex="-1"></a>qqplot(data<span class="op">=</span>chicken_data[<span class="st">'weight'</span>], line<span class="op">=</span><span class="st">'s'</span>)</span>
<span id="cb61-443"><a href="#cb61-443" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-444"><a href="#cb61-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-445"><a href="#cb61-445" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset the data and plot the weight of the subset</span></span>
<span id="cb61-446"><a href="#cb61-446" aria-hidden="true" tabindex="-1"></a>subset_data <span class="op">=</span> chicken_data[chicken_data[<span class="st">'Time'</span>] <span class="op">==</span> <span class="dv">2</span>]</span>
<span id="cb61-447"><a href="#cb61-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-448"><a href="#cb61-448" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>subset_data, x<span class="op">=</span><span class="st">'weight'</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb61-449"><a href="#cb61-449" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-450"><a href="#cb61-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-451"><a href="#cb61-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-452"><a href="#cb61-452" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.2</span></span>
<span id="cb61-453"><a href="#cb61-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-454"><a href="#cb61-454" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Analytical normality in an agricultural experiment {.unnumbered}</span></span>
<span id="cb61-455"><a href="#cb61-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-456"><a href="#cb61-456" aria-hidden="true" tabindex="-1"></a>Carrying on from your previous work, your visual inspections of the data indicate it may not be a normal dataset overall, but that the initial time point may be.</span>
<span id="cb61-457"><a href="#cb61-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-458"><a href="#cb61-458" aria-hidden="true" tabindex="-1"></a>Build on your previous work by using analytical methods to determine the normality of the dataset.</span>
<span id="cb61-459"><a href="#cb61-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-460"><a href="#cb61-460" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-461"><a href="#cb61-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-462"><a href="#cb61-462" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Run a Shapiro-Wilk test of normality on the <span class="in">`'weight'`</span> column and print the test statistic and p-value.</span>
<span id="cb61-463"><a href="#cb61-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-464"><a href="#cb61-464" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Run an Anderson-Darling test for normality and print out the test statistic, significance levels, and critical values from the returned object.</span>
<span id="cb61-465"><a href="#cb61-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-468"><a href="#cb61-468" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-469"><a href="#cb61-469" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-470"><a href="#cb61-470" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> shapiro</span>
<span id="cb61-471"><a href="#cb61-471" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> anderson</span>
<span id="cb61-472"><a href="#cb61-472" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-473"><a href="#cb61-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-474"><a href="#cb61-474" aria-hidden="true" tabindex="-1"></a>chicken_data <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chick_weight.csv'</span>)</span>
<span id="cb61-475"><a href="#cb61-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-476"><a href="#cb61-476" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a Shapiro-Wilk normality test on the weight column</span></span>
<span id="cb61-477"><a href="#cb61-477" aria-hidden="true" tabindex="-1"></a>test_statistic, p_value <span class="op">=</span> shapiro(chicken_data[<span class="st">'weight'</span>])</span>
<span id="cb61-478"><a href="#cb61-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-479"><a href="#cb61-479" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p: </span><span class="sc">{</span><span class="bu">round</span>(p_value, <span class="dv">4</span>)<span class="sc">}</span><span class="ss"> test stat: </span><span class="sc">{</span><span class="bu">round</span>(test_statistic, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-480"><a href="#cb61-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-481"><a href="#cb61-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the Anderson-Darling test</span></span>
<span id="cb61-482"><a href="#cb61-482" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> anderson(x<span class="op">=</span> chicken_data[<span class="st">'weight'</span>], dist<span class="op">=</span><span class="st">'norm'</span>)</span>
<span id="cb61-483"><a href="#cb61-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-484"><a href="#cb61-484" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test statistic: </span><span class="sc">{</span><span class="bu">round</span>(result.statistic, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-485"><a href="#cb61-485" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Significance Levels: </span><span class="sc">{</span>result<span class="sc">.</span>significance_level<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-486"><a href="#cb61-486" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Critical Values: </span><span class="sc">{</span>result<span class="sc">.</span>critical_values<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-487"><a href="#cb61-487" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-488"><a href="#cb61-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-489"><a href="#cb61-489" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 2</span></span>
<span id="cb61-490"><a href="#cb61-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-491"><a href="#cb61-491" aria-hidden="true" tabindex="-1"></a>At a significance level of 0.05, does the Shapiro-Wilk test indicate the data is normally distributed? _No_</span>
<span id="cb61-492"><a href="#cb61-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-493"><a href="#cb61-493" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 4</span></span>
<span id="cb61-494"><a href="#cb61-494" aria-hidden="true" tabindex="-1"></a>Given the returned Anderson-Darling test result, what could you conclude at the 5% significance level</span>
<span id="cb61-495"><a href="#cb61-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-496"><a href="#cb61-496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>_The critical value which matches the significance level of 5 is 0.781. When compared to the Anderson-Darling test statistic (12.5451), the critical value is much smaller and so we reject the null hypothesis and can conclude the data is unlikely to have been drawn from a normal distribution._</span>
<span id="cb61-497"><a href="#cb61-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-498"><a href="#cb61-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-499"><a href="#cb61-499" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 2: Experimental Design Techniques {#sec-Chapter2}</span></span>
<span id="cb61-500"><a href="#cb61-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-501"><a href="#cb61-501" aria-hidden="true" tabindex="-1"></a>You'll delve into sophisticated experimental design techniques, focusing on factorial designs, randomized block designs, and covariate adjustments. These methodologies are instrumental in enhancing the accuracy, efficiency, and interpretability of experimental results. Through a combination of theoretical insights and practical applications, you'll acquire the skills needed to design, implement, and analyze complex experiments in various fields of research.</span>
<span id="cb61-502"><a href="#cb61-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-503"><a href="#cb61-503" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.1: Factorial designs: principles and applications {#sec-Chapter2.1}</span></span>
<span id="cb61-504"><a href="#cb61-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-505"><a href="#cb61-505" aria-hidden="true" tabindex="-1"></a>Welcome back! In this lesson, we'll explore factorial designs.</span>
<span id="cb61-506"><a href="#cb61-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-507"><a href="#cb61-507" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding factorial design {.unnumbered}</span></span>
<span id="cb61-508"><a href="#cb61-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-509"><a href="#cb61-509" aria-hidden="true" tabindex="-1"></a>Factorial designs allow for the simultaneous examination of multiple variables. In this setup, every possible combination of factor levels is tested, which not only measures the direct effects of each factor but also the interactions between them. In the example shown of plant growth in different conditions, implementing a factorial design will mean that we can test the effect of different factors on plant growth, including light conditions and fertilizer type, simultaneously, and identify interactions between them. These interactions can illuminate complex dynamics that might be overlooked in simpler experimental setups.</span>
<span id="cb61-510"><a href="#cb61-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-511"><a href="#cb61-511" aria-hidden="true" tabindex="-1"></a>1 Image Generated with DALL·E 3</span>
<span id="cb61-512"><a href="#cb61-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-513"><a href="#cb61-513" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Factorial design data example {.unnumbered}</span></span>
<span id="cb61-514"><a href="#cb61-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-515"><a href="#cb61-515" aria-hidden="true" tabindex="-1"></a>To explain this concept further, we'll work with this plant growth DataFrame. It has 120 rows and four columns: an identifier column, two factors, and one response/dependent variable. Both factors have two levels: <span class="in">`Light_Condition`</span> can be Full Sunlight or Partial Shade, and <span class="in">`Fertilizer_Type`</span> can be either Synthetic or Organic. The <span class="in">`Growth_cm`</span> column is the numeric response, or dependent variable in the experiment.</span>
<span id="cb61-516"><a href="#cb61-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-517"><a href="#cb61-517" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Organizing data to visualize interactions {.unnumbered}</span></span>
<span id="cb61-518"><a href="#cb61-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-519"><a href="#cb61-519" aria-hidden="true" tabindex="-1"></a>We next create a pivot table from the DataFrame using pandas' <span class="in">`pivot_table`</span> function. It aggregates the <span class="in">`Growth_cm`</span> values by taking their mean for each combination of <span class="in">`Light_Condition`</span> and <span class="in">`Fertilizer_Type`</span>. The resulting table displays these average outcomes, with light values as rows and fertilizer values as columns, illustrating how the growth varies across different levels of the two factors. For example, the value 19.869 represents the average growth for the combination of Full Sunlight from <span class="in">`Light_Condition`</span> and Synthetic from <span class="in">`Fertilizer_Type`</span>.</span>
<span id="cb61-520"><a href="#cb61-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-521"><a href="#cb61-521" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualize interactions with heatmap {.unnumbered}</span></span>
<span id="cb61-522"><a href="#cb61-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-523"><a href="#cb61-523" aria-hidden="true" tabindex="-1"></a>The Seaborn heatmap function paints a picture of how these factors interact, with the color intensity revealing the strength and direction of their interactions. Setting annot to True displays the numerical value of the cell, and 'coolwarm' is a color map that ranges from cooler, or bluer colors, to warmer or redder colors. Lastly, the format argument fmt is set to 'g' to avoid scientific notation.</span>
<span id="cb61-524"><a href="#cb61-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-525"><a href="#cb61-525" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpreting interactions {.unnumbered}</span></span>
<span id="cb61-526"><a href="#cb61-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-527"><a href="#cb61-527" aria-hidden="true" tabindex="-1"></a>The variation in outcomes when changing levels within a factor indicates an interaction. For instance, the decrease from Organic to Synthetic fertilizer within Full Sunlight (from 20.602 to 19.869) contrasts with the modest change within Partial Shade, illustrating how outcomes differ based on factor levels. The differing changes in outcomes between Full Sunlight and Partial Shade across <span class="in">`Fertilizer_Type`</span> suggest the factors interact, underscoring the need for nuanced strategies considering the interaction of factors.</span>
<span id="cb61-528"><a href="#cb61-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-529"><a href="#cb61-529" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Factorial designs vs. randomized block designs {.unnumbered}</span></span>
<span id="cb61-530"><a href="#cb61-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-531"><a href="#cb61-531" aria-hidden="true" tabindex="-1"></a>Let's conclude by comparing factorial designs to the randomized block design we saw earlier in the course, and that we'll dive deeper into in the next video. Factorial designs investigate multiple treatments and their interactions to understand their combined effects on outcomes. They aim to unravel the effects and interactions of various factors, crucial for complex scenarios with multiple influencing variables. In factorial designs, units experience all treatment combinations, offering thorough exploration but requiring more subjects as treatments grow. Randomized block designs utilize blocks to group similar subjects, minimizing confounding impacts and clearer treatment effects. The focus of randomized block designs is on enhancing experimental precision by managing within-block variability, aiding in the detection of treatment differences. Randomized block designs assign one treatment per unit within blocks, ensuring each treatment's presence in every block to control for block-related variance and bolster treatment effect assessments.</span>
<span id="cb61-532"><a href="#cb61-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-533"><a href="#cb61-533" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.1</span></span>
<span id="cb61-534"><a href="#cb61-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-535"><a href="#cb61-535" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding marketing campaign effectiveness {.unnumbered}</span></span>
<span id="cb61-536"><a href="#cb61-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-537"><a href="#cb61-537" aria-hidden="true" tabindex="-1"></a>Imagine you're a digital marketer analyzing data from a recent campaign to understand what messaging style and time of day yield the highest conversions. This analysis is crucial for guiding future marketing strategies, ensuring that your messages reach potential customers when they're most likely to engage. In this exercise, you're working with a dataset giving the outcomes of different messaging styles (<span class="in">`'Casual'`</span> versus <span class="in">`'Formal'`</span>) and times of day (<span class="in">`'Morning'`</span> versus <span class="in">`'Evening'`</span>) on conversion rates, a common scenario in marketing data analysis.</span>
<span id="cb61-538"><a href="#cb61-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-539"><a href="#cb61-539" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-540"><a href="#cb61-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-541"><a href="#cb61-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a pivot table with <span class="in">`'Messaging_Style'`</span> as the index and <span class="in">`'Time_of_Day'`</span> as the columns, computing the mean of Conversions.</span>
<span id="cb61-542"><a href="#cb61-542" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print this pivot table.</span>
<span id="cb61-543"><a href="#cb61-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-546"><a href="#cb61-546" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-547"><a href="#cb61-547" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-548"><a href="#cb61-548" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-549"><a href="#cb61-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-550"><a href="#cb61-550" aria-hidden="true" tabindex="-1"></a>marketing_data <span class="op">=</span> pd.read_feather(<span class="st">'datasets/marketing_data.feather'</span>)</span>
<span id="cb61-551"><a href="#cb61-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-552"><a href="#cb61-552" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pivot table for marketing campaign data</span></span>
<span id="cb61-553"><a href="#cb61-553" aria-hidden="true" tabindex="-1"></a>marketing_pivot <span class="op">=</span> marketing_data.pivot_table(</span>
<span id="cb61-554"><a href="#cb61-554" aria-hidden="true" tabindex="-1"></a>  values<span class="op">=</span><span class="st">'Conversions'</span>, </span>
<span id="cb61-555"><a href="#cb61-555" aria-hidden="true" tabindex="-1"></a>  index<span class="op">=</span><span class="st">'Messaging_Style'</span>, </span>
<span id="cb61-556"><a href="#cb61-556" aria-hidden="true" tabindex="-1"></a>  columns<span class="op">=</span><span class="st">'Time_of_Day'</span>, </span>
<span id="cb61-557"><a href="#cb61-557" aria-hidden="true" tabindex="-1"></a>  aggfunc<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb61-558"><a href="#cb61-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-559"><a href="#cb61-559" aria-hidden="true" tabindex="-1"></a><span class="co"># View the pivoted results</span></span>
<span id="cb61-560"><a href="#cb61-560" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(marketing_pivot)</span>
<span id="cb61-561"><a href="#cb61-561" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-562"><a href="#cb61-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-563"><a href="#cb61-563" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-564"><a href="#cb61-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-565"><a href="#cb61-565" aria-hidden="true" tabindex="-1"></a>**!Notice that the mean conversion is highest for the** `'Formal'` **messaging style in the** `'Evening'` **time of day.**</span>
<span id="cb61-566"><a href="#cb61-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-567"><a href="#cb61-567" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-568"><a href="#cb61-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-569"><a href="#cb61-569" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.2</span></span>
<span id="cb61-570"><a href="#cb61-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-571"><a href="#cb61-571" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Heatmap of campaign interactions {.unnumbered}</span></span>
<span id="cb61-572"><a href="#cb61-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-573"><a href="#cb61-573" aria-hidden="true" tabindex="-1"></a>Visualizing data can often reveal patterns that are not immediately obvious. In the context of marketing, understanding how different factors interact and affect the success of a campaign is vital. By creating a heatmap of conversions based on messaging style and time of day, you can quickly identify which combinations perform best and which ones need reevaluation. This visual tool is invaluable for marketing teams looking to optimize their strategies for maximum impact.</span>
<span id="cb61-574"><a href="#cb61-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-575"><a href="#cb61-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-576"><a href="#cb61-576" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-577"><a href="#cb61-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-578"><a href="#cb61-578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize interactions between <span class="in">`Messaging_Style`</span> and <span class="in">`Time_of_Day`</span> with respect to conversions by creating an annotated cool-warm heatmap of <span class="in">`marketing_pivot`</span>.</span>
<span id="cb61-579"><a href="#cb61-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-582"><a href="#cb61-582" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-583"><a href="#cb61-583" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-584"><a href="#cb61-584" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-585"><a href="#cb61-585" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-586"><a href="#cb61-586" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-587"><a href="#cb61-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-588"><a href="#cb61-588" aria-hidden="true" tabindex="-1"></a>marketing_data <span class="op">=</span> pd.read_feather(<span class="st">'datasets/marketing_data.feather'</span>)</span>
<span id="cb61-589"><a href="#cb61-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-590"><a href="#cb61-590" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pivot table for marketing campaign data</span></span>
<span id="cb61-591"><a href="#cb61-591" aria-hidden="true" tabindex="-1"></a>marketing_pivot <span class="op">=</span> marketing_data.pivot_table(</span>
<span id="cb61-592"><a href="#cb61-592" aria-hidden="true" tabindex="-1"></a>  values<span class="op">=</span><span class="st">'Conversions'</span>, </span>
<span id="cb61-593"><a href="#cb61-593" aria-hidden="true" tabindex="-1"></a>  index<span class="op">=</span><span class="st">'Messaging_Style'</span>, </span>
<span id="cb61-594"><a href="#cb61-594" aria-hidden="true" tabindex="-1"></a>  columns<span class="op">=</span><span class="st">'Time_of_Day'</span>, </span>
<span id="cb61-595"><a href="#cb61-595" aria-hidden="true" tabindex="-1"></a>  aggfunc<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb61-596"><a href="#cb61-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-597"><a href="#cb61-597" aria-hidden="true" tabindex="-1"></a><span class="co"># View the pivoted results</span></span>
<span id="cb61-598"><a href="#cb61-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(marketing_pivot)</span>
<span id="cb61-599"><a href="#cb61-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-600"><a href="#cb61-600" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize interactions with a heatmap</span></span>
<span id="cb61-601"><a href="#cb61-601" aria-hidden="true" tabindex="-1"></a>sns.heatmap(marketing_pivot, </span>
<span id="cb61-602"><a href="#cb61-602" aria-hidden="true" tabindex="-1"></a>         annot<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb61-603"><a href="#cb61-603" aria-hidden="true" tabindex="-1"></a>         cmap<span class="op">=</span><span class="st">'coolwarm'</span>,</span>
<span id="cb61-604"><a href="#cb61-604" aria-hidden="true" tabindex="-1"></a>         fmt<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb61-605"><a href="#cb61-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-606"><a href="#cb61-606" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-607"><a href="#cb61-607" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-608"><a href="#cb61-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-609"><a href="#cb61-609" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.2: Randomized block design: controlling variance {#sec-Chapter2.2}</span></span>
<span id="cb61-610"><a href="#cb61-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-611"><a href="#cb61-611" aria-hidden="true" tabindex="-1"></a>Next, we'll delve further into the concept of blocking in experimental design.</span>
<span id="cb61-612"><a href="#cb61-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-613"><a href="#cb61-613" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding blocking {.unnumbered}</span></span>
<span id="cb61-614"><a href="#cb61-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-615"><a href="#cb61-615" aria-hidden="true" tabindex="-1"></a>Blocking involves grouping experimental units, often with similar characteristics, to minimize variance within these groups. This ensures that each block, representing a specific level of the blocking factor, receives every treatment. This approach allows us to concentrate on the treatment effects while controlling for variance attributable to the blocking factor, thus improving the precision of our results.</span>
<span id="cb61-616"><a href="#cb61-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-617"><a href="#cb61-617" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block design data example {.unnumbered}</span></span>
<span id="cb61-618"><a href="#cb61-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-619"><a href="#cb61-619" aria-hidden="true" tabindex="-1"></a>For this athlete performance DataFrame of 200 rows, blocking is represented by <span class="in">`Initial_Fitness_Level`</span> with categories of <span class="in">`Beginner`</span>, <span class="in">`Intermediate`</span>, and <span class="in">`Advanced`</span>. <span class="in">`Muscle_Gain_kg`</span> is a numeric response variable measured on participants for the year prior to blocks being assigned.</span>
<span id="cb61-620"><a href="#cb61-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-621"><a href="#cb61-621" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Implementing randomized block design {.unnumbered}</span></span>
<span id="cb61-622"><a href="#cb61-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-623"><a href="#cb61-623" aria-hidden="true" tabindex="-1"></a>To implement a randomized block design, we'll group the rows into blocks based on the <span class="in">`Initial_Fitness_Level`</span> in this case, shuffle the rows within these blocks, and randomly assign a treatment. To shuffle the rows in each <span class="in">`Initial_Fitness_Level`</span> block, we start with <span class="in">`.groupby()`</span> on <span class="in">`Initial_Fitness_Level`</span>. To shuffle each row in that block, we chain the <span class="in">`.apply()`</span> method to the <span class="in">`groupby`</span>, and pass it a <span class="in">`lambda`</span> function that reads: <span class="in">`for each group`</span>, denoted by x, we sample all rows with <span class="in">`frac=1`</span>, effectively shuffling them. We reset the index to not have both an index and column called Block. The grouped data is ordered alphabetically by fitness level.</span>
<span id="cb61-624"><a href="#cb61-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-625"><a href="#cb61-625" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Implemented randomized blocks {.unnumbered}</span></span>
<span id="cb61-626"><a href="#cb61-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-627"><a href="#cb61-627" aria-hidden="true" tabindex="-1"></a>Then, within each block, we assign exercise program treatments randomly using <span class="in">`numpy.random.choice()`</span>. This method allows us to control for block effects while focusing on the differences caused by the treatments. Here is a sample of the implemented randomized block DataFrame with the treatment randomly applied within each block.</span>
<span id="cb61-628"><a href="#cb61-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-629"><a href="#cb61-629" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing treatment effects within blocks {.unnumbered}</span></span>
<span id="cb61-630"><a href="#cb61-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-631"><a href="#cb61-631" aria-hidden="true" tabindex="-1"></a>A boxplot is an effective tool for visualizing the distribution of treatment effects across different blocks. By plotting the <span class="in">`Muscle_Gain_kg`</span> variable versus the <span class="in">`Initial_Fitness_Level`</span>, coloring by <span class="in">`Treatment`</span>, we observe the central tendencies and variabilities within each block. Scanning this boxplot, we see similar median values throughout the blocks and treatments. The variability is a bit wider for some, though, such as Cardio for Advanced and Beginner.</span>
<span id="cb61-632"><a href="#cb61-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-633"><a href="#cb61-633" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ANOVA within blocks {.unnumbered}</span></span>
<span id="cb61-634"><a href="#cb61-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-635"><a href="#cb61-635" aria-hidden="true" tabindex="-1"></a>We can use ANOVA to statistically check for these differences. Let's set a significance level at 5% prior to reviewing our results. We group the DataFrame by the blocking column and then apply a <span class="in">`lambda`</span> function to each group. Within the lambda function, we perform a one-way ANOVA test between the <span class="in">`Muscle_Gain_kg`</span> values for each treatment within each block using <span class="in">`f_oneway`</span> from <span class="in">`scipy.stats`</span>. Finally, it returns the <span class="in">`F-statistic`</span> and <span class="in">`p-value`</span> for each block's ANOVA test. Each of the p-values are above the alpha significance level of 5%. This gives evidence that significant differences don't exist across treatments within blocks. This is an ideal goal when setting up randomized block design experiments.</span>
<span id="cb61-636"><a href="#cb61-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-637"><a href="#cb61-637" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing effects across blocks {.unnumbered}</span></span>
<span id="cb61-638"><a href="#cb61-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-639"><a href="#cb61-639" aria-hidden="true" tabindex="-1"></a>We can also look for differences in the outcome across randomized blocks. Here, we do not break down further by treatment. These boxplots look similar, so we might guess that none of the blocks has a significantly different mean outcome compared to the others.</span>
<span id="cb61-640"><a href="#cb61-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-641"><a href="#cb61-641" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ANOVA between blocks {.unnumbered}</span></span>
<span id="cb61-642"><a href="#cb61-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-643"><a href="#cb61-643" aria-hidden="true" tabindex="-1"></a>Next we compute the one-way ANOVA test across the blocks. It compares the <span class="in">`Muscle_Gain_kg`</span> values for each block separately to assess whether there are significant differences in means among the blocks. The function <span class="in">`f_oneway`</span> calculates the <span class="in">`F-statistic`</span> and associated <span class="in">`p-value`</span>, indicating the likelihood of observing the data if the null hypothesis of equal means across all blocks is true. A p-value greater than 0.05 supports what we saw with the boxplot - that there is no significant difference.</span>
<span id="cb61-644"><a href="#cb61-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-645"><a href="#cb61-645" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.1</span></span>
<span id="cb61-646"><a href="#cb61-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-647"><a href="#cb61-647" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Implementing a randomized block design {.unnumbered}</span></span>
<span id="cb61-648"><a href="#cb61-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-649"><a href="#cb61-649" aria-hidden="true" tabindex="-1"></a>The manufacturing firm you worked with earlier is still interested in conducting some experiments on worker productivity. Previously, the two blocks were set randomly. While this can work, it can be better to group subjects based on similar characteristics.</span>
<span id="cb61-650"><a href="#cb61-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-651"><a href="#cb61-651" aria-hidden="true" tabindex="-1"></a>The same employees are again loaded but this time in a DataFrame called <span class="in">`productivity`</span> including 1200 other colleagues. It also includes a worker <span class="in">`'productivity_score'`</span> column based on units produced per hour. This column was binned into three groups to generate blocks based on similar productivity values. The firm would like to apply a new incentive program with three options (<span class="in">`'Bonus'`</span>, <span class="in">`'Profit Sharing'`</span> and <span class="in">`'Work from Home'`</span>) throughout the firm with treatment applied randomly.</span>
<span id="cb61-652"><a href="#cb61-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-653"><a href="#cb61-653" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-654"><a href="#cb61-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-655"><a href="#cb61-655" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shuffle the blocks to create a new DataFrame called <span class="in">`prod_df`</span>.</span>
<span id="cb61-656"><a href="#cb61-656" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reset the index so that <span class="in">`block`</span> is not both an index and a column.</span>
<span id="cb61-657"><a href="#cb61-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Randomly assign the three treatment values in the <span class="in">`'Treatment'`</span> column.</span>
<span id="cb61-658"><a href="#cb61-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-659"><a href="#cb61-659" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.2</span></span>
<span id="cb61-660"><a href="#cb61-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-661"><a href="#cb61-661" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing productivity within blocks by incentive {.unnumbered}</span></span>
<span id="cb61-662"><a href="#cb61-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-663"><a href="#cb61-663" aria-hidden="true" tabindex="-1"></a>Continuing with the worker productivity example, you'll explore if the productivity scores are distributed throughout the data as one would expect with random assignment of treatment. Note that this is a precautionary step, and the treatment and follow-up results on the impact of the three treatments is not done yet!</span>
<span id="cb61-664"><a href="#cb61-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-665"><a href="#cb61-665" aria-hidden="true" tabindex="-1"></a>seaborn and matplotlib.pyplot as sns and plt respectively are loaded.</span>
<span id="cb61-666"><a href="#cb61-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-667"><a href="#cb61-667" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions </span></span>
<span id="cb61-668"><a href="#cb61-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-669"><a href="#cb61-669" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize the productivity scores within blocks by treatment using a boxplot with <span class="in">`'block'`</span> for <span class="in">`x`</span>, <span class="in">`'productivity_score'`</span> for <span class="in">`y`</span>, and <span class="in">`'Treatment'`</span> for <span class="in">`hue`</span>.</span>
<span id="cb61-670"><a href="#cb61-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-671"><a href="#cb61-671" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.3</span></span>
<span id="cb61-672"><a href="#cb61-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-673"><a href="#cb61-673" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ANOVA within blocks of employees {.unnumbered}</span></span>
<span id="cb61-674"><a href="#cb61-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-675"><a href="#cb61-675" aria-hidden="true" tabindex="-1"></a>Building on your previous analyses with the manufacturing firm, where worker productivity was examined across different blocks and an incentive program was introduced, you're now delving deeper into the data. The firm, equipped with a more comprehensive dataset in the productivity DataFrame, including 1200 additional employees and their productivity_score, has structured the workforce into three blocks based on productivity levels. Each employee has been randomly assigned one of three incentive options: 'Bonus', 'Profit Sharing', or 'Work from Home'.</span>
<span id="cb61-676"><a href="#cb61-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-677"><a href="#cb61-677" aria-hidden="true" tabindex="-1"></a>Before assessing the full impact of these incentive treatments on productivity, it's crucial to verify that the initial treatment assignment was indeed random and equitable across the different productivity blocks. This step ensures that any observed differences in productivity post-treatment can be confidently attributed to the incentive programs themselves, rather than pre-existing disparities in the blocks.</span>
<span id="cb61-678"><a href="#cb61-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-679"><a href="#cb61-679" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-680"><a href="#cb61-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-681"><a href="#cb61-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Group <span class="in">`prod_df`</span> by the appropriate column that represents different blocks in your data.</span>
<span id="cb61-682"><a href="#cb61-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use a lambda function to apply the ANOVA test within each <span class="in">`block`</span>, specifying the <span class="in">`lambda`</span> function's argument.</span>
<span id="cb61-683"><a href="#cb61-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For each treatment group within the blocks, filter <span class="in">`prod_df`</span> based on the <span class="in">`'Treatment'`</span> column values and select the <span class="in">`'productivity_score'`</span> column.</span>
<span id="cb61-684"><a href="#cb61-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-687"><a href="#cb61-687" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-688"><a href="#cb61-688" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-689"><a href="#cb61-689" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-690"><a href="#cb61-690" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-691"><a href="#cb61-691" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-692"><a href="#cb61-692" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb61-693"><a href="#cb61-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-694"><a href="#cb61-694" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-695"><a href="#cb61-695" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb61-696"><a href="#cb61-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-697"><a href="#cb61-697" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb61-698"><a href="#cb61-698" aria-hidden="true" tabindex="-1"></a>subject_id <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">1301</span>)</span>
<span id="cb61-699"><a href="#cb61-699" aria-hidden="true" tabindex="-1"></a>productivity_score <span class="op">=</span> np.random.uniform(<span class="fl">1.1</span>, <span class="fl">30.0</span>, <span class="dv">1300</span>)</span>
<span id="cb61-700"><a href="#cb61-700" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1300</span>)</span>
<span id="cb61-701"><a href="#cb61-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-702"><a href="#cb61-702" aria-hidden="true" tabindex="-1"></a>productivity <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-703"><a href="#cb61-703" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subject_id'</span>: subject_id,</span>
<span id="cb61-704"><a href="#cb61-704" aria-hidden="true" tabindex="-1"></a>    <span class="st">'productivity_score'</span>: productivity_score,</span>
<span id="cb61-705"><a href="#cb61-705" aria-hidden="true" tabindex="-1"></a>    <span class="st">'block'</span>: block</span>
<span id="cb61-706"><a href="#cb61-706" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-707"><a href="#cb61-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-708"><a href="#cb61-708" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign workers to blocks</span></span>
<span id="cb61-709"><a href="#cb61-709" aria-hidden="true" tabindex="-1"></a>prod_df <span class="op">=</span> productivity.groupby(<span class="st">'block'</span>).<span class="bu">apply</span>(</span>
<span id="cb61-710"><a href="#cb61-710" aria-hidden="true" tabindex="-1"></a>  <span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-711"><a href="#cb61-711" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-712"><a href="#cb61-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-713"><a href="#cb61-713" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the index</span></span>
<span id="cb61-714"><a href="#cb61-714" aria-hidden="true" tabindex="-1"></a>prod_df <span class="op">=</span> prod_df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-715"><a href="#cb61-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-716"><a href="#cb61-716" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign treatment randomly</span></span>
<span id="cb61-717"><a href="#cb61-717" aria-hidden="true" tabindex="-1"></a>prod_df[<span class="st">'Treatment'</span>] <span class="op">=</span> np.random.choice(</span>
<span id="cb61-718"><a href="#cb61-718" aria-hidden="true" tabindex="-1"></a>  [<span class="st">'Bonus'</span>, <span class="st">'Profit Sharing'</span>, <span class="st">'Work from Home'</span>],</span>
<span id="cb61-719"><a href="#cb61-719" aria-hidden="true" tabindex="-1"></a>  size<span class="op">=</span><span class="bu">len</span>(prod_df)</span>
<span id="cb61-720"><a href="#cb61-720" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-721"><a href="#cb61-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-722"><a href="#cb61-722" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample Boxplot</span></span>
<span id="cb61-723"><a href="#cb61-723" aria-hidden="true" tabindex="-1"></a>sns.boxplot(x<span class="op">=</span><span class="st">'block'</span>, </span>
<span id="cb61-724"><a href="#cb61-724" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">'productivity_score'</span>, </span>
<span id="cb61-725"><a href="#cb61-725" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">'Treatment'</span>, </span>
<span id="cb61-726"><a href="#cb61-726" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>prod_df)</span>
<span id="cb61-727"><a href="#cb61-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-728"><a href="#cb61-728" aria-hidden="true" tabindex="-1"></a><span class="co"># Move the legend outside the plot</span></span>
<span id="cb61-729"><a href="#cb61-729" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Treatment'</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb61-730"><a href="#cb61-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-731"><a href="#cb61-731" aria-hidden="true" tabindex="-1"></a><span class="co"># Show plot</span></span>
<span id="cb61-732"><a href="#cb61-732" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-733"><a href="#cb61-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-734"><a href="#cb61-734" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the within blocks ANOVA, first grouping by block</span></span>
<span id="cb61-735"><a href="#cb61-735" aria-hidden="true" tabindex="-1"></a>within_block_anova <span class="op">=</span> prod_df.groupby(<span class="st">'block'</span>).<span class="bu">apply</span>(</span>
<span id="cb61-736"><a href="#cb61-736" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Set function</span></span>
<span id="cb61-737"><a href="#cb61-737" aria-hidden="true" tabindex="-1"></a>  <span class="kw">lambda</span> x: f_oneway(</span>
<span id="cb61-738"><a href="#cb61-738" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter Treatment values based on outcome</span></span>
<span id="cb61-739"><a href="#cb61-739" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Bonus'</span>][<span class="st">'productivity_score'</span>], </span>
<span id="cb61-740"><a href="#cb61-740" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Profit Sharing'</span>][<span class="st">'productivity_score'</span>],</span>
<span id="cb61-741"><a href="#cb61-741" aria-hidden="true" tabindex="-1"></a>    x[x[<span class="st">'Treatment'</span>] <span class="op">==</span> <span class="st">'Work from Home'</span>][<span class="st">'productivity_score'</span>])</span>
<span id="cb61-742"><a href="#cb61-742" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-743"><a href="#cb61-743" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(within_block_anova)</span>
<span id="cb61-744"><a href="#cb61-744" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-745"><a href="#cb61-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-746"><a href="#cb61-746" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-747"><a href="#cb61-747" aria-hidden="true" tabindex="-1"></a>**An ANOVA analysis was performed to compare productivity scores across different blocks for the three treatment groups. The results show that all three p-values exceed the alpha threshold of 0.05, indicating no significant differences in productivity scores among the treatment groups within the blocks.**</span>
<span id="cb61-748"><a href="#cb61-748" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-749"><a href="#cb61-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-750"><a href="#cb61-750" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.3: Covariate adjustment in experimental design {#sec-Chapter2.3}</span></span>
<span id="cb61-751"><a href="#cb61-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-752"><a href="#cb61-752" aria-hidden="true" tabindex="-1"></a>Let's now explore covariates in experimental design and analysis, and how they can be used to minimize confounding. We'll also learn about ANCOVA, or analysis of covariance, for evaluating treatment effects while controlling for covariates.</span>
<span id="cb61-753"><a href="#cb61-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-754"><a href="#cb61-754" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Introduction to covariates {.unnumbered}</span></span>
<span id="cb61-755"><a href="#cb61-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-756"><a href="#cb61-756" aria-hidden="true" tabindex="-1"></a>Recall that covariates are variables that are not of primary interest but are related to the outcome variable and can influence its analysis. Including covariates in statistical analyses is crucial for reducing confounding, which occurs when an external variable influences both the dependent variable and independent variable(s). By adjusting for covariates, researchers can isolate the effect of the independent variable on the outcome, minimizing the influence of confounders. Accounting for covariates in experimental design and analysis controls for variability that is not attributable to the primary variables being studied. This leads to more valid conclusions about the relationship between the independent and dependent variables, as the analysis better reflects the true effect by isolating it from the influence of covariates. Consider the investigation of a new teaching method's effectiveness on student test scores. Here, the primary variables of interest are the teaching method (independent variable) and the student test scores (dependent variable). However, students' prior subject knowledge serves as a crucial covariate because prior knowledge can significantly impact learning outcomes, yet it's not the main focus of the study.</span>
<span id="cb61-757"><a href="#cb61-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-758"><a href="#cb61-758" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Experimental data example {.unnumbered}</span></span>
<span id="cb61-759"><a href="#cb61-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-760"><a href="#cb61-760" aria-hidden="true" tabindex="-1"></a>Let's bring back our plant growth data and set it to experimental data as the <span class="in">`exp_data`</span> DataFrame, keeping <span class="in">`Fertilizer_Type`</span> as treatment and <span class="in">`Growth_cm`</span> as response.</span>
<span id="cb61-761"><a href="#cb61-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-762"><a href="#cb61-762" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Covariate data example {.unnumbered}</span></span>
<span id="cb61-763"><a href="#cb61-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-764"><a href="#cb61-764" aria-hidden="true" tabindex="-1"></a>The covariate_data DataFrame also includes <span class="in">`Plant_ID`</span> identifiers for each subject, again ranging from 1 to 120, ensuring each subject's covariate data is matched with their experimental data. <span class="in">`Watering_Days_Per_Week`</span> is another variable measured for each plant. Recall that covariates are additional variables potentially influencing the outcome and are included in analyses to control for their effects.</span>
<span id="cb61-765"><a href="#cb61-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-766"><a href="#cb61-766" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Combining experimental data with covariates {.unnumbered}</span></span>
<span id="cb61-767"><a href="#cb61-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-768"><a href="#cb61-768" aria-hidden="true" tabindex="-1"></a>Combining the experimental with covariate data is a crucial step in adjusting for covariates. We use <span class="in">`pandas`</span>' <span class="in">`merge`</span> function to combine DataFrames; we do this on the <span class="in">`Plant_ID`</span> to ensure each that subject's experimental and covariate data are aligned.</span>
<span id="cb61-769"><a href="#cb61-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-770"><a href="#cb61-770" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adjusting for covariates {.unnumbered}</span></span>
<span id="cb61-771"><a href="#cb61-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-772"><a href="#cb61-772" aria-hidden="true" tabindex="-1"></a>To adjust for covariates in our analysis, we employ ANCOVA, or analysis of covariance, using the ols model from statsmodels. This <span class="in">`ols()`</span> function takes a formula that specifies the dependent and independent variables. <span class="in">`Growth_cm`</span> is the dependent variable we're interested in, which we want to model using the <span class="in">`Fertilizer_Type`</span>, the categorical independent variable representing different groups in the experiment, and the potential covariate, <span class="in">`Watering_Days_Per_Week`</span>, to control for its effects. The first portion of summary output provides details on the significance of the model; it show a large p-value here of 0.531, which implies a lack of support for covariates affecting the model.</span>
<span id="cb61-773"><a href="#cb61-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-774"><a href="#cb61-774" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Further exploring ANCOVA results {.unnumbered}</span></span>
<span id="cb61-775"><a href="#cb61-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-776"><a href="#cb61-776" aria-hidden="true" tabindex="-1"></a>Looking at the second and third rows of this second portion of output from summary, we see that the factors and covariate each have large p-values of 0.760 and 0.275, concluding that each of them alone are not significant predictors of growth for this model.</span>
<span id="cb61-777"><a href="#cb61-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-778"><a href="#cb61-778" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing treatment effects with covariate adjustment {.unnumbered}</span></span>
<span id="cb61-779"><a href="#cb61-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-780"><a href="#cb61-780" aria-hidden="true" tabindex="-1"></a>This seaborn lmplot shows treatment effects adjusted for the covariate. The regression lines for each treatment category offer a visual representation of how treatment effects trend across different levels of the covariate. We see that <span class="in">`Organic`</span> remains relatively constant going from 1 watering to 7 <span class="in">`Watering_Days_Per_Week`</span>. Synthetic shows an increase. The crossing regression lines suggest we may want to add an interaction term of <span class="in">`Watering_Days_Per_Week`</span> by <span class="in">`Fertilizer_Type`</span> in another model. Parallel lines would suggest a lack of interaction.</span>
<span id="cb61-781"><a href="#cb61-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-782"><a href="#cb61-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-783"><a href="#cb61-783" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.3.1</span></span>
<span id="cb61-784"><a href="#cb61-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-785"><a href="#cb61-785" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Covariate adjustment with chick growth {.unnumbered}</span></span>
<span id="cb61-786"><a href="#cb61-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-787"><a href="#cb61-787" aria-hidden="true" tabindex="-1"></a>Imagine studying in agricultural science the growth patterns of chicks under various dietary regimens. The data from this study sheds light on the intricate relationship between their respective diets and the consequent impact on their weight. This data includes weight measurements of chicks at different ages, allowing for an exploration of covariate adjustment. <span class="in">`age`</span> serves as a covariate, potentially influencing the outcome variable: the <span class="in">`weight`</span> of the chicks.</span>
<span id="cb61-788"><a href="#cb61-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-789"><a href="#cb61-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-790"><a href="#cb61-790" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-791"><a href="#cb61-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-792"><a href="#cb61-792" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Join the experimental and covariate data based on common column(s), and print this merged data.</span>
<span id="cb61-793"><a href="#cb61-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-794"><a href="#cb61-794" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Produce an ANCOVA predicting <span class="in">`'weight'`</span> based on <span class="in">`'Diet'`</span> and <span class="in">`'Time'`</span>.</span>
<span id="cb61-795"><a href="#cb61-795" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Print a summary of the ANCOVA model.</span>
<span id="cb61-796"><a href="#cb61-796" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-797"><a href="#cb61-797" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Design an <span class="in">`lmplot`</span> to see <span class="in">`hue='Diet'`</span> effects on <span class="in">`y='weight'`</span> adjusted for <span class="in">`x='Time'`</span>.</span>
<span id="cb61-798"><a href="#cb61-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-801"><a href="#cb61-801" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-802"><a href="#cb61-802" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-803"><a href="#cb61-803" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-804"><a href="#cb61-804" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb61-805"><a href="#cb61-805" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-806"><a href="#cb61-806" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-807"><a href="#cb61-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-808"><a href="#cb61-808" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-809"><a href="#cb61-809" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb61-810"><a href="#cb61-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-811"><a href="#cb61-811" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Chick (randomly sampled from 1 to 50)</span></span>
<span id="cb61-812"><a href="#cb61-812" aria-hidden="true" tabindex="-1"></a>chick_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">51</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb61-813"><a href="#cb61-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-814"><a href="#cb61-814" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Diet (randomly sampled from 1 to 4)</span></span>
<span id="cb61-815"><a href="#cb61-815" aria-hidden="true" tabindex="-1"></a>diet_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">5</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb61-816"><a href="#cb61-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-817"><a href="#cb61-817" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for weight with approximate mean and std</span></span>
<span id="cb61-818"><a href="#cb61-818" aria-hidden="true" tabindex="-1"></a>weight_values <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">121.818</span>, scale<span class="op">=</span><span class="fl">71.072</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb61-819"><a href="#cb61-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-820"><a href="#cb61-820" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure weights are within the specified range</span></span>
<span id="cb61-821"><a href="#cb61-821" aria-hidden="true" tabindex="-1"></a>weight_values <span class="op">=</span> np.clip(weight_values, <span class="dv">35</span>, <span class="dv">373</span>)</span>
<span id="cb61-822"><a href="#cb61-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-823"><a href="#cb61-823" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb61-824"><a href="#cb61-824" aria-hidden="true" tabindex="-1"></a>exp_chick_data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-825"><a href="#cb61-825" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Chick"</span>: chick_values,</span>
<span id="cb61-826"><a href="#cb61-826" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Diet"</span>: diet_values,</span>
<span id="cb61-827"><a href="#cb61-827" aria-hidden="true" tabindex="-1"></a>    <span class="st">"weight"</span>: weight_values</span>
<span id="cb61-828"><a href="#cb61-828" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-829"><a href="#cb61-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-830"><a href="#cb61-830" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Chick (randomly sampled from 1 to 50)</span></span>
<span id="cb61-831"><a href="#cb61-831" aria-hidden="true" tabindex="-1"></a>chick_values <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">51</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb61-832"><a href="#cb61-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-833"><a href="#cb61-833" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 578 values for Time (normally distributed around mean 10.718 with std 6.758)</span></span>
<span id="cb61-834"><a href="#cb61-834" aria-hidden="true" tabindex="-1"></a>time_values <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">10.718</span>, scale<span class="op">=</span><span class="fl">6.758</span>, size<span class="op">=</span><span class="dv">578</span>)</span>
<span id="cb61-835"><a href="#cb61-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-836"><a href="#cb61-836" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure Time values are within the specified range (0 to 21)</span></span>
<span id="cb61-837"><a href="#cb61-837" aria-hidden="true" tabindex="-1"></a>time_values <span class="op">=</span> np.clip(time_values, <span class="dv">0</span>, <span class="dv">21</span>).astype(<span class="bu">int</span>)</span>
<span id="cb61-838"><a href="#cb61-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-839"><a href="#cb61-839" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb61-840"><a href="#cb61-840" aria-hidden="true" tabindex="-1"></a>cov_chick_data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-841"><a href="#cb61-841" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Chick"</span>: chick_values,</span>
<span id="cb61-842"><a href="#cb61-842" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Time"</span>: time_values</span>
<span id="cb61-843"><a href="#cb61-843" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-844"><a href="#cb61-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-845"><a href="#cb61-845" aria-hidden="true" tabindex="-1"></a><span class="co"># Join experimental and covariate data</span></span>
<span id="cb61-846"><a href="#cb61-846" aria-hidden="true" tabindex="-1"></a>merged_chick_data <span class="op">=</span> pd.merge(exp_chick_data, </span>
<span id="cb61-847"><a href="#cb61-847" aria-hidden="true" tabindex="-1"></a>                            cov_chick_data, on<span class="op">=</span><span class="st">'Chick'</span>)</span>
<span id="cb61-848"><a href="#cb61-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-849"><a href="#cb61-849" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the merged data</span></span>
<span id="cb61-850"><a href="#cb61-850" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(merged_chick_data)</span>
<span id="cb61-851"><a href="#cb61-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-852"><a href="#cb61-852" aria-hidden="true" tabindex="-1"></a><span class="co"># Join experimental and covariate data</span></span>
<span id="cb61-853"><a href="#cb61-853" aria-hidden="true" tabindex="-1"></a>merged_chick_data <span class="op">=</span> pd.merge(exp_chick_data, </span>
<span id="cb61-854"><a href="#cb61-854" aria-hidden="true" tabindex="-1"></a>                             cov_chick_data, on<span class="op">=</span><span class="st">'Chick'</span>)</span>
<span id="cb61-855"><a href="#cb61-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-856"><a href="#cb61-856" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform ANCOVA with Diet and Time as predictors</span></span>
<span id="cb61-857"><a href="#cb61-857" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ols(<span class="st">'weight ~ Diet + Time'</span>, data<span class="op">=</span>merged_chick_data).fit()</span>
<span id="cb61-858"><a href="#cb61-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-859"><a href="#cb61-859" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of the model</span></span>
<span id="cb61-860"><a href="#cb61-860" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary())</span>
<span id="cb61-861"><a href="#cb61-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-862"><a href="#cb61-862" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Diet effects with Time adjustment</span></span>
<span id="cb61-863"><a href="#cb61-863" aria-hidden="true" tabindex="-1"></a>sns.lmplot(x<span class="op">=</span><span class="st">'Time'</span>, y<span class="op">=</span><span class="st">'weight'</span>, </span>
<span id="cb61-864"><a href="#cb61-864" aria-hidden="true" tabindex="-1"></a>         hue<span class="op">=</span><span class="st">'Diet'</span>, </span>
<span id="cb61-865"><a href="#cb61-865" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>merged_chick_data)</span>
<span id="cb61-866"><a href="#cb61-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-867"><a href="#cb61-867" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-868"><a href="#cb61-868" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-869"><a href="#cb61-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-870"><a href="#cb61-870" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 3: Analyzing Experimental Data: Statistical Tests and Power {#sec-Chapter3}</span></span>
<span id="cb61-871"><a href="#cb61-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-872"><a href="#cb61-872" aria-hidden="true" tabindex="-1"></a>Master statistical tests like t-tests, ANOVA, and Chi-Square, and dive deep into post-hoc analyses and power analysis essentials. Learn to select the right test, interpret p-values and errors, and skillfully conduct power analysis to determine sample and effect sizes, all while leveraging Python's powerful libraries to bring your data insights to life.</span>
<span id="cb61-873"><a href="#cb61-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-874"><a href="#cb61-874" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.1. Choosing the right statistical test {#sec-Chapter3.1}</span></span>
<span id="cb61-875"><a href="#cb61-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-876"><a href="#cb61-876" aria-hidden="true" tabindex="-1"></a>We'll now look into choosing the right statistical test for analyzing experimental data.</span>
<span id="cb61-877"><a href="#cb61-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-878"><a href="#cb61-878" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Selecting the right test {.unnumbered}</span></span>
<span id="cb61-879"><a href="#cb61-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-880"><a href="#cb61-880" aria-hidden="true" tabindex="-1"></a>Just as choosing the right book or the right measurement tool for is vital to research, choosing the right statistical test is foundational to any data analysis. Understanding our dataset's features and the hypotheses under examination is vital. It necessitates assessing the data types—categorical or continuous—their distributions, often assumed to be normal by many statistical tests, and the number of variables in the study. It's essential to align the chosen statistical method with the dataset's properties and the study's goals to ensure accurate and dependable outcomes. In this chapter, we'll explore how to apply t-tests, ANOVA, and Chi-Square tests, focusing on analyzing experimental data.</span>
<span id="cb61-881"><a href="#cb61-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-882"><a href="#cb61-882" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The dataset: athletic performance {.unnumbered}</span></span>
<span id="cb61-883"><a href="#cb61-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-884"><a href="#cb61-884" aria-hidden="true" tabindex="-1"></a>We'll work with a DataFrame called <span class="in">`athletic_perf`</span> containing athletes' performance data, focusing on the effects of different training programs and diets on athletic performance. Key variables are the type of training program, assigned diet, initial fitness level, and the observed performance increase as a percentage.</span>
<span id="cb61-885"><a href="#cb61-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-886"><a href="#cb61-886" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Independent samples t-test {.unnumbered}</span></span>
<span id="cb61-887"><a href="#cb61-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-888"><a href="#cb61-888" aria-hidden="true" tabindex="-1"></a>An independent samples t-test is used to compare the means of two distinct groups to determine if there is a statistically significant difference between them. This test relies on the assumptions that the response data for both groups are normally distributed and have equal variances, ensuring the validity and reliability of the test results. We'll use an <span class="in">`alpha`</span> of <span class="in">`0.5`</span> and compare the mean athletic performance improvements between two groups undergoing High-Intensity Interval Training (HIIT) and Endurance training by assigning their performance increases to group1 and group2. Next we call <span class="in">`ttest_ind`</span> on <span class="in">`group1`</span> and <span class="in">`group2`</span> and retrieve the test statistics and p-value. A large p-value here leads us to conclude that there is no significant difference in the mean performance increase between the HIIT and Endurance groups.</span>
<span id="cb61-889"><a href="#cb61-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-890"><a href="#cb61-890" aria-hidden="true" tabindex="-1"></a><span class="fu">#### One-way ANOVA {.unnumbered}</span></span>
<span id="cb61-891"><a href="#cb61-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-892"><a href="#cb61-892" aria-hidden="true" tabindex="-1"></a>A one-way ANOVA test is employed to determine if there are statistically significant differences among the means of more than two groups. The one-way corresponds to ANOVA with a single independent variable, and it assumes that the variances among the groups are equal. For our example, we gather the athletic performance increase data for each training program type into a list of groups using a list comprehension. The <span class="in">`f_oneway`</span> function from <span class="in">`scipy.stats`</span> is then used to conduct the ANOVA test across these groups by unpacking the groups list using an asterisk. The relatively high P-value implies that, based on the provided data, we cannot confidently assert that different training programs lead to different mean increases in athletic performance.</span>
<span id="cb61-893"><a href="#cb61-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-894"><a href="#cb61-894" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Chi-square test of association {.unnumbered}</span></span>
<span id="cb61-895"><a href="#cb61-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-896"><a href="#cb61-896" aria-hidden="true" tabindex="-1"></a>The Chi-square test of association is a statistical method used to assess whether there is a significant association between two categorical variables. Unlike many other statistical tests, the chi-square test does not require assumptions about the distribution of the data. To prepare for the test, we start by creating a contingency table using <span class="in">`crosstab`</span> from <span class="in">`pandas`</span>, which cross-tabulates athletes by their <span class="in">`Training_Program`</span> and <span class="in">`Diet_Type`</span>.</span>
<span id="cb61-897"><a href="#cb61-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-898"><a href="#cb61-898" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Chi-square test of association {.unnumbered}</span></span>
<span id="cb61-899"><a href="#cb61-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-900"><a href="#cb61-900" aria-hidden="true" tabindex="-1"></a>The <span class="in">`chi2_contingency`</span> function from <span class="in">`scipy.stats`</span> is then employed to conduct the chi-square test on the contingency table. The large P-value suggests that any observed association between training programs and diet types is not statistically significant.</span>
<span id="cb61-901"><a href="#cb61-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-902"><a href="#cb61-902" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.1</span></span>
<span id="cb61-903"><a href="#cb61-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-904"><a href="#cb61-904" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Choosing the right test: petrochemicals {.unnumbered}</span></span>
<span id="cb61-905"><a href="#cb61-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-906"><a href="#cb61-906" aria-hidden="true" tabindex="-1"></a>In a chemistry research lab, scientists are examining the efficiency of three well-known catalysts—Palladium (Pd), Platinum (Pt), and Nickel (Ni)—in facilitating a particular reaction. Each catalyst is used in a set of identical reactions under controlled conditions, and the time taken for each reaction to reach completion is meticulously recorded. Your goal is to compare the mean reaction times across the three catalyst groups to identify which catalyst, if any, has a significantly different reaction time.</span>
<span id="cb61-907"><a href="#cb61-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-908"><a href="#cb61-908" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-909"><a href="#cb61-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-910"><a href="#cb61-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use a list comprehension to filter into <span class="in">`groups`</span> iterating over the <span class="in">`catalyst_types`</span> and each of their <span class="in">`'Reaction_Time'`</span>s.</span>
<span id="cb61-911"><a href="#cb61-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-914"><a href="#cb61-914" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-915"><a href="#cb61-915" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-916"><a href="#cb61-916" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-917"><a href="#cb61-917" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb61-918"><a href="#cb61-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-919"><a href="#cb61-919" aria-hidden="true" tabindex="-1"></a>chemical_reactions <span class="op">=</span> pd.read_csv(<span class="st">'datasets/chemical_reactions.csv'</span>)</span>
<span id="cb61-920"><a href="#cb61-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-921"><a href="#cb61-921" aria-hidden="true" tabindex="-1"></a>catalyst_types <span class="op">=</span> [<span class="st">'Palladium'</span>, <span class="st">'Platinum'</span>, <span class="st">'Nickel'</span>]</span>
<span id="cb61-922"><a href="#cb61-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-923"><a href="#cb61-923" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect reaction times for each catalyst into a list</span></span>
<span id="cb61-924"><a href="#cb61-924" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [chemical_reactions[chemical_reactions[<span class="st">'Catalyst'</span>] <span class="op">==</span> catalyst][<span class="st">'Reaction_Time'</span>] <span class="cf">for</span> catalyst <span class="kw">in</span> catalyst_types]</span>
<span id="cb61-925"><a href="#cb61-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-926"><a href="#cb61-926" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the one-way ANOVA across the three groups</span></span>
<span id="cb61-927"><a href="#cb61-927" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb61-928"><a href="#cb61-928" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F-Statistic: </span><span class="sc">{</span>f_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-929"><a href="#cb61-929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-930"><a href="#cb61-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-931"><a href="#cb61-931" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question {.unnumbered}</span></span>
<span id="cb61-932"><a href="#cb61-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-933"><a href="#cb61-933" aria-hidden="true" tabindex="-1"></a>Assume a significance level of $\alpha = 0.01$. What is the appropriate conclusion to glean from the P-value in comparison with this $\alpha$ value?</span>
<span id="cb61-934"><a href="#cb61-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-935"><a href="#cb61-935" aria-hidden="true" tabindex="-1"></a>_The P-value is substantially smaller than the $\alpha$ value, indicating a significant difference in reaction times across the catalysts._</span>
<span id="cb61-936"><a href="#cb61-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-937"><a href="#cb61-937" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.2</span></span>
<span id="cb61-938"><a href="#cb61-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-939"><a href="#cb61-939" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Choosing the right test: human resources {.unnumbered}</span></span>
<span id="cb61-940"><a href="#cb61-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-941"><a href="#cb61-941" aria-hidden="true" tabindex="-1"></a>In human resources, it's essential to understand the relationships between different variables that might influence employee satisfaction or turnover. Consider a scenario where an HR department is interested in understanding the association between the department in which employees work and their participation in a new workplace wellness program. The HR team has compiled this data over the past two years and has asked you if there's any significant association between an employee's department and their enrolling in the wellness program.</span>
<span id="cb61-942"><a href="#cb61-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-943"><a href="#cb61-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-944"><a href="#cb61-944" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-945"><a href="#cb61-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-946"><a href="#cb61-946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a contingency table comparing <span class="in">`'Department'`</span> and <span class="in">`'Wellness_Program_Status'`</span>.</span>
<span id="cb61-947"><a href="#cb61-947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Perform a chi-square test of association on the contingency table and print the p-value.</span>
<span id="cb61-948"><a href="#cb61-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-951"><a href="#cb61-951" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-952"><a href="#cb61-952" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-953"><a href="#cb61-953" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-954"><a href="#cb61-954" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb61-955"><a href="#cb61-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-956"><a href="#cb61-956" aria-hidden="true" tabindex="-1"></a>hr_wellness <span class="op">=</span> pd.read_csv(<span class="st">'datasets/hr_wellness.csv'</span>)</span>
<span id="cb61-957"><a href="#cb61-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-958"><a href="#cb61-958" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a contingency table</span></span>
<span id="cb61-959"><a href="#cb61-959" aria-hidden="true" tabindex="-1"></a>contingency_table <span class="op">=</span> pd.crosstab(</span>
<span id="cb61-960"><a href="#cb61-960" aria-hidden="true" tabindex="-1"></a>  hr_wellness[<span class="st">'Department'</span>], </span>
<span id="cb61-961"><a href="#cb61-961" aria-hidden="true" tabindex="-1"></a>  hr_wellness[<span class="st">'Wellness_Program_Status'</span>]</span>
<span id="cb61-962"><a href="#cb61-962" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-963"><a href="#cb61-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-964"><a href="#cb61-964" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the chi-square test of association</span></span>
<span id="cb61-965"><a href="#cb61-965" aria-hidden="true" tabindex="-1"></a>chi2_stat, p_val, dof, expected <span class="op">=</span> chi2_contingency(contingency_table)</span>
<span id="cb61-966"><a href="#cb61-966" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F-Statistic: </span><span class="sc">{</span>chi2_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-967"><a href="#cb61-967" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-968"><a href="#cb61-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-969"><a href="#cb61-969" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question {.unnumbered}</span></span>
<span id="cb61-970"><a href="#cb61-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-971"><a href="#cb61-971" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Assume a significance level of 0.05. Given the P-value, what is the appropriate conclusion?</span>
<span id="cb61-972"><a href="#cb61-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-973"><a href="#cb61-973" aria-hidden="true" tabindex="-1"></a>_There's no significant association between department and enrollment in the wellness program, as the P-value is larger than `0.05`._</span>
<span id="cb61-974"><a href="#cb61-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-975"><a href="#cb61-975" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.3</span></span>
<span id="cb61-976"><a href="#cb61-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-977"><a href="#cb61-977" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Choosing the right test: finance {.unnumbered}</span></span>
<span id="cb61-978"><a href="#cb61-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-979"><a href="#cb61-979" aria-hidden="true" tabindex="-1"></a>In the realm of finance, investment strategists are continually evaluating different approaches to maximize returns. Consider a scenario where a financial firm wishes to assess the effectiveness of two investment strategies: "Quantitative Analysis" and "Fundamental Analysis". The firm has applied each strategy to a separate set of investment portfolios for a year and now asks you to compare the annual returns to determine if there is any difference in strategy returns by comparing the mean returns of the two groups.</span>
<span id="cb61-980"><a href="#cb61-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-981"><a href="#cb61-981" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-982"><a href="#cb61-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-983"><a href="#cb61-983" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>What type of hypothesis test should be performed in this scenario?</span>
<span id="cb61-984"><a href="#cb61-984" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Possible answer: Independent samples t-test</span>
<span id="cb61-985"><a href="#cb61-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-986"><a href="#cb61-986" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Filter <span class="in">`'Strategy_Type'`</span> on <span class="in">`'Quantitative'`</span> to retrieve their <span class="in">`'Annual_Return'`</span> and do the same for <span class="in">`'Fundamental'`</span> strategies.</span>
<span id="cb61-987"><a href="#cb61-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-988"><a href="#cb61-988" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Complete for the two groups an independent samples t-test and print the p-value.</span>
<span id="cb61-989"><a href="#cb61-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-992"><a href="#cb61-992" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-993"><a href="#cb61-993" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-994"><a href="#cb61-994" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-995"><a href="#cb61-995" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb61-996"><a href="#cb61-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-997"><a href="#cb61-997" aria-hidden="true" tabindex="-1"></a>investment_returns <span class="op">=</span> pd.read_csv(<span class="st">'datasets/investment_returns.csv'</span>)</span>
<span id="cb61-998"><a href="#cb61-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-999"><a href="#cb61-999" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the annual returns by strategy type</span></span>
<span id="cb61-1000"><a href="#cb61-1000" aria-hidden="true" tabindex="-1"></a>quantitative_returns <span class="op">=</span> investment_returns[investment_returns[<span class="st">'Strategy_Type'</span>] <span class="op">==</span> <span class="st">'Quantitative'</span>][<span class="st">'Annual_Return'</span>]</span>
<span id="cb61-1001"><a href="#cb61-1001" aria-hidden="true" tabindex="-1"></a>fundamental_returns <span class="op">=</span> investment_returns[investment_returns[<span class="st">'Strategy_Type'</span>] <span class="op">==</span> <span class="st">'Fundamental'</span>][<span class="st">'Annual_Return'</span>]</span>
<span id="cb61-1002"><a href="#cb61-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1003"><a href="#cb61-1003" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the independent samples t-test between the two groups</span></span>
<span id="cb61-1004"><a href="#cb61-1004" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> ttest_ind(quantitative_returns, fundamental_returns)</span>
<span id="cb61-1005"><a href="#cb61-1005" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"T-statistic : </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ss">, P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-1006"><a href="#cb61-1006" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1007"><a href="#cb61-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1008"><a href="#cb61-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Assume a significance level of 0.1. What is the appropriate conclusion to glean from the P-value in comparison with this $\alpha$ value?</span>
<span id="cb61-1009"><a href="#cb61-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1010"><a href="#cb61-1010" aria-hidden="true" tabindex="-1"></a>_The P-value is much smaller than $\alpha$, suggesting a significant difference in returns between the two strategies._</span>
<span id="cb61-1011"><a href="#cb61-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1012"><a href="#cb61-1012" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.2: Post-hoc analysis following ANOVA {#sec-Chapter3.2}</span></span>
<span id="cb61-1013"><a href="#cb61-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1014"><a href="#cb61-1014" aria-hidden="true" tabindex="-1"></a>After conducting ANOVA, we often need to understand specific differences between groups. This is where post-hoc analysis comes in, providing detailed insights into pairwise comparisons.</span>
<span id="cb61-1015"><a href="#cb61-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1016"><a href="#cb61-1016" aria-hidden="true" tabindex="-1"></a><span class="fu">#### When to use post-hoc tests {.unnumbered}</span></span>
<span id="cb61-1017"><a href="#cb61-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1018"><a href="#cb61-1018" aria-hidden="true" tabindex="-1"></a>Post-hoc tests are pivotal when ANOVA reveals significant differences among groups. They allow us to pinpoint which specific pairs of groups differ, allowing us to peek behind the curtain to explore the inner workings of pairwise differences.</span>
<span id="cb61-1019"><a href="#cb61-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1020"><a href="#cb61-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key post-hoc methods {.unnumbered}</span></span>
<span id="cb61-1021"><a href="#cb61-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1022"><a href="#cb61-1022" aria-hidden="true" tabindex="-1"></a>There are two common post-hoc methods: Tukey's HSD, named after statistician John Tukey, which is known for its robustness in multiple comparisons. There's also the Bonferroni correction, named after mathematician Carlo Bonferroni, which adjusts p-values to control for Type I errors. For broader comparisons, use Tukey's HSD; Bonferroni is better for reducing false positives in more focused tests.</span>
<span id="cb61-1023"><a href="#cb61-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1024"><a href="#cb61-1024" aria-hidden="true" tabindex="-1"></a>1 <span class="co">[</span><span class="ot">https://www.amphilsoc.org/item-detail/photograph-john-wilder-tukey</span><span class="co">](https://www.amphilsoc.org/item-detail/photograph-john-wilder-tukey)</span></span>
<span id="cb61-1025"><a href="#cb61-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1026"><a href="#cb61-1026" aria-hidden="true" tabindex="-1"></a>2 <span class="co">[</span><span class="ot">https://en.wikipedia.org/wiki/Carlo_Emilio_Bonferroni</span><span class="co">](https://en.wikipedia.org/wiki/Carlo_Emilio_Bonferroni)</span></span>
<span id="cb61-1027"><a href="#cb61-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1028"><a href="#cb61-1028" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The dataset: marketing ad campaigns {.unnumbered}</span></span>
<span id="cb61-1029"><a href="#cb61-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1030"><a href="#cb61-1030" aria-hidden="true" tabindex="-1"></a>We'll work with a dataset of marketing campaigns, examining the <span class="in">`Click_Through_Rate`</span> for different <span class="in">`Ad`</span> campaigns to identify differences and which strategy is most effective.</span>
<span id="cb61-1031"><a href="#cb61-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1032"><a href="#cb61-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data organization with pivot tables {.unnumbered}</span></span>
<span id="cb61-1033"><a href="#cb61-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1034"><a href="#cb61-1034" aria-hidden="true" tabindex="-1"></a>Pivot tables in pandas can be extremely helpful for organizing data, especially before conducting post-hoc analysis. It provides a clear comparison of the mean <span class="in">`Click_Through_Rates`</span> for each campaign type.</span>
<span id="cb61-1035"><a href="#cb61-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1036"><a href="#cb61-1036" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Performing ANOVA {.unnumbered}</span></span>
<span id="cb61-1037"><a href="#cb61-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1038"><a href="#cb61-1038" aria-hidden="true" tabindex="-1"></a>We start with ANOVA to assess if there's a significant difference in these <span class="in">`Click_Through_Rates`</span> among the campaigns. This sets the stage for further analysis if significant differences are found. First, we specify the different campaign types. Then we create the groups using a list comprehension to extract the <span class="in">`Click_Through_Rate`</span> for each <span class="in">`Ad_Campaign`</span>. Next, we perform the ANOVA across the three campaign types, unpacking the groups using an asterisk, to compare their mean click-through rates. The very small P-value here indicates significant differences in these means.</span>
<span id="cb61-1039"><a href="#cb61-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1040"><a href="#cb61-1040" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Tukey's HSD test {.unnumbered}</span></span>
<span id="cb61-1041"><a href="#cb61-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1042"><a href="#cb61-1042" aria-hidden="true" tabindex="-1"></a>If ANOVA indicates significant differences, Tukey's HSD test helps us understand exactly which campaigns differ. The <span class="in">`pairwise_tukeyhsd`</span> function from <span class="in">`statsmodels.stats`</span> takes arguments for the continuous response variable, <span class="in">`Click_Through_Rat`</span>e in this case, the categorical variable with more than two groups, <span class="in">`Ad_Campaign`</span>, and $\alpha$. To interpret the results of this table, we focus on the meandiff, p-adj (adjusted P-value), and reject columns. For the first row, Loyalty Reward versus New Arrival, the mean difference is 0.2211, with a p-value less than 0.05, indicating that the <span class="in">`Loyalty Reward`</span> group has a significantly higher mean than the <span class="in">`New Arrival group`</span>. For <span class="in">`Loyalty Reward`</span> versus Seasonal Discount, on row 2, the mean difference is -0.2738. With a p-value less than 0.05, it suggests that the Loyalty Reward group has a significantly lower mean than the Seasonal Discount group. Lastly, for New Arrival versus Seasonal Discount, the mean difference is -0.4949, with a p-value less than 0.05, indicating that the New Arrival group has a significantly lower mean than the Seasonal Discount group.</span>
<span id="cb61-1043"><a href="#cb61-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1044"><a href="#cb61-1044" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bonferroni correction set-up {.unnumbered}</span></span>
<span id="cb61-1045"><a href="#cb61-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1046"><a href="#cb61-1046" aria-hidden="true" tabindex="-1"></a>The Bonferroni correction is a stringent method to adjust p-values when conducting multiple pairwise comparisons, effectively reducing the chances of a Type I error. A little more data preparation is required before applying the Bonferroni correction. We begin by creating an empty P-values list. Then, we lay out a list of tuples containing the pairwise comparisons that we will iterate over. Next, we iterate over the tuples in comparisons, using the tuple elements to extract the <span class="in">`Click_Through_Rate`</span> for both groups. We run <span class="in">`ttest_ind`</span> on the click through rates in a pairwise fashion, and append the p-values to our list.</span>
<span id="cb61-1047"><a href="#cb61-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1048"><a href="#cb61-1048" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Performing Bonferroni correction {.unnumbered}</span></span>
<span id="cb61-1049"><a href="#cb61-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1050"><a href="#cb61-1050" aria-hidden="true" tabindex="-1"></a>Now we apply the Bonferroni correction using the multipletests function. The resulting p-values for the three comparisons are all extremely small. This again provides evidence that each of the three groups have significant click through rate differences.</span>
<span id="cb61-1051"><a href="#cb61-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1052"><a href="#cb61-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1053"><a href="#cb61-1053" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.2.1</span></span>
<span id="cb61-1054"><a href="#cb61-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1055"><a href="#cb61-1055" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Anxiety treatments ANOVA {.unnumbered}</span></span>
<span id="cb61-1056"><a href="#cb61-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1057"><a href="#cb61-1057" aria-hidden="true" tabindex="-1"></a>Psychologists conducted a study to compare the effectiveness of three types of therapy on reducing anxiety levels: Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), and Acceptance and Commitment Therapy (ACT). Participants were randomly assigned to one of the three therapy groups, and their anxiety levels were measured before and after the therapy sessions. The psychologists have asked you to determine if there are any significant differences in the effectiveness of these therapies.</span>
<span id="cb61-1058"><a href="#cb61-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1059"><a href="#cb61-1059" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1060"><a href="#cb61-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1061"><a href="#cb61-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a pivot table to calculate the mean <span class="in">`'Anxiety_Reduction'`</span> value across groups of <span class="in">`'Therapy_Type'`</span> in this data.</span>
<span id="cb61-1062"><a href="#cb61-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Filter groups of therapy types and their <span class="in">`'Anxiety_Reduction'`</span> values by first creating a list of the three therapy types: <span class="in">`'CBT'`</span>, <span class="in">`'DBT'`</span>, and <span class="in">`'ACT'`</span>.</span>
<span id="cb61-1063"><a href="#cb61-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1066"><a href="#cb61-1066" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1067"><a href="#cb61-1067" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1068"><a href="#cb61-1068" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1069"><a href="#cb61-1069" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb61-1070"><a href="#cb61-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1071"><a href="#cb61-1071" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1072"><a href="#cb61-1072" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb61-1073"><a href="#cb61-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1074"><a href="#cb61-1074" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb61-1075"><a href="#cb61-1075" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb61-1076"><a href="#cb61-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1077"><a href="#cb61-1077" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb61-1078"><a href="#cb61-1078" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb61-1079"><a href="#cb61-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1080"><a href="#cb61-1080" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb61-1081"><a href="#cb61-1081" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb61-1082"><a href="#cb61-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1083"><a href="#cb61-1083" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb61-1084"><a href="#cb61-1084" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb61-1085"><a href="#cb61-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1086"><a href="#cb61-1086" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb61-1087"><a href="#cb61-1087" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1088"><a href="#cb61-1088" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb61-1089"><a href="#cb61-1089" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb61-1090"><a href="#cb61-1090" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1091"><a href="#cb61-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1092"><a href="#cb61-1092" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary statistics</span></span>
<span id="cb61-1093"><a href="#cb61-1093" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(therapy_outcomes.describe())</span>
<span id="cb61-1094"><a href="#cb61-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1095"><a href="#cb61-1095" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pivot table</span></span>
<span id="cb61-1096"><a href="#cb61-1096" aria-hidden="true" tabindex="-1"></a>pivot_table <span class="op">=</span> therapy_outcomes.pivot_table(</span>
<span id="cb61-1097"><a href="#cb61-1097" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span><span class="st">'Anxiety_Reduction'</span>, </span>
<span id="cb61-1098"><a href="#cb61-1098" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="st">'Therapy_Type'</span>, </span>
<span id="cb61-1099"><a href="#cb61-1099" aria-hidden="true" tabindex="-1"></a>    aggfunc<span class="op">=</span><span class="st">'mean'</span></span>
<span id="cb61-1100"><a href="#cb61-1100" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-1101"><a href="#cb61-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1102"><a href="#cb61-1102" aria-hidden="true" tabindex="-1"></a><span class="co"># Pivot to view the mean anxiety reduction for each therapy</span></span>
<span id="cb61-1103"><a href="#cb61-1103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pivot_table)</span>
<span id="cb61-1104"><a href="#cb61-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1105"><a href="#cb61-1105" aria-hidden="true" tabindex="-1"></a><span class="co"># Create groups to prepare the data for ANOVA</span></span>
<span id="cb61-1106"><a href="#cb61-1106" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb61-1107"><a href="#cb61-1107" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> therapy][<span class="st">'Anxiety_Reduction'</span>] <span class="cf">for</span> therapy <span class="kw">in</span> therapy_types]</span>
<span id="cb61-1108"><a href="#cb61-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1109"><a href="#cb61-1109" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct ANOVA</span></span>
<span id="cb61-1110"><a href="#cb61-1110" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb61-1111"><a href="#cb61-1111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_val)</span>
<span id="cb61-1112"><a href="#cb61-1112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1113"><a href="#cb61-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1114"><a href="#cb61-1114" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution collapse="true"}</span>
<span id="cb61-1115"><a href="#cb61-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1116"><a href="#cb61-1116" aria-hidden="true" tabindex="-1"></a>_By analyzing the data with ANOVA, you've taken an important step in comparing the effectiveness of different therapies. Assuming an $\alpha$ of 0.05, the P-value indicates significant differences in therapy effectiveness._</span>
<span id="cb61-1117"><a href="#cb61-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1118"><a href="#cb61-1118" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-1119"><a href="#cb61-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1120"><a href="#cb61-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.2.2</span></span>
<span id="cb61-1121"><a href="#cb61-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1122"><a href="#cb61-1122" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Applying Tukey's HSD {.unnumbered}</span></span>
<span id="cb61-1123"><a href="#cb61-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1124"><a href="#cb61-1124" aria-hidden="true" tabindex="-1"></a>Following the ANOVA analysis which suggested significant differences in the effectiveness of the three types of therapy, the psychologists are keen to delve deeper. They wish for you to explain exactly which therapy types differ from each other in terms of reducing anxiety levels. This is where Tukey's Honest Significant Difference (HSD) test comes into play. It's a post-hoc test used to make pairwise comparisons between group means after an ANOVA has shown a significant difference. Tukey's HSD test helps in identifying specific pairs of groups that have significant differences in their means.</span>
<span id="cb61-1125"><a href="#cb61-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1126"><a href="#cb61-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1127"><a href="#cb61-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1128"><a href="#cb61-1128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At a significance level of <span class="in">`0.05`</span>, perform Tukey's HSD test to compare the mean anxiety reduction across the three therapy groups.</span>
<span id="cb61-1129"><a href="#cb61-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1132"><a href="#cb61-1132" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1133"><a href="#cb61-1133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1134"><a href="#cb61-1134" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1135"><a href="#cb61-1135" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway</span>
<span id="cb61-1136"><a href="#cb61-1136" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multicomp <span class="im">import</span> pairwise_tukeyhsd</span>
<span id="cb61-1137"><a href="#cb61-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1138"><a href="#cb61-1138" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1139"><a href="#cb61-1139" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb61-1140"><a href="#cb61-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1141"><a href="#cb61-1141" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb61-1142"><a href="#cb61-1142" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb61-1143"><a href="#cb61-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1144"><a href="#cb61-1144" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb61-1145"><a href="#cb61-1145" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb61-1146"><a href="#cb61-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1147"><a href="#cb61-1147" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb61-1148"><a href="#cb61-1148" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb61-1149"><a href="#cb61-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1150"><a href="#cb61-1150" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb61-1151"><a href="#cb61-1151" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb61-1152"><a href="#cb61-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1153"><a href="#cb61-1153" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb61-1154"><a href="#cb61-1154" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1155"><a href="#cb61-1155" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb61-1156"><a href="#cb61-1156" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb61-1157"><a href="#cb61-1157" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1158"><a href="#cb61-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1159"><a href="#cb61-1159" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pivot table</span></span>
<span id="cb61-1160"><a href="#cb61-1160" aria-hidden="true" tabindex="-1"></a>pivot_table <span class="op">=</span> therapy_outcomes.pivot_table(</span>
<span id="cb61-1161"><a href="#cb61-1161" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span><span class="st">'Anxiety_Reduction'</span>, </span>
<span id="cb61-1162"><a href="#cb61-1162" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="st">'Therapy_Type'</span>, </span>
<span id="cb61-1163"><a href="#cb61-1163" aria-hidden="true" tabindex="-1"></a>    aggfunc<span class="op">=</span><span class="st">'mean'</span></span>
<span id="cb61-1164"><a href="#cb61-1164" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-1165"><a href="#cb61-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1166"><a href="#cb61-1166" aria-hidden="true" tabindex="-1"></a><span class="co"># Pivot to view the mean anxiety reduction for each therapy</span></span>
<span id="cb61-1167"><a href="#cb61-1167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pivot_table)</span>
<span id="cb61-1168"><a href="#cb61-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1169"><a href="#cb61-1169" aria-hidden="true" tabindex="-1"></a><span class="co"># Create groups to prepare the data for ANOVA</span></span>
<span id="cb61-1170"><a href="#cb61-1170" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb61-1171"><a href="#cb61-1171" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> [therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> therapy][<span class="st">'Anxiety_Reduction'</span>] <span class="cf">for</span> therapy <span class="kw">in</span> therapy_types]</span>
<span id="cb61-1172"><a href="#cb61-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1173"><a href="#cb61-1173" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct ANOVA</span></span>
<span id="cb61-1174"><a href="#cb61-1174" aria-hidden="true" tabindex="-1"></a>f_stat, p_val <span class="op">=</span> f_oneway(<span class="op">*</span>groups)</span>
<span id="cb61-1175"><a href="#cb61-1175" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_val)</span>
<span id="cb61-1176"><a href="#cb61-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1177"><a href="#cb61-1177" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Tukey's HSD test</span></span>
<span id="cb61-1178"><a href="#cb61-1178" aria-hidden="true" tabindex="-1"></a>tukey_results <span class="op">=</span> pairwise_tukeyhsd(</span>
<span id="cb61-1179"><a href="#cb61-1179" aria-hidden="true" tabindex="-1"></a>    therapy_outcomes[<span class="st">'Anxiety_Reduction'</span>], </span>
<span id="cb61-1180"><a href="#cb61-1180" aria-hidden="true" tabindex="-1"></a>    therapy_outcomes[<span class="st">'Therapy_Type'</span>], </span>
<span id="cb61-1181"><a href="#cb61-1181" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb61-1182"><a href="#cb61-1182" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-1183"><a href="#cb61-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1184"><a href="#cb61-1184" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tukey_results)</span>
<span id="cb61-1185"><a href="#cb61-1185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1186"><a href="#cb61-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1187"><a href="#cb61-1187" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-1188"><a href="#cb61-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1189"><a href="#cb61-1189" aria-hidden="true" tabindex="-1"></a>_The Tukey HSD test provided clear insights into which therapy types significantly differ in reducing anxiety. These findings can guide psychologists in refining treatment approaches. Did you catch that (ACT and DBT) and (CBT and DBT)don't differ significantly from this experiment?_</span>
<span id="cb61-1190"><a href="#cb61-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1191"><a href="#cb61-1191" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-1192"><a href="#cb61-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1193"><a href="#cb61-1193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.2.3</span></span>
<span id="cb61-1194"><a href="#cb61-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1195"><a href="#cb61-1195" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Applying Bonferoni correction {.unnumbered}</span></span>
<span id="cb61-1196"><a href="#cb61-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1197"><a href="#cb61-1197" aria-hidden="true" tabindex="-1"></a>After identifying significant differences between therapy groups with Tukey's HSD, we want to confirm our findings with the Bonferroni correction. The Bonferroni correction is a conservative statistical adjustment used to counteract the problem of multiple comparisons. It reduces the chance of obtaining false-positive results by adjusting the significance level. In the context of your study on the effectiveness of CBT, DBT, and ACT, applying the Bonferroni correction will help ensure that the significant differences you observe between therapy groups are not due to chance.</span>
<span id="cb61-1198"><a href="#cb61-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1199"><a href="#cb61-1199" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1200"><a href="#cb61-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1201"><a href="#cb61-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conduct independent t-tests between all pairs of therapy groups in <span class="in">`therapy_pairs`</span> and append the p-values (<span class="in">`p_val`</span>) to the <span class="in">`p_values`</span> list.</span>
<span id="cb61-1202"><a href="#cb61-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply the Bonferroni correction to adjust the <span class="in">`p-values`</span> from the multiple tests and print them.</span>
<span id="cb61-1203"><a href="#cb61-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1206"><a href="#cb61-1206" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1207"><a href="#cb61-1207" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1208"><a href="#cb61-1208" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1209"><a href="#cb61-1209" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb61-1210"><a href="#cb61-1210" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.sandbox.stats.multicomp <span class="im">import</span> multipletests</span>
<span id="cb61-1211"><a href="#cb61-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1212"><a href="#cb61-1212" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1213"><a href="#cb61-1213" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb61-1214"><a href="#cb61-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1215"><a href="#cb61-1215" aria-hidden="true" tabindex="-1"></a><span class="co"># Define therapy types</span></span>
<span id="cb61-1216"><a href="#cb61-1216" aria-hidden="true" tabindex="-1"></a>therapy_types <span class="op">=</span> [<span class="st">'CBT'</span>, <span class="st">'DBT'</span>, <span class="st">'ACT'</span>]</span>
<span id="cb61-1217"><a href="#cb61-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1218"><a href="#cb61-1218" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random therapy assignments</span></span>
<span id="cb61-1219"><a href="#cb61-1219" aria-hidden="true" tabindex="-1"></a>therapy_assignments <span class="op">=</span> np.random.choice(therapy_types, size<span class="op">=</span><span class="dv">1422</span>, p<span class="op">=</span>[<span class="fl">0.34</span>, <span class="fl">0.33</span>, <span class="fl">0.33</span>])</span>
<span id="cb61-1220"><a href="#cb61-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1221"><a href="#cb61-1221" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Anxiety Reduction values following a normal distribution with given mean and std</span></span>
<span id="cb61-1222"><a href="#cb61-1222" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">15.201</span>, scale<span class="op">=</span><span class="fl">4.938</span>, size<span class="op">=</span><span class="dv">1422</span>)</span>
<span id="cb61-1223"><a href="#cb61-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1224"><a href="#cb61-1224" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure min and max are within expected range</span></span>
<span id="cb61-1225"><a href="#cb61-1225" aria-hidden="true" tabindex="-1"></a>danxiety_reduction <span class="op">=</span> np.clip(danxiety_reduction, <span class="op">-</span><span class="fl">1.206</span>, <span class="fl">34.264</span>)</span>
<span id="cb61-1226"><a href="#cb61-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1227"><a href="#cb61-1227" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb61-1228"><a href="#cb61-1228" aria-hidden="true" tabindex="-1"></a>therapy_outcomes <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1229"><a href="#cb61-1229" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Therapy_Type'</span>: therapy_assignments,</span>
<span id="cb61-1230"><a href="#cb61-1230" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anxiety_Reduction'</span>: danxiety_reduction</span>
<span id="cb61-1231"><a href="#cb61-1231" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1232"><a href="#cb61-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1233"><a href="#cb61-1233" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> []</span>
<span id="cb61-1234"><a href="#cb61-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1235"><a href="#cb61-1235" aria-hidden="true" tabindex="-1"></a>therapy_pairs <span class="op">=</span> [(<span class="st">'CBT'</span>, <span class="st">'DBT'</span>), (<span class="st">'CBT'</span>, <span class="st">'ACT'</span>), (<span class="st">'DBT'</span>, <span class="st">'ACT'</span>)]</span>
<span id="cb61-1236"><a href="#cb61-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1237"><a href="#cb61-1237" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct t-tests and collect P-values</span></span>
<span id="cb61-1238"><a href="#cb61-1238" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pair <span class="kw">in</span> therapy_pairs:</span>
<span id="cb61-1239"><a href="#cb61-1239" aria-hidden="true" tabindex="-1"></a>    group1 <span class="op">=</span> therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> pair[<span class="dv">0</span>]][<span class="st">'Anxiety_Reduction'</span>]</span>
<span id="cb61-1240"><a href="#cb61-1240" aria-hidden="true" tabindex="-1"></a>    group2 <span class="op">=</span> therapy_outcomes[therapy_outcomes[<span class="st">'Therapy_Type'</span>] <span class="op">==</span> pair[<span class="dv">1</span>]][<span class="st">'Anxiety_Reduction'</span>]</span>
<span id="cb61-1241"><a href="#cb61-1241" aria-hidden="true" tabindex="-1"></a>    t_stat, p_val <span class="op">=</span> ttest_ind(group1, group2)</span>
<span id="cb61-1242"><a href="#cb61-1242" aria-hidden="true" tabindex="-1"></a>    p_values.append(p_val)</span>
<span id="cb61-1243"><a href="#cb61-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1244"><a href="#cb61-1244" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Bonferroni correction</span></span>
<span id="cb61-1245"><a href="#cb61-1245" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(multipletests(p_values, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'bonferroni'</span>)[<span class="dv">1</span>])</span>
<span id="cb61-1246"><a href="#cb61-1246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1247"><a href="#cb61-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1248"><a href="#cb61-1248" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-1249"><a href="#cb61-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1250"><a href="#cb61-1250" aria-hidden="true" tabindex="-1"></a>_The Bonferroni correction applied to adjust for the P-values for multiple comparisons. This step is critical to control for Type I error, ensuring the reliability of my findings. Here again it is obvious that (ACT and DBT) and (CBT and DBT) don't differ significantly from this experiment due to the corrected P-value of 1._</span>
<span id="cb61-1251"><a href="#cb61-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1252"><a href="#cb61-1252" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-1253"><a href="#cb61-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1254"><a href="#cb61-1254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.3: P-values, alpha, and errors {#sec-Chapter3.3}</span></span>
<span id="cb61-1255"><a href="#cb61-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1256"><a href="#cb61-1256" aria-hidden="true" tabindex="-1"></a>In this lesson, we'll deepen our understanding of p-values, alpha levels, and experimental errors. This will prepare us for the next video, where we'll tackle a key concept in experimental design called power analysis!</span>
<span id="cb61-1257"><a href="#cb61-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1258"><a href="#cb61-1258" aria-hidden="true" tabindex="-1"></a><span class="fu">#### P-values and alpha {.unnumbered}</span></span>
<span id="cb61-1259"><a href="#cb61-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1260"><a href="#cb61-1260" aria-hidden="true" tabindex="-1"></a>P-values and alpha can be viewed as a game. Think of conducting a scientific experiment where we are trying to determine whether a certain strategy (our hypothesis) leads to winning (or a significant result) more often than just by chance. P-values help us understand the likelihood of observing our data if the null hypothesis was true. That is they serve as the scoreboard of the game. Setting an $\alpha$ level, often 0.05, allows us to determine the threshold at which we consider our results statistically significant, akin to setting the rules of a game before playing. Alpha is like establishing a rule for what counts as a "remarkable" win in this game. If your P-value is below this alpha level, it's as if we've achieved a high score or a remarkable performance in the game, leading us to conclude that our strategy (the alternative hypothesis) might indeed be effective, and it's not just the luck of the draw.</span>
<span id="cb61-1261"><a href="#cb61-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1262"><a href="#cb61-1262" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The dataset: crop yields {.unnumbered}</span></span>
<span id="cb61-1263"><a href="#cb61-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1264"><a href="#cb61-1264" aria-hidden="true" tabindex="-1"></a>We'll work with a dataset of crop yields from different fields, where each field was treated with either organic or synthetic fertilizer. Our goal is to analyze this data to determine if there's a significant difference in crop yields between the two fertilizer types.</span>
<span id="cb61-1265"><a href="#cb61-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1266"><a href="#cb61-1266" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing the data {.unnumbered}</span></span>
<span id="cb61-1267"><a href="#cb61-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1268"><a href="#cb61-1268" aria-hidden="true" tabindex="-1"></a>It's helpful to visualize the crop yields for each fertilizer type. By plotting the kernel density estimates (kde), we get a sense of how the two fertilizers might differ in terms of their effect on crop yields and whether there's an overlap between their effects. It appears that Organic tends to produce a higher yield than Synthetic with some overlap.</span>
<span id="cb61-1269"><a href="#cb61-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1270"><a href="#cb61-1270" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Conducting an independent samples t-test {.unnumbered}</span></span>
<span id="cb61-1271"><a href="#cb61-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1272"><a href="#cb61-1272" aria-hidden="true" tabindex="-1"></a>We set our alpha to the standard five-percent level. To compare the effectiveness of organic versus synthetic fertilizers, we perform a t-test on the crop yields from the two groups. The p-value is smaller than alpha suggesting that fertilizer type has a statistically significant impact on crop yield.</span>
<span id="cb61-1273"><a href="#cb61-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1274"><a href="#cb61-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exploring experimental errors {.unnumbered}</span></span>
<span id="cb61-1275"><a href="#cb61-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1276"><a href="#cb61-1276" aria-hidden="true" tabindex="-1"></a>In experimental design, we encounter two main types of errors. Type I errors occur when we incorrectly reject a true null hypothesis, akin to a false alarm. Type II errors happen when we fail to reject a false null hypothesis, similar to a missed detection.</span>
<span id="cb61-1277"><a href="#cb61-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1278"><a href="#cb61-1278" aria-hidden="true" tabindex="-1"></a><span class="fu">#### More on alpha {.unnumbered}</span></span>
<span id="cb61-1279"><a href="#cb61-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1280"><a href="#cb61-1280" aria-hidden="true" tabindex="-1"></a>Alpha, or the significance level, is crucial in hypothesis testing; it indicates the probability of a Type I error—rejecting a true null hypothesis. Common $\alpha$ levels include 0.05, 0.01, and 0.10, representing risks of 5%, 1%, and 10%, respectively, for such errors. Selecting an alpha hinges on the study's context and a balance between tolerating a Type I error and the risk of overlooking a true effect, known as a Type II error. The choice should align with the study's goals and the implications of potential errors. Conventionally, 0.05 is the standard for statistical significance across many disciplines. For more rigorous scrutiny, particularly where the cost of a Type I error is high, an $\alpha$ of 0.01 is preferred. In preliminary studies, where a higher error tolerance is permissible, an $\alpha$ of 0.10 may be utilized, allowing for a broader exploration of potential effects with subsequent validation through more stringent testing.</span>
<span id="cb61-1281"><a href="#cb61-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1282"><a href="#cb61-1282" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.3.1</span></span>
<span id="cb61-1283"><a href="#cb61-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1284"><a href="#cb61-1284" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Analyzing toy durability {.unnumbered}</span></span>
<span id="cb61-1285"><a href="#cb61-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1286"><a href="#cb61-1286" aria-hidden="true" tabindex="-1"></a>In product development within the toy industry, it's crucial to understand the durability of toys, particularly when comparing educational toys to recreational ones. Durability can significantly impact customer satisfaction and repeat business. Researchers in a toy manufacturing company have asked you to conduct the analysis of a study comparing the durability of educational toys versus recreational toys. The <span class="in">`toy_durability`</span> DataFrame contains the results of these tests, with durability scores assigned based on rigorous testing protocols.</span>
<span id="cb61-1287"><a href="#cb61-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1288"><a href="#cb61-1288" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1289"><a href="#cb61-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1290"><a href="#cb61-1290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the mean <span class="in">`'Durability_Score'`</span> for both <span class="in">`'Educational'`</span> and <span class="in">`'Recreational'`</span> toys using a pivot table.</span>
<span id="cb61-1291"><a href="#cb61-1291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Perform an independent samples t-test to compare the durability of <span class="in">`'Educational'`</span> and 'Recreational' toys by first separating durability scores by <span class="in">`Toy_Type`</span>.</span>
<span id="cb61-1292"><a href="#cb61-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1295"><a href="#cb61-1295" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1296"><a href="#cb61-1296" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1297"><a href="#cb61-1297" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1298"><a href="#cb61-1298" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb61-1299"><a href="#cb61-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1300"><a href="#cb61-1300" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1301"><a href="#cb61-1301" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">36</span>)</span>
<span id="cb61-1302"><a href="#cb61-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1303"><a href="#cb61-1303" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Toy_Type column with approximately equal distribution</span></span>
<span id="cb61-1304"><a href="#cb61-1304" aria-hidden="true" tabindex="-1"></a>toy_types <span class="op">=</span> np.random.choice([<span class="st">'Educational'</span>, <span class="st">'Recreational'</span>], size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb61-1305"><a href="#cb61-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1306"><a href="#cb61-1306" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Durability_Score based on the means for each Toy_Type</span></span>
<span id="cb61-1307"><a href="#cb61-1307" aria-hidden="true" tabindex="-1"></a>durability_scores <span class="op">=</span> np.where(</span>
<span id="cb61-1308"><a href="#cb61-1308" aria-hidden="true" tabindex="-1"></a>    toy_types <span class="op">==</span> <span class="st">'Educational'</span>,</span>
<span id="cb61-1309"><a href="#cb61-1309" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">80.101</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>),</span>
<span id="cb61-1310"><a href="#cb61-1310" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">79.461</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb61-1311"><a href="#cb61-1311" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-1312"><a href="#cb61-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1313"><a href="#cb61-1313" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb61-1314"><a href="#cb61-1314" aria-hidden="true" tabindex="-1"></a>toy_durability <span class="op">=</span> pd.DataFrame({<span class="st">'Toy_Type'</span>: toy_types, <span class="st">'Durability_Score'</span>: durability_scores})</span>
<span id="cb61-1315"><a href="#cb61-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1316"><a href="#cb61-1316" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pivot table</span></span>
<span id="cb61-1317"><a href="#cb61-1317" aria-hidden="true" tabindex="-1"></a>mean_durability <span class="op">=</span> toy_durability.pivot_table(values<span class="op">=</span><span class="st">'Durability_Score'</span>, index<span class="op">=</span><span class="st">'Toy_Type'</span>, aggfunc<span class="op">=</span>np.mean)</span>
<span id="cb61-1318"><a href="#cb61-1318" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_durability)</span>
<span id="cb61-1319"><a href="#cb61-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1320"><a href="#cb61-1320" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform t-test</span></span>
<span id="cb61-1321"><a href="#cb61-1321" aria-hidden="true" tabindex="-1"></a>educational_durability_scores <span class="op">=</span> toy_durability[toy_durability[<span class="st">'Toy_Type'</span>] <span class="op">==</span> <span class="st">'Educational'</span>][<span class="st">'Durability_Score'</span>]</span>
<span id="cb61-1322"><a href="#cb61-1322" aria-hidden="true" tabindex="-1"></a>recreational_durability_scores <span class="op">=</span> toy_durability[toy_durability[<span class="st">'Toy_Type'</span>] <span class="op">==</span> <span class="st">'Recreational'</span>][<span class="st">'Durability_Score'</span>]</span>
<span id="cb61-1323"><a href="#cb61-1323" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> ttest_ind(educational_durability_scores, recreational_durability_scores)</span>
<span id="cb61-1324"><a href="#cb61-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1325"><a href="#cb61-1325" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P-value:"</span>, p_val)</span>
<span id="cb61-1326"><a href="#cb61-1326" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1327"><a href="#cb61-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1328"><a href="#cb61-1328" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-1329"><a href="#cb61-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1330"><a href="#cb61-1330" aria-hidden="true" tabindex="-1"></a>_The P-value suggests that there's a statistically significant difference in durability between `'Educational'` and `'Recreational'` toys, assuming an alpha of `0.05`. This insight could be crucial for product development and marketing strategies._</span>
<span id="cb61-1331"><a href="#cb61-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1332"><a href="#cb61-1332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-1333"><a href="#cb61-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1334"><a href="#cb61-1334" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.3.2</span></span>
<span id="cb61-1335"><a href="#cb61-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1336"><a href="#cb61-1336" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing durability differences {.unnumbered}</span></span>
<span id="cb61-1337"><a href="#cb61-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1338"><a href="#cb61-1338" aria-hidden="true" tabindex="-1"></a>Following the analysis of toy durability, the research team is interested in you visualizing the distribution of durability scores for both Educational and Recreational toys. Such visualizations can offer intuitive insights into the data, potentially highlighting the range and variability of scores within each category. This step is essential for presenting findings to non-technical stakeholders and guiding further product development decisions.</span>
<span id="cb61-1339"><a href="#cb61-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1340"><a href="#cb61-1340" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1341"><a href="#cb61-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1342"><a href="#cb61-1342" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize the distribution of <span class="in">`'Durability_Score'`</span> for Educational and Recreational toys using a Kernel Density Estimate (KDE) plot, highlighting differences by using the <span class="in">`'Toy_Type'`</span> column to color the distributions differently.</span>
<span id="cb61-1343"><a href="#cb61-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1346"><a href="#cb61-1346" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1347"><a href="#cb61-1347" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1348"><a href="#cb61-1348" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1349"><a href="#cb61-1349" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1350"><a href="#cb61-1350" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1351"><a href="#cb61-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1352"><a href="#cb61-1352" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1353"><a href="#cb61-1353" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">36</span>)</span>
<span id="cb61-1354"><a href="#cb61-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1355"><a href="#cb61-1355" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Toy_Type column with approximately equal distribution</span></span>
<span id="cb61-1356"><a href="#cb61-1356" aria-hidden="true" tabindex="-1"></a>toy_types <span class="op">=</span> np.random.choice([<span class="st">'Educational'</span>, <span class="st">'Recreational'</span>], size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb61-1357"><a href="#cb61-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1358"><a href="#cb61-1358" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Durability_Score based on the means for each Toy_Type</span></span>
<span id="cb61-1359"><a href="#cb61-1359" aria-hidden="true" tabindex="-1"></a>durability_scores <span class="op">=</span> np.where(</span>
<span id="cb61-1360"><a href="#cb61-1360" aria-hidden="true" tabindex="-1"></a>    toy_types <span class="op">==</span> <span class="st">'Educational'</span>,</span>
<span id="cb61-1361"><a href="#cb61-1361" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">80.101</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>),</span>
<span id="cb61-1362"><a href="#cb61-1362" aria-hidden="true" tabindex="-1"></a>    np.random.normal(loc<span class="op">=</span><span class="fl">79.461</span>, scale<span class="op">=</span><span class="fl">6.0</span>, size<span class="op">=</span><span class="dv">1900</span>)</span>
<span id="cb61-1363"><a href="#cb61-1363" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-1364"><a href="#cb61-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1365"><a href="#cb61-1365" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame</span></span>
<span id="cb61-1366"><a href="#cb61-1366" aria-hidden="true" tabindex="-1"></a>toy_durability <span class="op">=</span> pd.DataFrame({<span class="st">'Toy_Type'</span>: toy_types, <span class="st">'Durability_Score'</span>: durability_scores})</span>
<span id="cb61-1367"><a href="#cb61-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1368"><a href="#cb61-1368" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of Durability_Score for each Toy_Type</span></span>
<span id="cb61-1369"><a href="#cb61-1369" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>toy_durability, x<span class="op">=</span><span class="st">"Durability_Score"</span>, </span>
<span id="cb61-1370"><a href="#cb61-1370" aria-hidden="true" tabindex="-1"></a>         hue<span class="op">=</span><span class="st">"Toy_Type"</span>, kind<span class="op">=</span><span class="st">"kde"</span>)</span>
<span id="cb61-1371"><a href="#cb61-1371" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Durability Score Distribution by Toy Type'</span>)</span>
<span id="cb61-1372"><a href="#cb61-1372" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Durability Score'</span>)</span>
<span id="cb61-1373"><a href="#cb61-1373" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb61-1374"><a href="#cb61-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1375"><a href="#cb61-1375" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1376"><a href="#cb61-1376" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1377"><a href="#cb61-1377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1378"><a href="#cb61-1378" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.4: Power analysis: sample and effect size {#sec-Chapter3.4}</span></span>
<span id="cb61-1379"><a href="#cb61-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1380"><a href="#cb61-1380" aria-hidden="true" tabindex="-1"></a>We now dive into the intricacies of power analysis, focusing on understanding effect size and how it influences sample size.</span>
<span id="cb61-1381"><a href="#cb61-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1382"><a href="#cb61-1382" aria-hidden="true" tabindex="-1"></a><span class="fu">#### A primer on effect size {.unnumbered}</span></span>
<span id="cb61-1383"><a href="#cb61-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1384"><a href="#cb61-1384" aria-hidden="true" tabindex="-1"></a>Effect size quantifies the magnitude of the difference between groups, beyond just noting if the difference is statistically significant. Cohen's d is a commonly used measure, calculated as the difference in means divided by a pooled standard deviation.</span>
<span id="cb61-1385"><a href="#cb61-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1386"><a href="#cb61-1386" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The dataset: video game engagement {.unnumbered}</span></span>
<span id="cb61-1387"><a href="#cb61-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1388"><a href="#cb61-1388" aria-hidden="true" tabindex="-1"></a>A video game company conducted an experiment with sixty participants to understand player engagement across two game genres: Action and Puzzle. They recorded the average number of hours players spent engaged to assess which type tends to captivate players more effectively.</span>
<span id="cb61-1389"><a href="#cb61-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1390"><a href="#cb61-1390" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating power overview {.unnumbered}</span></span>
<span id="cb61-1391"><a href="#cb61-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1392"><a href="#cb61-1392" aria-hidden="true" tabindex="-1"></a>Power analysis revolves around the probability that our test will correctly reject a false null hypothesis. This corresponds to identifying a true effect, avoiding a Type II error. A type II error is denoted as beta, so power is one minus beta and it ranges from zero to one, where one is certainty in our ability to detect a true effect. To calculate power, we first assume an effect size. Here we choose a value of 1, derived from historical data comparing the engagement scores of video game genres. We can also use our sample data to make an estimate of the effect size, but traditionally power analysis is done prior to the data collection. This can also help us determine how big of a sample size we should use in our study. We initialize the power object and call the <span class="in">`.solve_power()`</span> method, using a sample size for how many video game players were assessed in either group (30), our assumed effect size, and our alpha of 0.05. This high power tells us the likelihood that our test will detect a significant result, given our effect size and sample size.</span>
<span id="cb61-1393"><a href="#cb61-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1394"><a href="#cb61-1394" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Cohen's d formulation {.unnumbered}</span></span>
<span id="cb61-1395"><a href="#cb61-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1396"><a href="#cb61-1396" aria-hidden="true" tabindex="-1"></a>To calculate Cohen's d as an effect size, we define a function. Its two inputs are numeric data corresponding to the two groups from our sample data. We calculate the difference in the means of the two groups, their sample sizes, and their variances. Next, we determine a pooled standard deviation using this information. Lastly, Cohen's d is the difference in means divided by the pooled standard deviation.</span>
<span id="cb61-1397"><a href="#cb61-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1398"><a href="#cb61-1398" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Cohen's d for video game data {.unnumbered}</span></span>
<span id="cb61-1399"><a href="#cb61-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1400"><a href="#cb61-1400" aria-hidden="true" tabindex="-1"></a>To apply this to the video game data, we first split the data into two groups based on the genre. Then we apply our function to get the effect size. The result here is near the theoretical result of 1 we assumed earlier.</span>
<span id="cb61-1401"><a href="#cb61-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1402"><a href="#cb61-1402" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding sample size and power {.unnumbered}</span></span>
<span id="cb61-1403"><a href="#cb61-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1404"><a href="#cb61-1404" aria-hidden="true" tabindex="-1"></a>Balancing the need for sufficient power with practical constraints on sample size is a fundamental aspect of planning a study, such as comparing engagement times across different video game genres. A larger sample size can enhance an experiment's power, improving the likelihood of detecting a true effect.</span>
<span id="cb61-1405"><a href="#cb61-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1406"><a href="#cb61-1406" aria-hidden="true" tabindex="-1"></a>1 <span class="co">[</span><span class="ot">https://grabngoinfo.com/power-analysis-for-sample-size-using-python/</span><span class="co">](https://grabngoinfo.com/power-analysis-for-sample-size-using-python/)</span></span>
<span id="cb61-1407"><a href="#cb61-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1408"><a href="#cb61-1408" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sample size calculation in context {.unnumbered}</span></span>
<span id="cb61-1409"><a href="#cb61-1409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1410"><a href="#cb61-1410" aria-hidden="true" tabindex="-1"></a>Let's contextualize this within our video game study. Assuming our calculated value for Cohen's d engagement time between game genres, we calculate the sample size needed for each group to achieve 99% power with an alpha of 0.05 and equally-sized groups with a ratio of 1. This calculation is pivotal in ensuring our study is adequately powered to detect meaningful differences in player engagement across genres. Assuming we have an effect size of around 1.2, we would need at least 28 participants in each group to achieve a power of 99%. Recall we collected 30 participants, so we can feel confident about our experiment's power.</span>
<span id="cb61-1411"><a href="#cb61-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1412"><a href="#cb61-1412" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing sample size requirements {.unnumbered}</span></span>
<span id="cb61-1413"><a href="#cb61-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1414"><a href="#cb61-1414" aria-hidden="true" tabindex="-1"></a>We next build a visualization illustrating the relationship between effect size measured as Cohen's d and required sample size for our video game study, by plotting varying effect sizes against required sample sizes. As effect size increases, the required sample size for each group decreases, highlighting the importance of understanding the expected magnitude of differences when planning a study.</span>
<span id="cb61-1415"><a href="#cb61-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1416"><a href="#cb61-1416" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.4.1</span></span>
<span id="cb61-1417"><a href="#cb61-1417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1418"><a href="#cb61-1418" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimating required sample size for energy study {.unnumbered}</span></span>
<span id="cb61-1419"><a href="#cb61-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1420"><a href="#cb61-1420" aria-hidden="true" tabindex="-1"></a>In the energy sector, researchers are often tasked with evaluating the effectiveness of new technologies or initiatives to enhance energy efficiency or reduce consumption. A study is being designed to compare the impact of two energy-saving measures: "Smart Thermostats" and "LED Lighting". To ensure the study has sufficient power to detect a meaningful difference in energy savings between these two measures, you'll conduct a power analysis.</span>
<span id="cb61-1421"><a href="#cb61-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1422"><a href="#cb61-1422" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1423"><a href="#cb61-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1424"><a href="#cb61-1424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Instantiate a <span class="in">`TTestIndPower`</span> object.</span>
<span id="cb61-1425"><a href="#cb61-1425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conduct the power analysis to estimate the required sample size for each group (Smart Thermostats and LED Lighting) to achieve a power of 0.9, assuming a moderate effect size (Cohen's d = 0.5) and an alpha of 0.05 with an equal sized groups.</span>
<span id="cb61-1426"><a href="#cb61-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1427"><a href="#cb61-1427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1428"><a href="#cb61-1428" aria-hidden="true" tabindex="-1"></a><span class="in">import numpy as np</span></span>
<span id="cb61-1429"><a href="#cb61-1429" aria-hidden="true" tabindex="-1"></a><span class="in">import pandas as pd</span></span>
<span id="cb61-1430"><a href="#cb61-1430" aria-hidden="true" tabindex="-1"></a><span class="in">from statsmodels.stats.power import TTestIndPower</span></span>
<span id="cb61-1431"><a href="#cb61-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1432"><a href="#cb61-1432" aria-hidden="true" tabindex="-1"></a><span class="in"># Instantiate a TTestIndPower object</span></span>
<span id="cb61-1433"><a href="#cb61-1433" aria-hidden="true" tabindex="-1"></a><span class="in">power_analysis = TTestIndPower()</span></span>
<span id="cb61-1434"><a href="#cb61-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1435"><a href="#cb61-1435" aria-hidden="true" tabindex="-1"></a><span class="in"># Conduct a power analysis to determine the required sample size</span></span>
<span id="cb61-1436"><a href="#cb61-1436" aria-hidden="true" tabindex="-1"></a><span class="in">required_n = power_analysis.solve_power(</span></span>
<span id="cb61-1437"><a href="#cb61-1437" aria-hidden="true" tabindex="-1"></a><span class="in">    effect_size=0.5, </span></span>
<span id="cb61-1438"><a href="#cb61-1438" aria-hidden="true" tabindex="-1"></a><span class="in">    alpha=0.05, </span></span>
<span id="cb61-1439"><a href="#cb61-1439" aria-hidden="true" tabindex="-1"></a><span class="in">    power=0.9, </span></span>
<span id="cb61-1440"><a href="#cb61-1440" aria-hidden="true" tabindex="-1"></a><span class="in">    ratio=1)</span></span>
<span id="cb61-1441"><a href="#cb61-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1442"><a href="#cb61-1442" aria-hidden="true" tabindex="-1"></a><span class="in">print(required_n)</span></span>
<span id="cb61-1443"><a href="#cb61-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1444"><a href="#cb61-1444" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script.py&gt; output:</span></span>
<span id="cb61-1445"><a href="#cb61-1445" aria-hidden="true" tabindex="-1"></a><span class="in">    85.03128688801092</span></span>
<span id="cb61-1446"><a href="#cb61-1446" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1447"><a href="#cb61-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1448"><a href="#cb61-1448" aria-hidden="true" tabindex="-1"></a>_By conducting a power analysis, you've determined that approximately `85` participants are required in each group to achieve a power of `0.9`, assuming an Cohen's d effect size of `0.5`. This information is crucial for planning a sufficiently powered study to compare the energy-saving effectiveness of Smart Thermostats versus LED Lighting._</span>
<span id="cb61-1449"><a href="#cb61-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1450"><a href="#cb61-1450" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 4: Advanced Insights from Experimental Complexity</span></span>
<span id="cb61-1451"><a href="#cb61-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1452"><a href="#cb61-1452" aria-hidden="true" tabindex="-1"></a>Hop into the complexities of experimental data analysis. Learn to synthesize insights using pandas, address data issues like heteroscedasticity with <span class="in">`scipy.stats`</span>, and apply nonparametric tests like Mann-Whitney U. Learn additional techniques for transforming, visualizing, and interpreting complex data, enhancing your ability to conduct robust analyses in various experimental settings.</span>
<span id="cb61-1453"><a href="#cb61-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1454"><a href="#cb61-1454" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.1: Synthesizing insights from complex experiments</span></span>
<span id="cb61-1455"><a href="#cb61-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1456"><a href="#cb61-1456" aria-hidden="true" tabindex="-1"></a>We'll next explore how to synthesize insights from complex experiments, focusing on integrating data across different experimental factors to derive meaningful conclusions.</span>
<span id="cb61-1457"><a href="#cb61-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1458"><a href="#cb61-1458" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manufacturing yield data {.unnumbered}</span></span>
<span id="cb61-1459"><a href="#cb61-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1460"><a href="#cb61-1460" aria-hidden="true" tabindex="-1"></a>We'll work with <span class="in">`manufacturing_yield`</span> dataset, which captures how factors like <span class="in">`material`</span> type, <span class="in">`production speed`</span>, and <span class="in">`temperature`</span> settings impact the <span class="in">`yield`</span> in our experiment. The <span class="in">`BatchID`</span> column stores a unique identifier for each item in the data. Determining whether these factors have an impact on the yield strength can be used to optimize manufacturing outcomes.</span>
<span id="cb61-1461"><a href="#cb61-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1462"><a href="#cb61-1462" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1463"><a href="#cb61-1463" aria-hidden="true" tabindex="-1"></a><span class="in">manufacturing_yield</span></span>
<span id="cb61-1464"><a href="#cb61-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1465"><a href="#cb61-1465" aria-hidden="true" tabindex="-1"></a><span class="in">BatchID MaterialType ProductionSpeed TemperatureSetting YieldStrength</span></span>
<span id="cb61-1466"><a href="#cb61-1466" aria-hidden="true" tabindex="-1"></a><span class="in">39           Polymer          Medium            Optimal         58.83</span></span>
<span id="cb61-1467"><a href="#cb61-1467" aria-hidden="true" tabindex="-1"></a><span class="in">195            Metal            High               High         51.29</span></span>
<span id="cb61-1468"><a href="#cb61-1468" aria-hidden="true" tabindex="-1"></a><span class="in">462           Polymer           High             Optimal        55.15</span></span>
<span id="cb61-1469"><a href="#cb61-1469" aria-hidden="true" tabindex="-1"></a><span class="in">696         Composite         Medium                 Low        50.27</span></span>
<span id="cb61-1470"><a href="#cb61-1470" aria-hidden="true" tabindex="-1"></a><span class="in">142         Composite           High                 Low        57.62</span></span>
<span id="cb61-1471"><a href="#cb61-1471" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1472"><a href="#cb61-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1473"><a href="#cb61-1473" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manufacturing quality data {.unnumbered}</span></span>
<span id="cb61-1474"><a href="#cb61-1474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1475"><a href="#cb61-1475" aria-hidden="true" tabindex="-1"></a>A separate experiment was also done on the same items exploring the impact of production speed on the quality of the product as the response. This data is stored in the <span class="in">`manufacturing_quality`</span> DataFrame.</span>
<span id="cb61-1476"><a href="#cb61-1476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1477"><a href="#cb61-1477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1478"><a href="#cb61-1478" aria-hidden="true" tabindex="-1"></a><span class="in">manufacturing_quality</span></span>
<span id="cb61-1479"><a href="#cb61-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1480"><a href="#cb61-1480" aria-hidden="true" tabindex="-1"></a><span class="in">BatchID ProductionSpeed ProductQuality</span></span>
<span id="cb61-1481"><a href="#cb61-1481" aria-hidden="true" tabindex="-1"></a><span class="in">149                 Low         93.87</span></span>
<span id="cb61-1482"><a href="#cb61-1482" aria-hidden="true" tabindex="-1"></a><span class="in">739                 High        93.35</span></span>
<span id="cb61-1483"><a href="#cb61-1483" aria-hidden="true" tabindex="-1"></a><span class="in">617               Medium        90.45</span></span>
<span id="cb61-1484"><a href="#cb61-1484" aria-hidden="true" tabindex="-1"></a><span class="in">131                 High        90.26</span></span>
<span id="cb61-1485"><a href="#cb61-1485" aria-hidden="true" tabindex="-1"></a><span class="in">684                  Low        91.62</span></span>
<span id="cb61-1486"><a href="#cb61-1486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1487"><a href="#cb61-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1488"><a href="#cb61-1488" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Merging strategy {.unnumbered}</span></span>
<span id="cb61-1489"><a href="#cb61-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1490"><a href="#cb61-1490" aria-hidden="true" tabindex="-1"></a>We can use the pandas merge method to seamlessly integrate the <span class="in">`manufacturing_yield`</span> and <span class="in">`manufacturing_quality`</span> datasets, joining on the <span class="in">`BatchID`</span> and <span class="in">`ProductionSpeed`</span> columns so associated data is connected together. We can now explore this data in a variety of ways, looking for relationships in the data with the two response columns of yield and quality.</span>
<span id="cb61-1491"><a href="#cb61-1491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1492"><a href="#cb61-1492" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1493"><a href="#cb61-1493" aria-hidden="true" tabindex="-1"></a><span class="in">merged_manufacturing = pd.merge(manufacturing_yield, manufacturing_quality, on=['BatchID', 'ProductionSpeed'])</span></span>
<span id="cb61-1494"><a href="#cb61-1494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1495"><a href="#cb61-1495" aria-hidden="true" tabindex="-1"></a><span class="in">print(merged_manufacturing)</span></span>
<span id="cb61-1496"><a href="#cb61-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1497"><a href="#cb61-1497" aria-hidden="true" tabindex="-1"></a><span class="in">BatchID MaterialType ProductionSpeed TemperatureSetting YieldStrength ProductQuality</span></span>
<span id="cb61-1498"><a href="#cb61-1498" aria-hidden="true" tabindex="-1"></a><span class="in">1               Metal            Low               High         57.32          91.19</span></span>
<span id="cb61-1499"><a href="#cb61-1499" aria-hidden="true" tabindex="-1"></a><span class="in">5           Composite         Medium             Optimal        51.82          90.20</span></span>
<span id="cb61-1500"><a href="#cb61-1500" aria-hidden="true" tabindex="-1"></a><span class="in">7             Polymer            Low                High        56.12          91.66</span></span>
<span id="cb61-1501"><a href="#cb61-1501" aria-hidden="true" tabindex="-1"></a><span class="in">8           Composite           High             Optimal        50.91          93.05</span></span>
<span id="cb61-1502"><a href="#cb61-1502" aria-hidden="true" tabindex="-1"></a><span class="in">11            Polymer            Low                High         50.13         92.31</span></span>
<span id="cb61-1503"><a href="#cb61-1503" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1504"><a href="#cb61-1504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1505"><a href="#cb61-1505" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Side-by-side bar graph {.unnumbered}</span></span>
<span id="cb61-1506"><a href="#cb61-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1507"><a href="#cb61-1507" aria-hidden="true" tabindex="-1"></a>We can showcase potential interactions between <span class="in">`MaterialType`</span> and <span class="in">`ProductionSpeed`</span> on <span class="in">`YieldStrength`</span> using Seaborn's catplot function. <span class="in">`Yield`</span> is on the vertical axis broken down by <span class="in">`material`</span> on the horizontal, and the bars are colored by <span class="in">`ProductionSpeed`</span>. It seems that Polymer tends to have the highest yield followed by Composite and then by Metal. Production speed has a negative impact on yield across each of the materials as well with slower production leading to better yield than faster production.</span>
<span id="cb61-1508"><a href="#cb61-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1509"><a href="#cb61-1509" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1510"><a href="#cb61-1510" aria-hidden="true" tabindex="-1"></a><span class="in">import seaborn as sns</span></span>
<span id="cb61-1511"><a href="#cb61-1511" aria-hidden="true" tabindex="-1"></a><span class="in">sns.catplot(x='MaterialType', y='YieldStrength', hue='ProductionSpeed', kind='bar',</span></span>
<span id="cb61-1512"><a href="#cb61-1512" aria-hidden="true" tabindex="-1"></a><span class="in">data=merged_manufacturing)</span></span>
<span id="cb61-1513"><a href="#cb61-1513" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1514"><a href="#cb61-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1515"><a href="#cb61-1515" aria-hidden="true" tabindex="-1"></a><span class="al">![image](catplot.png)</span></span>
<span id="cb61-1516"><a href="#cb61-1516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1517"><a href="#cb61-1517" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Three variable scatterplot {.unnumbered}</span></span>
<span id="cb61-1518"><a href="#cb61-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1519"><a href="#cb61-1519" aria-hidden="true" tabindex="-1"></a>To further explore relationships in the data, we can look to see how both of the response variables relate conditioned on ProductionSpeed. We use a scatterplot with each of the response variables on the axes colored by speed. The green High values tend to be lower in each, with the orange Medium values more near the center of the plot, and the low ProductionSpeed points tending to be near the upper right of the plot.</span>
<span id="cb61-1520"><a href="#cb61-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1521"><a href="#cb61-1521" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1522"><a href="#cb61-1522" aria-hidden="true" tabindex="-1"></a><span class="in">sns.relplot(x='YieldStrength', y='ProductQuality', hue='ProductionSpeed',</span></span>
<span id="cb61-1523"><a href="#cb61-1523" aria-hidden="true" tabindex="-1"></a><span class="in">kind='scatter', data=merged_manufacturing)</span></span>
<span id="cb61-1524"><a href="#cb61-1524" aria-hidden="true" tabindex="-1"></a><span class="in">plt.title('Yield Strength vs. Product Quality by Production Speed')</span></span>
<span id="cb61-1525"><a href="#cb61-1525" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1526"><a href="#cb61-1526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1527"><a href="#cb61-1527" aria-hidden="true" tabindex="-1"></a><span class="al">![](scatter.png)</span></span>
<span id="cb61-1528"><a href="#cb61-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1529"><a href="#cb61-1529" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Communicating data to technical audiences {.unnumbered}</span></span>
<span id="cb61-1530"><a href="#cb61-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1531"><a href="#cb61-1531" aria-hidden="true" tabindex="-1"></a>Now that we've seen some visualizations on complex experimental data, let's focus on how we can tailor our approach when presenting to technical audiences. Crafting data narratives for this group involves integrating detailed statistical analysis, such as p-values, test statistics, and significance levels, into our stories. This not only enriches the narrative but also supports the validity of our findings with concrete evidence. Additionally, visualizing complex data for technical stakeholders should go beyond basic charts and include advanced visualizations like heat maps, scatter plots using multiple colors, and projection lines. These types of visuals can more precisely demonstrate relationships and trends within the data, catering to an audience that values depth and detail in data exploration.</span>
<span id="cb61-1532"><a href="#cb61-1532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1533"><a href="#cb61-1533" aria-hidden="true" tabindex="-1"></a><span class="al">![](comm.png)</span></span>
<span id="cb61-1534"><a href="#cb61-1534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1535"><a href="#cb61-1535" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Engaging non-technical audiences with data {.unnumbered}</span></span>
<span id="cb61-1536"><a href="#cb61-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1537"><a href="#cb61-1537" aria-hidden="true" tabindex="-1"></a>Moving on to non-technical audiences, our focus shifts towards simplifying the insights derived from our data. It's crucial to distill complex information into its essence, presenting it in a clear and straightforward manner. Use foundational visualizations like bar graphs and line charts, which are easier to interpret and highlight key points without the need for statistical jargon. When preparing presentations for a non-technical crowd, ensure that the content is audience-centric by highlighting why the data matters to them in practical terms. Connect the data insights to real-world applications and outcomes that resonate with their interests and professional challenges. This approach not only maintains relevance but also enhances engagement by aligning the presentation contents with their level of expertise and need for application rather than detailed analysis.</span>
<span id="cb61-1538"><a href="#cb61-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1539"><a href="#cb61-1539" aria-hidden="true" tabindex="-1"></a><span class="al">![](eng.png)</span></span>
<span id="cb61-1540"><a href="#cb61-1540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1541"><a href="#cb61-1541" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.1</span></span>
<span id="cb61-1542"><a href="#cb61-1542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1543"><a href="#cb61-1543" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing loan approval yield {.unnumbered}</span></span>
<span id="cb61-1544"><a href="#cb61-1544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1545"><a href="#cb61-1545" aria-hidden="true" tabindex="-1"></a>In the realm of financial services, understanding the factors that influence loan approval rates is crucial for both lenders and borrowers. A financial institution has conducted a study and collected data on loan applications, detailing the amount requested, the applicant's credit score, employment status, and the ultimate yield of the approval process. This rich dataset offers a window into the nuanced dynamics at play in loan decision-making. You have been asked to dive into the <span class="in">`loan_approval_yield`</span> dataset to understand how loan amounts and credit scores influence approval yields.</span>
<span id="cb61-1546"><a href="#cb61-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1547"><a href="#cb61-1547" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1548"><a href="#cb61-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1549"><a href="#cb61-1549" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a side-by-side bar graph, setting the x-axis to <span class="in">`'LoanAmount'`</span>, the y-axis to <span class="in">`'ApprovalYield'`</span>, and differentiating the bars with hues for <span class="in">`'CreditScore'`</span>.</span>
<span id="cb61-1550"><a href="#cb61-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1551"><a href="#cb61-1551" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Question: What does the analysis of approval yields across different credit scores and loan amounts reveal?</span>
<span id="cb61-1552"><a href="#cb61-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1553"><a href="#cb61-1553" aria-hidden="true" tabindex="-1"></a>_The data shows that Poor credit scores tend to have similar approval yields across various loan amounts, while Good credit scores exhibit more variability, reflecting different lending criteria based on the loan size._</span>
<span id="cb61-1554"><a href="#cb61-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1557"><a href="#cb61-1557" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1558"><a href="#cb61-1558" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1559"><a href="#cb61-1559" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1560"><a href="#cb61-1560" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1561"><a href="#cb61-1561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1562"><a href="#cb61-1562" aria-hidden="true" tabindex="-1"></a>loan_approval_yield <span class="op">=</span> pd.read_csv(<span class="st">'datasets/loan_approval_yield.csv'</span>)</span>
<span id="cb61-1563"><a href="#cb61-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1564"><a href="#cb61-1564" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Seaborn to create the bar graph</span></span>
<span id="cb61-1565"><a href="#cb61-1565" aria-hidden="true" tabindex="-1"></a>sns.catplot(x<span class="op">=</span><span class="st">"LoanAmount"</span>, </span>
<span id="cb61-1566"><a href="#cb61-1566" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"ApprovalYield"</span>, </span>
<span id="cb61-1567"><a href="#cb61-1567" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">"CreditScore"</span>, </span>
<span id="cb61-1568"><a href="#cb61-1568" aria-hidden="true" tabindex="-1"></a>            kind<span class="op">=</span><span class="st">"bar"</span>, </span>
<span id="cb61-1569"><a href="#cb61-1569" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>loan_approval_yield)</span>
<span id="cb61-1570"><a href="#cb61-1570" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loan Approval Yield by Amount and Credit Score"</span>)</span>
<span id="cb61-1571"><a href="#cb61-1571" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1572"><a href="#cb61-1572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1573"><a href="#cb61-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1574"><a href="#cb61-1574" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.2</span></span>
<span id="cb61-1575"><a href="#cb61-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1576"><a href="#cb61-1576" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exploring customer satisfaction {.unnumbered}</span></span>
<span id="cb61-1577"><a href="#cb61-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1578"><a href="#cb61-1578" aria-hidden="true" tabindex="-1"></a>Merging datasets is a crucial skill in data analysis, especially when dealing with related data from different sources. You're working on a project for a financial institution to understand the relationship between loan approval rates and customer satisfaction. Two separate studies have been conducted: one focusing on loan approval yield based on various factors, and another on customer satisfaction under different conditions. Your task is to analyze how approval yield correlates with customer satisfaction, considering another variable such as interest rates.</span>
<span id="cb61-1579"><a href="#cb61-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1580"><a href="#cb61-1580" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions  {.unnumbered}</span></span>
<span id="cb61-1581"><a href="#cb61-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1582"><a href="#cb61-1582" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Merge <span class="in">`loan_approval_yield`</span> with <span class="in">`customer_satisfaction`</span>.</span>
<span id="cb61-1583"><a href="#cb61-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1584"><a href="#cb61-1584" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Create a scatter plot to compare <span class="in">`'SatisfactionQuality'`</span> versus <span class="in">`'ApprovalYield'`</span>, coloring the points by <span class="in">`'InterestRate'`</span>.</span>
<span id="cb61-1585"><a href="#cb61-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1588"><a href="#cb61-1588" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1589"><a href="#cb61-1589" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1590"><a href="#cb61-1590" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1591"><a href="#cb61-1591" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1592"><a href="#cb61-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1593"><a href="#cb61-1593" aria-hidden="true" tabindex="-1"></a>loan_approval_yield <span class="op">=</span> pd.read_csv(<span class="st">'datasets/loan_approval_yield.csv'</span>)</span>
<span id="cb61-1594"><a href="#cb61-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1595"><a href="#cb61-1595" aria-hidden="true" tabindex="-1"></a>customer_satisfaction <span class="op">=</span> pd.read_csv(<span class="st">'datasets/customer_satisfaction.csv'</span>)</span>
<span id="cb61-1596"><a href="#cb61-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1597"><a href="#cb61-1597" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge the two datasets</span></span>
<span id="cb61-1598"><a href="#cb61-1598" aria-hidden="true" tabindex="-1"></a>merged_data <span class="op">=</span> pd.merge(loan_approval_yield, </span>
<span id="cb61-1599"><a href="#cb61-1599" aria-hidden="true" tabindex="-1"></a>                      customer_satisfaction, </span>
<span id="cb61-1600"><a href="#cb61-1600" aria-hidden="true" tabindex="-1"></a>                      on<span class="op">=</span><span class="st">'ApplicationID'</span>)</span>
<span id="cb61-1601"><a href="#cb61-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1602"><a href="#cb61-1602" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Seaborn to create the scatter plot</span></span>
<span id="cb61-1603"><a href="#cb61-1603" aria-hidden="true" tabindex="-1"></a>sns.relplot(x<span class="op">=</span><span class="st">"ApprovalYield"</span>, </span>
<span id="cb61-1604"><a href="#cb61-1604" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"SatisfactionQuality"</span>, </span>
<span id="cb61-1605"><a href="#cb61-1605" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="st">"InterestRate"</span>, </span>
<span id="cb61-1606"><a href="#cb61-1606" aria-hidden="true" tabindex="-1"></a>            kind<span class="op">=</span><span class="st">"scatter"</span>, </span>
<span id="cb61-1607"><a href="#cb61-1607" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>merged_data)</span>
<span id="cb61-1608"><a href="#cb61-1608" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Satisfaction Quality by Approval Yield and Interest Rate"</span>)</span>
<span id="cb61-1609"><a href="#cb61-1609" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1610"><a href="#cb61-1610" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1611"><a href="#cb61-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1612"><a href="#cb61-1612" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Question: What does the scatterplot of Customer Satisfaction versus Approval Yield, including Interest Rate as a variable, indicate about their relationship in the experimental data?</span>
<span id="cb61-1613"><a href="#cb61-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1614"><a href="#cb61-1614" aria-hidden="true" tabindex="-1"></a>_There isn't a strong relationship between Customer Satisfaction and Approval Yield in this experimental data. The resulting scatterplot looks similar to white noise scattered all about even when including Interest Rate_</span>
<span id="cb61-1615"><a href="#cb61-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1616"><a href="#cb61-1616" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.2: Addressing complexities in experimental data</span></span>
<span id="cb61-1617"><a href="#cb61-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1618"><a href="#cb61-1618" aria-hidden="true" tabindex="-1"></a>Next, we will look into addressing complexities in experimental data, focusing on identifying and mitigating issues like interactions, confounding variables, and heteroscedasticity.</span>
<span id="cb61-1619"><a href="#cb61-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1620"><a href="#cb61-1620" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Geological data {.unnumbered}</span></span>
<span id="cb61-1621"><a href="#cb61-1621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1622"><a href="#cb61-1622" aria-hidden="true" tabindex="-1"></a>The <span class="in">`mineral_rocks`</span> dataset encompasses 300 rock samples, detailing attributes like rock type, geographical location, mineral hardness, and rock porosity. Each entry in the dataset represents a unique sample, identified by its <span class="in">`SampleID`</span>, and characterized by varying levels of  <span class="in">`MineralHardness`</span> and <span class="in">`RockPorosity`</span> across different rock types and locations. Understanding the distribution and interactions within this data is critical for selecting the right statistical tests for our analysis.</span>
<span id="cb61-1623"><a href="#cb61-1623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1624"><a href="#cb61-1624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1625"><a href="#cb61-1625" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks</span></span>
<span id="cb61-1626"><a href="#cb61-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1627"><a href="#cb61-1627" aria-hidden="true" tabindex="-1"></a><span class="in">SampleID     RockType Location MineralHardness RockPorosity</span></span>
<span id="cb61-1628"><a href="#cb61-1628" aria-hidden="true" tabindex="-1"></a><span class="in">       1  Metamorphic     West             5.9         12.3</span></span>
<span id="cb61-1629"><a href="#cb61-1629" aria-hidden="true" tabindex="-1"></a><span class="in">       2      Igneous    North             5.3          1.6</span></span>
<span id="cb61-1630"><a href="#cb61-1630" aria-hidden="true" tabindex="-1"></a><span class="in">       3  Metamorphic     East             5.6         11.0</span></span>
<span id="cb61-1631"><a href="#cb61-1631" aria-hidden="true" tabindex="-1"></a><span class="in">       4  Metamorphic    South             3.2         12.2</span></span>
<span id="cb61-1632"><a href="#cb61-1632" aria-hidden="true" tabindex="-1"></a><span class="in">       5  Sedimentary    South             2.0         29.8</span></span>
<span id="cb61-1633"><a href="#cb61-1633" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1634"><a href="#cb61-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1635"><a href="#cb61-1635" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding data complexities {.unnumbered}</span></span>
<span id="cb61-1636"><a href="#cb61-1636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1637"><a href="#cb61-1637" aria-hidden="true" tabindex="-1"></a>Our exploration begins by identifying potential complexities within our <span class="in">`mineral_rocks`</span> dataset: Interactions between rock types and their mineral hardness might influence the observed mineral properties. The variance in rock porosity, a key feature of our dataset, might not be consistent across all samples, indicating potential heteroscedasticity. There could be confounding variables that affect both mineral hardness and rock porosity. This is often the hardest problem to solve as it likely means that further data gathering is necessary to retrieve that extra variable information. Understanding these issues helps us decide whether parametric tests, which assume normality and homoscedasticity, can be employed or if we should rely on non-parametric tests, not assuming a specific distribution.</span>
<span id="cb61-1638"><a href="#cb61-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1639"><a href="#cb61-1639" aria-hidden="true" tabindex="-1"></a><span class="al">![](rock.png)</span></span>
<span id="cb61-1640"><a href="#cb61-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1641"><a href="#cb61-1641" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Addressing interactions {.unnumbered}</span></span>
<span id="cb61-1642"><a href="#cb61-1642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1643"><a href="#cb61-1643" aria-hidden="true" tabindex="-1"></a>With the <span class="in">`mineral_rocks`</span> dataset, we begin by visualizing the relationship between <span class="in">`MineralHardness`</span> and <span class="in">`RockPorosity`</span>, colored by <span class="in">`RockType`</span>. This initial exploration helps identify potential complexities, such as interactions between variables. We seem to have an interaction between rock type and mineral hardness on rock porosity from the plot, since there are distinct groupings by <span class="in">`RockType`</span>. Addressing interactions helps us understand whether more robust non-parametric methods are necessary for accurate analysis.</span>
<span id="cb61-1644"><a href="#cb61-1644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1645"><a href="#cb61-1645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1646"><a href="#cb61-1646" aria-hidden="true" tabindex="-1"></a><span class="in">sns.scatterplot(x='MineralHardness', y='RockPorosity',</span></span>
<span id="cb61-1647"><a href="#cb61-1647" aria-hidden="true" tabindex="-1"></a><span class="in">hue='RockType', data=mineral_rocks)</span></span>
<span id="cb61-1648"><a href="#cb61-1648" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1649"><a href="#cb61-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1650"><a href="#cb61-1650" aria-hidden="true" tabindex="-1"></a><span class="al">![](graph.png)</span></span>
<span id="cb61-1651"><a href="#cb61-1651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1652"><a href="#cb61-1652" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Addressing heteroscedasticity {.unnumbered}</span></span>
<span id="cb61-1653"><a href="#cb61-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1654"><a href="#cb61-1654" aria-hidden="true" tabindex="-1"></a>Heteroscedasticity refers to the changing variability of a variable across the range of another variable. We use Seaborn's <span class="in">`residplot`</span> to check for heteroscedasticity in our data, plotting residuals of <span class="in">`RockPorosity`</span> against <span class="in">`MineralHardness`</span>. We include the <span class="in">`lowess`</span> smoothing option to show the trend in the data going from left to right. We see that, overall, the <span class="in">`lowess`</span> line remains somewhat close to 0 and relatively flat, but the curve does lead us to be a little cautious since it highlights the spread being different in some areas of our data.</span>
<span id="cb61-1655"><a href="#cb61-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1656"><a href="#cb61-1656" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1657"><a href="#cb61-1657" aria-hidden="true" tabindex="-1"></a><span class="in">sns.residplot(x='MineralHardness', y='RockPorosity',</span></span>
<span id="cb61-1658"><a href="#cb61-1658" aria-hidden="true" tabindex="-1"></a><span class="in">data=mineral_rocks, lowess=True)</span></span>
<span id="cb61-1659"><a href="#cb61-1659" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1660"><a href="#cb61-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1661"><a href="#cb61-1661" aria-hidden="true" tabindex="-1"></a><span class="al">![](graph2.png)</span></span>
<span id="cb61-1662"><a href="#cb61-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1663"><a href="#cb61-1663" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Non-normal data {.unnumbered}</span></span>
<span id="cb61-1664"><a href="#cb61-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1665"><a href="#cb61-1665" aria-hidden="true" tabindex="-1"></a>When the residual plot deviates from expectations, it can be useful to explore the distribution of the variables used. Here, we investigate <span class="in">`RockPorosity`</span> with a histogram using Seaborn's <span class="in">`displot`</span> function. We see that the data is skewed and of a non-normal shape.</span>
<span id="cb61-1666"><a href="#cb61-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1667"><a href="#cb61-1667" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1668"><a href="#cb61-1668" aria-hidden="true" tabindex="-1"></a><span class="in">sns.displot(mineral_rocks['RockPorosity'])</span></span>
<span id="cb61-1669"><a href="#cb61-1669" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1670"><a href="#cb61-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1671"><a href="#cb61-1671" aria-hidden="true" tabindex="-1"></a><span class="al">![](graph3.png)</span></span>
<span id="cb61-1672"><a href="#cb61-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1673"><a href="#cb61-1673" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data transformation with Box-Cox {.unnumbered}</span></span>
<span id="cb61-1674"><a href="#cb61-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1675"><a href="#cb61-1675" aria-hidden="true" tabindex="-1"></a>To address issues like skewness and heteroscedasticity, we can apply data transformations. Here, we use the <span class="in">`Box-Cox`</span> transformation from <span class="in">`scipy.stats`</span> on <span class="in">`RockPorosity`</span> to stabilize variance and make the data more closely resemble a normal distribution. We add the transformed data as a column to our DataFrame. The <span class="in">`Box-Cox`</span> transformation requires non-zero entries, which we have for all <span class="in">`RockPorosity`</span> values. Note that this transformed data isn't perfectly normal, but does have much more of that bell shape than it did originally.</span>
<span id="cb61-1676"><a href="#cb61-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1677"><a href="#cb61-1677" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1678"><a href="#cb61-1678" aria-hidden="true" tabindex="-1"></a><span class="in">from scipy.stats import boxcox</span></span>
<span id="cb61-1679"><a href="#cb61-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1680"><a href="#cb61-1680" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks['TransformedRockPorosity'], _ = boxcox(mineral_rocks['RockPorosity'])</span></span>
<span id="cb61-1681"><a href="#cb61-1681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1682"><a href="#cb61-1682" aria-hidden="true" tabindex="-1"></a><span class="in">sns.displot(mineral_rocks['TransformedRockPorosity'])</span></span>
<span id="cb61-1683"><a href="#cb61-1683" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1684"><a href="#cb61-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1685"><a href="#cb61-1685" aria-hidden="true" tabindex="-1"></a><span class="al">![](graph4.png)</span></span>
<span id="cb61-1686"><a href="#cb61-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1687"><a href="#cb61-1687" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Post-transformation analysis {.unnumbered}</span></span>
<span id="cb61-1688"><a href="#cb61-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1689"><a href="#cb61-1689" aria-hidden="true" tabindex="-1"></a>To verify that we've better addressed the heteroscedasticity with the Box-Cox transformation, we can repeat our <span class="in">`residplot`</span> with the <span class="in">`TransformedRockPorosity`</span>. This visualization helps us understand whether the Box-Cox transformation has successfully stabilized the variance across the range of  <span class="in">`MineralHardness`</span>, an important assumption for many statistical tests. The <span class="in">`lowess`</span> line is now much flatter, going from left to right across the plot. We can now feel more confident that this transformed data has better addressed heteroscedasticity than the non-transformed data.</span>
<span id="cb61-1690"><a href="#cb61-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1691"><a href="#cb61-1691" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1692"><a href="#cb61-1692" aria-hidden="true" tabindex="-1"></a><span class="in">sns.residplot(x='MineralHardness', y='TransformedRockPorosity',</span></span>
<span id="cb61-1693"><a href="#cb61-1693" aria-hidden="true" tabindex="-1"></a><span class="in">data=mineral_rocks, lowess=True)</span></span>
<span id="cb61-1694"><a href="#cb61-1694" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1695"><a href="#cb61-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1696"><a href="#cb61-1696" aria-hidden="true" tabindex="-1"></a><span class="al">![](graph5.png)</span></span>
<span id="cb61-1697"><a href="#cb61-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1698"><a href="#cb61-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1699"><a href="#cb61-1699" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.1</span></span>
<span id="cb61-1700"><a href="#cb61-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1701"><a href="#cb61-1701" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Check for heteroscedasticity in shelf life {.unnumbered}</span></span>
<span id="cb61-1702"><a href="#cb61-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1703"><a href="#cb61-1703" aria-hidden="true" tabindex="-1"></a>When examining food preservation methods, it's crucial to understand how the variance of one variable, such as shelf life, might change across the range of another variable like nutrient retention. Identifying such patterns, known as heteroscedasticity, can provide insights into the consistency of preservation effects. The <span class="in">`food_preservation`</span> dataset encapsulates the outcomes of various preservation methods on different food types, specifically highlighting the balance between nutrient retention and resultant shelf life.</span>
<span id="cb61-1704"><a href="#cb61-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1705"><a href="#cb61-1705" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1706"><a href="#cb61-1706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1707"><a href="#cb61-1707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use an appropriate plot to check for heteroscedasticity between <span class="in">`'NutrientRetention'`</span> and <span class="in">`'ShelfLife'`</span>.</span>
<span id="cb61-1708"><a href="#cb61-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1711"><a href="#cb61-1711" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1712"><a href="#cb61-1712" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1713"><a href="#cb61-1713" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1714"><a href="#cb61-1714" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1715"><a href="#cb61-1715" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1716"><a href="#cb61-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1717"><a href="#cb61-1717" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1718"><a href="#cb61-1718" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb61-1719"><a href="#cb61-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1720"><a href="#cb61-1720" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb61-1721"><a href="#cb61-1721" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb61-1722"><a href="#cb61-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1723"><a href="#cb61-1723" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb61-1724"><a href="#cb61-1724" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb61-1725"><a href="#cb61-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1726"><a href="#cb61-1726" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb61-1727"><a href="#cb61-1727" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1728"><a href="#cb61-1728" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1729"><a href="#cb61-1729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1730"><a href="#cb61-1730" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb61-1731"><a href="#cb61-1731" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1732"><a href="#cb61-1732" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1733"><a href="#cb61-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1734"><a href="#cb61-1734" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb61-1735"><a href="#cb61-1735" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb61-1736"><a href="#cb61-1736" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb61-1737"><a href="#cb61-1737" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb61-1738"><a href="#cb61-1738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1739"><a href="#cb61-1739" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb61-1740"><a href="#cb61-1740" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb61-1741"><a href="#cb61-1741" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb61-1742"><a href="#cb61-1742" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb61-1743"><a href="#cb61-1743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1744"><a href="#cb61-1744" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb61-1745"><a href="#cb61-1745" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb61-1746"><a href="#cb61-1746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1747"><a href="#cb61-1747" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb61-1748"><a href="#cb61-1748" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1749"><a href="#cb61-1749" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb61-1750"><a href="#cb61-1750" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb61-1751"><a href="#cb61-1751" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb61-1752"><a href="#cb61-1752" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1753"><a href="#cb61-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1754"><a href="#cb61-1754" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb61-1755"><a href="#cb61-1755" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-1756"><a href="#cb61-1756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1757"><a href="#cb61-1757" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for heteroscedasticity with a residual plot</span></span>
<span id="cb61-1758"><a href="#cb61-1758" aria-hidden="true" tabindex="-1"></a>sns.residplot(x<span class="op">=</span><span class="st">'NutrientRetention'</span>, y<span class="op">=</span><span class="st">'ShelfLife'</span>, </span>
<span id="cb61-1759"><a href="#cb61-1759" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>food_preservation, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-1760"><a href="#cb61-1760" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Residual Plot of Shelf Life and Nutrient Retention'</span>)</span>
<span id="cb61-1761"><a href="#cb61-1761" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Nutrient Retention (%)'</span>)</span>
<span id="cb61-1762"><a href="#cb61-1762" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb61-1763"><a href="#cb61-1763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1764"><a href="#cb61-1764" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1765"><a href="#cb61-1765" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1766"><a href="#cb61-1766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1767"><a href="#cb61-1767" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.3</span></span>
<span id="cb61-1768"><a href="#cb61-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1769"><a href="#cb61-1769" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exploring and transforming shelf life data {.unnumbered}</span></span>
<span id="cb61-1770"><a href="#cb61-1770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1771"><a href="#cb61-1771" aria-hidden="true" tabindex="-1"></a>Understanding the distribution of different variables in our data is a key aspect of any data work including experimental analysis. The <span class="in">`food_preservation`</span> dataset captures various food preservation methods and their impact on nutrient retention and shelf life. A crucial aspect of this data involves the shelf life of preserved foods, which can vary significantly across different preservation methods and food types.</span>
<span id="cb61-1772"><a href="#cb61-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1773"><a href="#cb61-1773" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1774"><a href="#cb61-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1775"><a href="#cb61-1775" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Visualize the original distribution of the <span class="in">`'ShelfLife'`</span> column.</span>
<span id="cb61-1776"><a href="#cb61-1776" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Visualize the distribution of the <span class="in">`'ShelfLifeTransformed'`</span>.</span>
<span id="cb61-1777"><a href="#cb61-1777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1780"><a href="#cb61-1780" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1781"><a href="#cb61-1781" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1782"><a href="#cb61-1782" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1783"><a href="#cb61-1783" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1784"><a href="#cb61-1784" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1785"><a href="#cb61-1785" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> boxcox</span>
<span id="cb61-1786"><a href="#cb61-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1787"><a href="#cb61-1787" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1788"><a href="#cb61-1788" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb61-1789"><a href="#cb61-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1790"><a href="#cb61-1790" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb61-1791"><a href="#cb61-1791" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb61-1792"><a href="#cb61-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1793"><a href="#cb61-1793" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb61-1794"><a href="#cb61-1794" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb61-1795"><a href="#cb61-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1796"><a href="#cb61-1796" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb61-1797"><a href="#cb61-1797" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1798"><a href="#cb61-1798" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1799"><a href="#cb61-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1800"><a href="#cb61-1800" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb61-1801"><a href="#cb61-1801" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1802"><a href="#cb61-1802" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1803"><a href="#cb61-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1804"><a href="#cb61-1804" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb61-1805"><a href="#cb61-1805" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb61-1806"><a href="#cb61-1806" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb61-1807"><a href="#cb61-1807" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb61-1808"><a href="#cb61-1808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1809"><a href="#cb61-1809" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb61-1810"><a href="#cb61-1810" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb61-1811"><a href="#cb61-1811" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb61-1812"><a href="#cb61-1812" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb61-1813"><a href="#cb61-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1814"><a href="#cb61-1814" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb61-1815"><a href="#cb61-1815" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb61-1816"><a href="#cb61-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1817"><a href="#cb61-1817" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb61-1818"><a href="#cb61-1818" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1819"><a href="#cb61-1819" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb61-1820"><a href="#cb61-1820" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb61-1821"><a href="#cb61-1821" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb61-1822"><a href="#cb61-1822" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1823"><a href="#cb61-1823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1824"><a href="#cb61-1824" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb61-1825"><a href="#cb61-1825" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-1826"><a href="#cb61-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1827"><a href="#cb61-1827" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the original ShelfLife distribution</span></span>
<span id="cb61-1828"><a href="#cb61-1828" aria-hidden="true" tabindex="-1"></a>sns.displot(food_preservation[<span class="st">'ShelfLife'</span>])</span>
<span id="cb61-1829"><a href="#cb61-1829" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Shelf Life Distribution'</span>)</span>
<span id="cb61-1830"><a href="#cb61-1830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1831"><a href="#cb61-1831" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1832"><a href="#cb61-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1833"><a href="#cb61-1833" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Box-Cox transformation</span></span>
<span id="cb61-1834"><a href="#cb61-1834" aria-hidden="true" tabindex="-1"></a>ShelfLifeTransformed, _ <span class="op">=</span> boxcox(food_preservation[<span class="st">'ShelfLife'</span>])</span>
<span id="cb61-1835"><a href="#cb61-1835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1836"><a href="#cb61-1836" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformed ShelfLife distribution</span></span>
<span id="cb61-1837"><a href="#cb61-1837" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb61-1838"><a href="#cb61-1838" aria-hidden="true" tabindex="-1"></a>sns.displot(ShelfLifeTransformed)</span>
<span id="cb61-1839"><a href="#cb61-1839" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Transformed Shelf Life Distribution'</span>)</span>
<span id="cb61-1840"><a href="#cb61-1840" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1841"><a href="#cb61-1841" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1842"><a href="#cb61-1842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1843"><a href="#cb61-1843" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.3: Applying nonparametric tests in experimental analysis</span></span>
<span id="cb61-1844"><a href="#cb61-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1845"><a href="#cb61-1845" aria-hidden="true" tabindex="-1"></a>We'll now explore the world of nonparametric tests, which are vital tools in situations where parametric test assumptions don't hold.</span>
<span id="cb61-1846"><a href="#cb61-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1847"><a href="#cb61-1847" aria-hidden="true" tabindex="-1"></a><span class="fu">#### When to use nonparametric tests {.unnumbered}</span></span>
<span id="cb61-1848"><a href="#cb61-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1849"><a href="#cb61-1849" aria-hidden="true" tabindex="-1"></a>Nonparametric tests come into play when data challenges the usual assumptions of parametric tests. For example, they serve as an alternative to needing to transform data in order for normality assumptions to hold. They're ideal for ordinal data or distributions far from normality, offering resilience against outliers and accommodating a wider range of data behaviors.</span>
<span id="cb61-1850"><a href="#cb61-1850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1851"><a href="#cb61-1851" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exploring nonparametric methods {.unnumbered}</span></span>
<span id="cb61-1852"><a href="#cb61-1852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1853"><a href="#cb61-1853" aria-hidden="true" tabindex="-1"></a>When data doesn't meet parametric assumptions, nonparametric methods offer a solution. The <span class="in">`Mann-Whitney U`</span> Test is our go-to for comparing two independent groups - the non-parametric alternative to the independent two-sample <span class="in">`t-test`</span>. When our experiment involves more than two groups with a numeric response, we turn to the <span class="in">`Kruskal-Wallis`</span> Test - the non-parametric version of the one-way ANOVA test.</span>
<span id="cb61-1854"><a href="#cb61-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1855"><a href="#cb61-1855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1856"><a href="#cb61-1856" aria-hidden="true" tabindex="-1"></a><span class="al">![](image1.png)</span></span>
<span id="cb61-1857"><a href="#cb61-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1858"><a href="#cb61-1858" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing nonparametric data {.unnumbered}</span></span>
<span id="cb61-1859"><a href="#cb61-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1860"><a href="#cb61-1860" aria-hidden="true" tabindex="-1"></a>Visualizing nonparametric data effectively can reveal underlying patterns. Violin plots offer a comprehensive view of our data's distribution across multiple groups. Let's compare <span class="in">`MineralHardness`</span> for Igneous and Metamorphic rocks from our data. We begin by using the <span class="in">`.isin()`</span> method to extract these two groups of data into a DataFrame called <span class="in">`condensed_data`</span>. Next, we use Seaborn's <span class="in">`violinplot`</span> function on the two variables of interest. This violin plot contrasts <span class="in">`MineralHardness`</span> between metamorphic and igneous rocks. Notice that the violins for each do not have a normal shape mirrored vertically, but instead exhibit some skew. Metamorphic rocks show a greater hardness range and lower median than igneous rocks (denoted by the white line in the center of each "violin"). Igneous rocks display smaller hardness variability and higher median values.</span>
<span id="cb61-1861"><a href="#cb61-1861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1862"><a href="#cb61-1862" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1863"><a href="#cb61-1863" aria-hidden="true" tabindex="-1"></a><span class="in">condensed_data = mineral_rocks[mineral_rocks['RockType'].isin(['Igneous', 'Metamorphic'])]</span></span>
<span id="cb61-1864"><a href="#cb61-1864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1865"><a href="#cb61-1865" aria-hidden="true" tabindex="-1"></a><span class="in">sns.violinplot(x='RockType', y='MineralHardness', data=condensed_data)</span></span>
<span id="cb61-1866"><a href="#cb61-1866" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1867"><a href="#cb61-1867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1868"><a href="#cb61-1868" aria-hidden="true" tabindex="-1"></a><span class="al">![](image2.png)</span></span>
<span id="cb61-1869"><a href="#cb61-1869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1870"><a href="#cb61-1870" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing nonparametric data {.unnumbered}</span></span>
<span id="cb61-1871"><a href="#cb61-1871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1872"><a href="#cb61-1872" aria-hidden="true" tabindex="-1"></a>Boxen plots are an extended version of box plots that provide more information about the shape of the distribution. We use Seaborn's <span class="in">`boxenplot`</span> function to display the distribution of <span class="in">`MineralHardness`</span> across three rock types: metamorphic, igneous, and sedimentary. Sedimentary rocks show the smallest median hardness value, with outliers indicating some extreme values. Metamorphic rocks show the most skew of the three rock types and have a median hardness between that of sedimentary and igneous. They also have a wider interquartile range, indicating significant variability. Igneous rocks exhibit the highest median hardness and a narrower interquartile range, suggesting less variability.</span>
<span id="cb61-1873"><a href="#cb61-1873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1874"><a href="#cb61-1874" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1875"><a href="#cb61-1875" aria-hidden="true" tabindex="-1"></a><span class="in">sns.boxenplot(x=</span></span>
<span id="cb61-1876"><a href="#cb61-1876" aria-hidden="true" tabindex="-1"></a><span class="in">'RockType'</span></span>
<span id="cb61-1877"><a href="#cb61-1877" aria-hidden="true" tabindex="-1"></a><span class="in">, y=</span></span>
<span id="cb61-1878"><a href="#cb61-1878" aria-hidden="true" tabindex="-1"></a><span class="in">'MineralHardness'</span></span>
<span id="cb61-1879"><a href="#cb61-1879" aria-hidden="true" tabindex="-1"></a><span class="in">, data=mineral_rocks)</span></span>
<span id="cb61-1880"><a href="#cb61-1880" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1881"><a href="#cb61-1881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1882"><a href="#cb61-1882" aria-hidden="true" tabindex="-1"></a><span class="al">![](image3.png)</span></span>
<span id="cb61-1883"><a href="#cb61-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1884"><a href="#cb61-1884" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Applying nonparametric tests - Mann Whitney U {.unnumbered}</span></span>
<span id="cb61-1885"><a href="#cb61-1885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1886"><a href="#cb61-1886" aria-hidden="true" tabindex="-1"></a>We perform the <span class="in">`Mann-Whitney U`</span> test to compare the distributions of <span class="in">`MineralHardness`</span> between igneous and sedimentary rocks using data from the <span class="in">`mineral_rocks`</span> DataFrame. We select the hardness values corresponding to each rock type and apply the test to determine if there's a statistically significant difference in their medians. The test returns a p-value of <span class="in">`0.9724`</span>. The high p-value indicates that there is no significant difference in the median mineral hardness between igneous and sedimentary rocks at the common significance levels.</span>
<span id="cb61-1887"><a href="#cb61-1887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1888"><a href="#cb61-1888" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1889"><a href="#cb61-1889" aria-hidden="true" tabindex="-1"></a><span class="in">from scipy.stats import mannwhitneyu, kruskal</span></span>
<span id="cb61-1890"><a href="#cb61-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1891"><a href="#cb61-1891" aria-hidden="true" tabindex="-1"></a><span class="in">u_stat, u_pval = mannwhitneyu(</span></span>
<span id="cb61-1892"><a href="#cb61-1892" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks[mineral_rocks['RockType'] == 'Igneous']['MineralHardness'],</span></span>
<span id="cb61-1893"><a href="#cb61-1893" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks[mineral_rocks['RockType'] == 'Sedimentary']['MineralHardness']</span></span>
<span id="cb61-1894"><a href="#cb61-1894" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb61-1895"><a href="#cb61-1895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1896"><a href="#cb61-1896" aria-hidden="true" tabindex="-1"></a><span class="in">print(f"Mann-Whitney U test p-value: {u_pval:.4f}")</span></span>
<span id="cb61-1897"><a href="#cb61-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1898"><a href="#cb61-1898" aria-hidden="true" tabindex="-1"></a><span class="in">Mann-Whitney U test p-value: 0.9724</span></span>
<span id="cb61-1899"><a href="#cb61-1899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1900"><a href="#cb61-1900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1901"><a href="#cb61-1901" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Applying nonparametric tests - Kruskal-Wallis {.unnumbered}</span></span>
<span id="cb61-1902"><a href="#cb61-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1903"><a href="#cb61-1903" aria-hidden="true" tabindex="-1"></a>We apply the <span class="in">`Kruskal-Wallis`</span> test, a nonparametric method, to determine if there are statistically significant differences in mineral hardness distributions across igneous, sedimentary, and metamorphic rock types from the <span class="in">`mineral_rocks`</span> dataset. It computes the p-value for the hypothesis that the medians of all groups are equal. This test returns a p-value of <span class="in">`0.0630`</span>, which indicates that there's a suggestion of a difference in medians, but it does not reach the conventional significance threshold of <span class="in">`0.05`</span>. Therefore, while there may be differences in mineral hardness by rock type, they are not statistically significant at the 5% level.</span>
<span id="cb61-1904"><a href="#cb61-1904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1905"><a href="#cb61-1905" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1906"><a href="#cb61-1906" aria-hidden="true" tabindex="-1"></a><span class="in">k_stat, k_pval = kruskal(</span></span>
<span id="cb61-1907"><a href="#cb61-1907" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks[mineral_rocks['RockType'] == 'Igneous']['MineralHardness'],</span></span>
<span id="cb61-1908"><a href="#cb61-1908" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks[mineral_rocks['RockType'] == 'Sedimentary']['MineralHardness'],</span></span>
<span id="cb61-1909"><a href="#cb61-1909" aria-hidden="true" tabindex="-1"></a><span class="in">mineral_rocks[mineral_rocks['RockType'] == 'Metamorphic']['MineralHardness']</span></span>
<span id="cb61-1910"><a href="#cb61-1910" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb61-1911"><a href="#cb61-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1912"><a href="#cb61-1912" aria-hidden="true" tabindex="-1"></a><span class="in">print(f"Kruskal-Wallis test p-value: {k_pval:.4f}")</span></span>
<span id="cb61-1913"><a href="#cb61-1913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1914"><a href="#cb61-1914" aria-hidden="true" tabindex="-1"></a><span class="in">Kruskal-Wallis test p-value: 0.0630</span></span>
<span id="cb61-1915"><a href="#cb61-1915" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-1916"><a href="#cb61-1916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1917"><a href="#cb61-1917" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.1</span></span>
<span id="cb61-1918"><a href="#cb61-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1919"><a href="#cb61-1919" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing and testing preservation methods {.unnumbered}</span></span>
<span id="cb61-1920"><a href="#cb61-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1921"><a href="#cb61-1921" aria-hidden="true" tabindex="-1"></a>As a food scientist, you're tasked with evaluating the effectiveness of different preservation methods on nutrient retention and how these methods impact shelf life. You have been provided with a dataset, food_preservation, that includes various types of food preserved by methods such as freezing and canning. Each entry in the dataset captures the nutrient retention and calculated shelf life for these foods, providing a unique opportunity to analyze the impacts of preservation techniques on food quality.</span>
<span id="cb61-1922"><a href="#cb61-1922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1923"><a href="#cb61-1923" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-1924"><a href="#cb61-1924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1925"><a href="#cb61-1925" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Filter the DataFrame to include only Freezing and Canning rows.</span>
<span id="cb61-1926"><a href="#cb61-1926" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Create a violin plot to visualize the distribution of nutrient retention for different preservation methods.</span>
<span id="cb61-1927"><a href="#cb61-1927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1928"><a href="#cb61-1928" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Extract the nutrient retention values for both Freezing and Canning entries.</span>
<span id="cb61-1929"><a href="#cb61-1929" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Perform a Mann Whitney U test to compare nutrient retention between Freezing and Canning methods.</span>
<span id="cb61-1930"><a href="#cb61-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1933"><a href="#cb61-1933" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-1934"><a href="#cb61-1934" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-1935"><a href="#cb61-1935" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-1936"><a href="#cb61-1936" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-1937"><a href="#cb61-1937" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-1938"><a href="#cb61-1938" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> mannwhitneyu</span>
<span id="cb61-1939"><a href="#cb61-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1940"><a href="#cb61-1940" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-1941"><a href="#cb61-1941" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb61-1942"><a href="#cb61-1942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1943"><a href="#cb61-1943" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb61-1944"><a href="#cb61-1944" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb61-1945"><a href="#cb61-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1946"><a href="#cb61-1946" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb61-1947"><a href="#cb61-1947" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb61-1948"><a href="#cb61-1948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1949"><a href="#cb61-1949" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb61-1950"><a href="#cb61-1950" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1951"><a href="#cb61-1951" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1952"><a href="#cb61-1952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1953"><a href="#cb61-1953" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb61-1954"><a href="#cb61-1954" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-1955"><a href="#cb61-1955" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-1956"><a href="#cb61-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1957"><a href="#cb61-1957" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb61-1958"><a href="#cb61-1958" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb61-1959"><a href="#cb61-1959" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb61-1960"><a href="#cb61-1960" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb61-1961"><a href="#cb61-1961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1962"><a href="#cb61-1962" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb61-1963"><a href="#cb61-1963" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb61-1964"><a href="#cb61-1964" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb61-1965"><a href="#cb61-1965" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb61-1966"><a href="#cb61-1966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1967"><a href="#cb61-1967" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb61-1968"><a href="#cb61-1968" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb61-1969"><a href="#cb61-1969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1970"><a href="#cb61-1970" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb61-1971"><a href="#cb61-1971" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-1972"><a href="#cb61-1972" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb61-1973"><a href="#cb61-1973" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb61-1974"><a href="#cb61-1974" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb61-1975"><a href="#cb61-1975" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-1976"><a href="#cb61-1976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1977"><a href="#cb61-1977" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb61-1978"><a href="#cb61-1978" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-1979"><a href="#cb61-1979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1980"><a href="#cb61-1980" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to Freezing and Canning data</span></span>
<span id="cb61-1981"><a href="#cb61-1981" aria-hidden="true" tabindex="-1"></a>condensed_food_data <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>].isin([<span class="st">'Freezing'</span>, <span class="st">'Canning'</span>])]</span>
<span id="cb61-1982"><a href="#cb61-1982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1983"><a href="#cb61-1983" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a violin plot for nutrient retention by preservation method</span></span>
<span id="cb61-1984"><a href="#cb61-1984" aria-hidden="true" tabindex="-1"></a>sns.violinplot(data<span class="op">=</span>condensed_food_data, </span>
<span id="cb61-1985"><a href="#cb61-1985" aria-hidden="true" tabindex="-1"></a>     x<span class="op">=</span><span class="st">"PreservationMethod"</span>, </span>
<span id="cb61-1986"><a href="#cb61-1986" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span><span class="st">"NutrientRetention"</span>)</span>
<span id="cb61-1987"><a href="#cb61-1987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1988"><a href="#cb61-1988" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-1989"><a href="#cb61-1989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1990"><a href="#cb61-1990" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate nutrient retention for Freezing and Canning methods</span></span>
<span id="cb61-1991"><a href="#cb61-1991" aria-hidden="true" tabindex="-1"></a>freezing <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Freezing'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb61-1992"><a href="#cb61-1992" aria-hidden="true" tabindex="-1"></a>canning <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Canning'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb61-1993"><a href="#cb61-1993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1994"><a href="#cb61-1994" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Mann Whitney U test</span></span>
<span id="cb61-1995"><a href="#cb61-1995" aria-hidden="true" tabindex="-1"></a>u_stat, p_val <span class="op">=</span> mannwhitneyu(freezing, canning)</span>
<span id="cb61-1996"><a href="#cb61-1996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-1997"><a href="#cb61-1997" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the p-value</span></span>
<span id="cb61-1998"><a href="#cb61-1998" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mann Whitney U test p-value:"</span>, p_val)</span>
<span id="cb61-1999"><a href="#cb61-1999" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-2000"><a href="#cb61-2000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2001"><a href="#cb61-2001" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-2002"><a href="#cb61-2002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2003"><a href="#cb61-2003" aria-hidden="true" tabindex="-1"></a>_The violin plot shows that the distribution and median values are similar across Freezing and Canning. The large p-value leads us to suspect that a statistical difference does not exist in the medians of nutrient retention for freezing versus canning preservation methods._</span>
<span id="cb61-2004"><a href="#cb61-2004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2005"><a href="#cb61-2005" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb61-2006"><a href="#cb61-2006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2007"><a href="#cb61-2007" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.2</span></span>
<span id="cb61-2008"><a href="#cb61-2008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2009"><a href="#cb61-2009" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Further analyzing food preservation techniques {.unnumbered}</span></span>
<span id="cb61-2010"><a href="#cb61-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2011"><a href="#cb61-2011" aria-hidden="true" tabindex="-1"></a>In your role as a food scientist, you're exploring into the comparative effects of various food preservation methods on nutrient retention, utilizing a <span class="in">`food_preservation`</span> dataset that includes measurements from freezing, canning, and drying methods. This dataset has been crafted to incorporate variations in shelf life that depend on the nutrient retention values, reflecting real-world scenarios where preservation efficacy varies significantly. Your analysis will involve visually exploring these differences using advanced plotting techniques and nonparametric tests.</span>
<span id="cb61-2012"><a href="#cb61-2012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2013"><a href="#cb61-2013" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb61-2014"><a href="#cb61-2014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2015"><a href="#cb61-2015" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a boxen plot to explore the distribution of nutrient retention across the three different preservation methods.</span>
<span id="cb61-2016"><a href="#cb61-2016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2019"><a href="#cb61-2019" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb61-2020"><a href="#cb61-2020" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb61-2021"><a href="#cb61-2021" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-2022"><a href="#cb61-2022" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb61-2023"><a href="#cb61-2023" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-2024"><a href="#cb61-2024" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> kruskal</span>
<span id="cb61-2025"><a href="#cb61-2025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2026"><a href="#cb61-2026" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb61-2027"><a href="#cb61-2027" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">242</span>)</span>
<span id="cb61-2028"><a href="#cb61-2028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2029"><a href="#cb61-2029" aria-hidden="true" tabindex="-1"></a><span class="co"># Define number of rows</span></span>
<span id="cb61-2030"><a href="#cb61-2030" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> <span class="dv">215</span></span>
<span id="cb61-2031"><a href="#cb61-2031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2032"><a href="#cb61-2032" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ProductID from 1 to 215</span></span>
<span id="cb61-2033"><a href="#cb61-2033" aria-hidden="true" tabindex="-1"></a>product_id <span class="op">=</span> np.arange(<span class="dv">1</span>, num_rows <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb61-2034"><a href="#cb61-2034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2035"><a href="#cb61-2035" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate NutrientRetention with the given summary statistics</span></span>
<span id="cb61-2036"><a href="#cb61-2036" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">73.481</span>, scale<span class="op">=</span><span class="fl">14.838</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-2037"><a href="#cb61-2037" aria-hidden="true" tabindex="-1"></a>nutrient_retention <span class="op">=</span> np.clip(nutrient_retention, <span class="fl">50.030</span>, <span class="fl">99.810</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-2038"><a href="#cb61-2038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2039"><a href="#cb61-2039" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb61-2040"><a href="#cb61-2040" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-2041"><a href="#cb61-2041" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-2042"><a href="#cb61-2042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2043"><a href="#cb61-2043" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ShelfLife with the given summary statistics</span></span>
<span id="cb61-2044"><a href="#cb61-2044" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">105.210</span>, scale<span class="op">=</span><span class="fl">54.661</span>, size<span class="op">=</span>num_rows)</span>
<span id="cb61-2045"><a href="#cb61-2045" aria-hidden="true" tabindex="-1"></a>shelf_life <span class="op">=</span> np.clip(shelf_life, <span class="fl">27.860</span>, <span class="fl">267.500</span>)  <span class="co"># Ensuring within min-max range</span></span>
<span id="cb61-2046"><a href="#cb61-2046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2047"><a href="#cb61-2047" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FoodType and PreservationMethod categories</span></span>
<span id="cb61-2048"><a href="#cb61-2048" aria-hidden="true" tabindex="-1"></a>food_types <span class="op">=</span> [<span class="st">'Fruit'</span>, <span class="st">'Meat'</span>, <span class="st">'Vegetable'</span>]</span>
<span id="cb61-2049"><a href="#cb61-2049" aria-hidden="true" tabindex="-1"></a>preservation_methods <span class="op">=</span> [<span class="st">'Canning'</span>, <span class="st">'Drying'</span>, <span class="st">'Freezing'</span>]</span>
<span id="cb61-2050"><a href="#cb61-2050" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> [<span class="dv">22</span>, <span class="dv">32</span>, <span class="dv">26</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">25</span>]</span>
<span id="cb61-2051"><a href="#cb61-2051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2052"><a href="#cb61-2052" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate categorical data for FoodType and PreservationMethod based on given counts</span></span>
<span id="cb61-2053"><a href="#cb61-2053" aria-hidden="true" tabindex="-1"></a>food_preservation_data <span class="op">=</span> []</span>
<span id="cb61-2054"><a href="#cb61-2054" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (food, method), count <span class="kw">in</span> <span class="bu">zip</span>([(f, p) <span class="cf">for</span> f <span class="kw">in</span> food_types <span class="cf">for</span> p <span class="kw">in</span> preservation_methods], counts):</span>
<span id="cb61-2055"><a href="#cb61-2055" aria-hidden="true" tabindex="-1"></a>    food_preservation_data.extend([(food, method)] <span class="op">*</span> count)</span>
<span id="cb61-2056"><a href="#cb61-2056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2057"><a href="#cb61-2057" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and ensure it has the required 215 rows</span></span>
<span id="cb61-2058"><a href="#cb61-2058" aria-hidden="true" tabindex="-1"></a>food_preservation_categorical <span class="op">=</span> pd.DataFrame(food_preservation_data[:num_rows], columns<span class="op">=</span>[<span class="st">'FoodType'</span>, <span class="st">'PreservationMethod'</span>])</span>
<span id="cb61-2059"><a href="#cb61-2059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2060"><a href="#cb61-2060" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final DataFrame</span></span>
<span id="cb61-2061"><a href="#cb61-2061" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.DataFrame({</span>
<span id="cb61-2062"><a href="#cb61-2062" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ProductID'</span>: product_id,</span>
<span id="cb61-2063"><a href="#cb61-2063" aria-hidden="true" tabindex="-1"></a>    <span class="st">'NutrientRetention'</span>: nutrient_retention,</span>
<span id="cb61-2064"><a href="#cb61-2064" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ShelfLife'</span>: shelf_life</span>
<span id="cb61-2065"><a href="#cb61-2065" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb61-2066"><a href="#cb61-2066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2067"><a href="#cb61-2067" aria-hidden="true" tabindex="-1"></a><span class="co"># Add categorical data</span></span>
<span id="cb61-2068"><a href="#cb61-2068" aria-hidden="true" tabindex="-1"></a>food_preservation <span class="op">=</span> pd.concat([food_preservation, food_preservation_categorical], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-2069"><a href="#cb61-2069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2070"><a href="#cb61-2070" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a boxen plot for nutrient retention by preservation</span></span>
<span id="cb61-2071"><a href="#cb61-2071" aria-hidden="true" tabindex="-1"></a>sns.boxenplot(data<span class="op">=</span>food_preservation, </span>
<span id="cb61-2072"><a href="#cb61-2072" aria-hidden="true" tabindex="-1"></a>      x<span class="op">=</span><span class="st">"PreservationMethod"</span>, </span>
<span id="cb61-2073"><a href="#cb61-2073" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span><span class="st">"NutrientRetention"</span>)</span>
<span id="cb61-2074"><a href="#cb61-2074" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-2075"><a href="#cb61-2075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2076"><a href="#cb61-2076" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate nutrient retention for each preservation method</span></span>
<span id="cb61-2077"><a href="#cb61-2077" aria-hidden="true" tabindex="-1"></a>freezing <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Freezing'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb61-2078"><a href="#cb61-2078" aria-hidden="true" tabindex="-1"></a>canning <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Canning'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb61-2079"><a href="#cb61-2079" aria-hidden="true" tabindex="-1"></a>drying <span class="op">=</span> food_preservation[food_preservation[<span class="st">'PreservationMethod'</span>] <span class="op">==</span> <span class="st">'Drying'</span>][<span class="st">'NutrientRetention'</span>]</span>
<span id="cb61-2080"><a href="#cb61-2080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2081"><a href="#cb61-2081" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Kruskal-Wallis test</span></span>
<span id="cb61-2082"><a href="#cb61-2082" aria-hidden="true" tabindex="-1"></a>k_stat, k_pval <span class="op">=</span> kruskal(freezing, canning, drying)</span>
<span id="cb61-2083"><a href="#cb61-2083" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Kruskal-Wallis test p-value:"</span>, k_pval)</span>
<span id="cb61-2084"><a href="#cb61-2084" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb61-2085"><a href="#cb61-2085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2086"><a href="#cb61-2086" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb61-2087"><a href="#cb61-2087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2088"><a href="#cb61-2088" aria-hidden="true" tabindex="-1"></a>_By effectively visualizing and statistically analyzing the nutrient retention across different preservation methods, I've gained insights into how these methods impact food quality. The boxen plot provided a deeper understanding of the data's distribution, and the Kruskal-Wallis test helped me assess the statistical differences between groups. The large p-value leads us to fail to conclude that a difference in the median values across the three groups of preservation methods exists for nutrient retention._</span>
<span id="cb61-2089"><a href="#cb61-2089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2090"><a href="#cb61-2090" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>